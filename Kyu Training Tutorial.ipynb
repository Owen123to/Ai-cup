{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb173248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Conv2D, ReLU, Flatten, Dense, Softmax, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62384345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95ed0c",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "Open **kyu_train.csv** file and split the games into a list.\n",
    "Every row of csv: `KL0000000001,B,B[pq],W[dd],B[dp],W[pd],B[jc],...`. \n",
    "\n",
    "Columns are:\n",
    "\n",
    "    1. KL0000000001: Game ID\n",
    "    2. B: Player's color\n",
    "    3-... : Moves\n",
    "    \n",
    "We cropped only the moves to game list as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8872fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = open('./Training Dataset/kyu_train.csv').read().splitlines()\n",
    "games = [i.split(',',2)[-1] for i in df]\n",
    "colors = [i.split(',',2)[1] for i in df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58532b01",
   "metadata": {},
   "source": [
    "Create a dictionary to convert the coordinates from characters to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "496585f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = 'abcdefghijklmnopqrs'\n",
    "coordinates = {k:v for v,k in enumerate(chars)}\n",
    "chartonumbers = {k:v for k,v in enumerate(chars)}\n",
    "coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92277370",
   "metadata": {},
   "source": [
    "We decided to build a DCNN model in this tutorial. We create data samples by using every move in every game, meaning that the target is to predict the next move by feeding the previous state of the table in every game for every move. Therefore, we can collect much more data samples from games.\n",
    "\n",
    "For the simplicity, we used 4 dimensional feature map to represent the data as below:\n",
    " 1. Positions of black stones: mark them as 1 and the rest of the table as 0\n",
    " 2. Positions of white stones: mark them as 1 and the rest of the table as 0\n",
    " 3. Empty areas of the table: mark the empty areas as 1 and occupied areas as 0\n",
    " 4. The last move in the table: mark the position of the last move as 1 and the rest as 0\n",
    " \n",
    "Target value is a number between 0-361(19\\*19). Later this will be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0adb423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0:所有黑棋\n",
    "#1:所有白棋\n",
    "#2:標示空地\n",
    "#3~10:最後8步\n",
    "#11:周圍黑棋7*7\n",
    "#12:周圍白棋7*7\n",
    "def prepare_input(moves):\n",
    "    x = np.zeros((19,19,13))\n",
    "    if len(moves) == 0:\n",
    "        return x\n",
    "    for move in moves:\n",
    "        color = move[0]\n",
    "        column = coordinates[move[2]]\n",
    "        row = coordinates[move[3]]\n",
    "        if color == 'B':\n",
    "            x[row,column,0] = 1\n",
    "            x[row,column,2] = 1\n",
    "        if color == 'W':\n",
    "            x[row,column,1] = 1\n",
    "            x[row,column,2] = 1\n",
    "    x[:,:,2] = np.where(x[:,:,2] == 0, 1, 0)\n",
    "    \n",
    "    #倒數8步\n",
    "    sz = len(moves)\n",
    "    last1 = sz - 1\n",
    "    last8 = max(sz - 9, 0)\n",
    "    for i in range(last1, last8, -1):\n",
    "        col = coordinates[moves[i][2]]\n",
    "        row = coordinates[moves[i][3]]\n",
    "        x[row, col, 11 - (sz - i)] = 1\n",
    "    \n",
    "    #周圍7*7\n",
    "    last_col = coordinates[moves[-1][2]]\n",
    "    last_row = coordinates[moves[-1][3]]\n",
    "    rad = 3 #要改範圍大小的話改這個\n",
    "    row1 = max(0, last_row - rad)\n",
    "    row7 = min(18, last_row + rad)\n",
    "    col1 = max(0, last_col - rad)\n",
    "    col7 = min(18, last_col + rad)\n",
    "    for i in range(row1, row7 + 1, 1):\n",
    "        for j in range(col1, col7 + 1, 1):\n",
    "            x[i, j, 11] = x[i, j, 1]\n",
    "            x[i, j, 12] = x[i, j, 2]\n",
    "            \n",
    "    #列印所有棋盤\n",
    "#     for i in range(0, 13, 1):\n",
    "#         print(\"  a b c d e f g h i j k l m n o p q r s\")\n",
    "#         for j in range(0, 19, 1):\n",
    "#             print(chars[j], end = \" \")\n",
    "#             for k in range(0, 19, 1):\n",
    "#                 print(int(x[j, k, i]), end = \" \")\n",
    "#             print(\"\")\n",
    "#         print(\"\")\n",
    "    \n",
    "    return x\n",
    "def prepare_label(move):\n",
    "    column = coordinates[move[2]]\n",
    "    row = coordinates[move[3]]\n",
    "    return column*19+row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758808ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Games: 118500, Total Moves: 27135638\n"
     ]
    }
   ],
   "source": [
    "# Check how many samples can be obtained\n",
    "n_games = 0\n",
    "n_moves = 0\n",
    "for game in games:\n",
    "    n_games += 1\n",
    "    moves_list = game.split(',')\n",
    "    for move in moves_list:\n",
    "        n_moves += 1\n",
    "print(f\"Total Games: {n_games}, Total Moves: {n_moves}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46403360",
   "metadata": {},
   "source": [
    "The code below is run for baseline model only by using only the first 500 games from the dataset. You might need to create a data generator to use complete dataset. Otherwise your RAM might not enough to store all (If you run the code on free version of Google Colab, it will crash above 500 game samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9bb0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(games, batch_size):\n",
    "    def generator():\n",
    "        x_batch = [] # Initialize data batch\n",
    "        y_batch = [] # Initialize target batch\n",
    "        for game_i, game in enumerate(games): # Iterate through games\n",
    "            moves_list = game.split(',')\n",
    "#             print(moves_list)\n",
    "            for count, move in enumerate(moves_list):\n",
    "                if colors[game_i] == move[0]:\n",
    "#                     print(move)\n",
    "                    x_batch.append(prepare_input(moves_list[:count]))\n",
    "                    y_batch.append(prepare_label(moves_list[count]))\n",
    "                    if len(x_batch) == batch_size: # Yield when reached batch size\n",
    "                        yield np.array(x_batch), tf.one_hot(np.array(y_batch), depth=19*19)\n",
    "                        x_batch = []\n",
    "                        y_batch = []\n",
    "    return generator\n",
    "\n",
    "batch_size = 128\n",
    "val_rate = 0.1\n",
    "split_point = int(len(games) * (1 - val_rate))\n",
    "generator = data_generator(games[:split_point], batch_size)\n",
    "dataset = tf.data.Dataset.from_generator(generator, \n",
    "                                         output_types=(tf.float32, tf.float32),\n",
    "                                         output_shapes=(tf.TensorShape((batch_size,19,19,13)),tf.TensorShape((batch_size,361)))\n",
    "                                        )\n",
    "# SHUFFLE_BUFFER_SIZE = 200\n",
    "# dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b2392a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = data_generator(games[:split_point], batch_size)\n",
    "val_dataset = tf.data.Dataset.from_generator(val_generator, \n",
    "                                         output_types=(tf.float32, tf.float32),\n",
    "                                         output_shapes=(tf.TensorShape((batch_size,19,19,13)),tf.TensorShape((batch_size,361)))\n",
    "                                        )\n",
    "# SHUFFLE_BUFFER_SIZE = 200\n",
    "# dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(batch_size)\n",
    "val_dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c5de9",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "### Simple DCNN Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "208834da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = Input(shape=(19, 19, 13))\n",
    "    outputs = Conv2D(kernel_size=7, filters=64, padding='same', activation='relu')(inputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=7, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=5, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=5, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=1, padding='same', activation='relu')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Softmax()(outputs)\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    opt = Nadam(learning_rate = 0.0005)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a66e90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 19, 19, 13)]      0         \n",
      "_________________________________________________________________\n",
      "conv2d_108 (Conv2D)          (None, 19, 19, 64)        40832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_108 (Bat (None, 19, 19, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_109 (Conv2D)          (None, 19, 19, 64)        200768    \n",
      "_________________________________________________________________\n",
      "batch_normalization_109 (Bat (None, 19, 19, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_110 (Conv2D)          (None, 19, 19, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_110 (Bat (None, 19, 19, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_111 (Conv2D)          (None, 19, 19, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_111 (Bat (None, 19, 19, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_112 (Conv2D)          (None, 19, 19, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_112 (Bat (None, 19, 19, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_113 (Conv2D)          (None, 19, 19, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_113 (Bat (None, 19, 19, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_114 (Conv2D)          (None, 19, 19, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_114 (Bat (None, 19, 19, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_115 (Conv2D)          (None, 19, 19, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_115 (Bat (None, 19, 19, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_116 (Conv2D)          (None, 19, 19, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_116 (Bat (None, 19, 19, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_117 (Conv2D)          (None, 19, 19, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_117 (Bat (None, 19, 19, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_118 (Conv2D)          (None, 19, 19, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_118 (Bat (None, 19, 19, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_119 (Conv2D)          (None, 19, 19, 1)         577       \n",
      "_________________________________________________________________\n",
      "batch_normalization_119 (Bat (None, 19, 19, 1)         4         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 361)               0         \n",
      "_________________________________________________________________\n",
      "softmax_9 (Softmax)          (None, 361)               0         \n",
      "=================================================================\n",
      "Total params: 708,421\n",
      "Trainable params: 707,011\n",
      "Non-trainable params: 1,410\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "276965b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2D_BN(inputs,growth_rate):\n",
    "    nfilter = growth_rate * 4  \n",
    "    outputs  = keras.layers.Activation('relu')(inputs)\n",
    "    outputs  = keras.layers.BatchNormalization()(outputs)\n",
    "    outputs = keras.layers.Conv2D(filters = nfilter,kernel_size = (1,1),padding='same',strides=1)(outputs)\n",
    "    outputs  = keras.layers.Activation('relu')(outputs)\n",
    "    outputs  = keras.layers.BatchNormalization()(outputs)\n",
    "    outputs = keras.layers.Conv2D(filters = growth_rate,kernel_size=(3,3),padding='same',strides=1)(outputs)\n",
    "    return outputs\n",
    "\n",
    "def dense_block(inputs,growth_rate,n_filter,layers):\n",
    "    concat = inputs \n",
    "    for i in range(layers):\n",
    "        outputs = Conv2D_BN(concat,growth_rate)\n",
    "        concat = keras.layers.concatenate([outputs,concat])\n",
    "        n_filter += growth_rate\n",
    "    return concat,n_filter\n",
    "\n",
    "def transition_block(inputs,n_filter):\n",
    "    print(n_filter)\n",
    "    nfilter = int(n_filter*0.5)\n",
    "    outputs  = keras.layers.Activation('relu')(inputs)\n",
    "    outputs  = keras.layers.BatchNormalization()(outputs)\n",
    "    outputs = keras.layers.Conv2D(filters = nfilter,kernel_size = (1,1),padding='same',strides=1)(outputs )\n",
    "#     outputs = keras.layers.AveragePooling2D(pool_size=2)(outputs)\n",
    "    return outputs,nfilter\n",
    "\n",
    "def get_model():\n",
    "    nfilter = 32\n",
    "    growth_rate = 32\n",
    "    inputs = keras.Input(shape=(19,19, 11))\n",
    "    x = keras.layers.Conv2D(filters = nfilter,kernel_size=(7,7),padding='same',strides=1,activation='relu')(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "#     x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "    \n",
    "    x,nfilter = dense_block(x,growth_rate,nfilter,6)\n",
    "    x,nfilter = transition_block(x,nfilter)\n",
    "    x,nfilter = dense_block(x,growth_rate,nfilter,12)\n",
    "    x,nfilter = transition_block(x,nfilter)\n",
    "    x,nfilter = dense_block(x,growth_rate,nfilter,24)\n",
    "    x,nfilter = transition_block(x,nfilter)\n",
    "    x,nfilter = dense_block(x,growth_rate,nfilter,16)\n",
    "    \n",
    "#     x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dense(361,activation='softmax')(x)\n",
    "    model = keras.Model(inputs=inputs,outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67bbb916",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "496\n",
      "1016\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 19, 19, 11)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 10, 10, 32)   17280       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 10, 10, 32)   128         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 10, 10, 32)   0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 10, 10, 32)   128         activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 10, 10, 128)  4224        batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 10, 10, 128)  0           conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 10, 10, 128)  512         activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 10, 10, 64)   0           conv2d_122[0][0]                 \n",
      "                                                                 batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 10, 10, 64)   0           concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 10, 10, 64)   256         activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 10, 10, 128)  8320        batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 10, 10, 128)  0           conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 10, 10, 128)  512         activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 10, 10, 96)   0           conv2d_124[0][0]                 \n",
      "                                                                 concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 10, 10, 96)   0           concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 10, 10, 96)   384         activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 10, 10, 128)  12416       batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 10, 10, 128)  0           conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 10, 10, 128)  512         activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 10, 10, 128)  0           conv2d_126[0][0]                 \n",
      "                                                                 concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 10, 10, 128)  0           concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 10, 10, 128)  512         activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 10, 10, 128)  16512       batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 10, 10, 128)  0           conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 10, 10, 128)  512         activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 10, 10, 160)  0           conv2d_128[0][0]                 \n",
      "                                                                 concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 10, 10, 160)  0           concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 10, 10, 160)  640         activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 10, 10, 128)  20608       batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 10, 10, 128)  0           conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 10, 10, 128)  512         activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 10, 10, 192)  0           conv2d_130[0][0]                 \n",
      "                                                                 concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 10, 10, 192)  0           concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 10, 10, 192)  768         activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 10, 10, 128)  24704       batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 10, 10, 128)  0           conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 10, 10, 128)  512         activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 10, 10, 224)  0           conv2d_132[0][0]                 \n",
      "                                                                 concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 10, 10, 224)  0           concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 10, 10, 224)  896         activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 10, 10, 112)  25200       batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 10, 10, 112)  0           conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 10, 10, 112)  448         activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 10, 10, 128)  14464       batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 10, 10, 128)  0           conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 10, 10, 128)  512         activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_64 (Concatenate)    (None, 10, 10, 144)  0           conv2d_135[0][0]                 \n",
      "                                                                 conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 10, 10, 144)  0           concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 10, 10, 144)  576         activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 10, 10, 128)  18560       batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 10, 10, 128)  0           conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 10, 10, 128)  512         activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_65 (Concatenate)    (None, 10, 10, 176)  0           conv2d_137[0][0]                 \n",
      "                                                                 concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 10, 10, 176)  0           concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 10, 10, 176)  704         activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 10, 10, 128)  22656       batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 10, 10, 128)  0           conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 10, 10, 128)  512         activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, 10, 10, 208)  0           conv2d_139[0][0]                 \n",
      "                                                                 concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 10, 10, 208)  0           concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 10, 10, 208)  832         activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 10, 10, 128)  26752       batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 10, 10, 128)  0           conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 10, 10, 128)  512         activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 10, 10, 240)  0           conv2d_141[0][0]                 \n",
      "                                                                 concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 10, 10, 240)  0           concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 10, 10, 240)  960         activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 10, 10, 128)  30848       batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 10, 10, 128)  0           conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 10, 10, 128)  512         activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, 10, 10, 272)  0           conv2d_143[0][0]                 \n",
      "                                                                 concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 10, 10, 272)  0           concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 10, 10, 272)  1088        activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 10, 10, 128)  34944       batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 10, 10, 128)  0           conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 10, 10, 128)  512         activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, 10, 10, 304)  0           conv2d_145[0][0]                 \n",
      "                                                                 concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 10, 10, 304)  0           concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 10, 10, 304)  1216        activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 10, 10, 128)  39040       batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 10, 10, 128)  0           conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 10, 10, 128)  512         activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_70 (Concatenate)    (None, 10, 10, 336)  0           conv2d_147[0][0]                 \n",
      "                                                                 concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 10, 10, 336)  0           concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 10, 10, 336)  1344        activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 10, 10, 128)  43136       batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 10, 10, 128)  0           conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 10, 10, 128)  512         activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_71 (Concatenate)    (None, 10, 10, 368)  0           conv2d_149[0][0]                 \n",
      "                                                                 concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 10, 10, 368)  0           concatenate_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 10, 10, 368)  1472        activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 10, 10, 128)  47232       batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 10, 10, 128)  0           conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 10, 10, 128)  512         activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_72 (Concatenate)    (None, 10, 10, 400)  0           conv2d_151[0][0]                 \n",
      "                                                                 concatenate_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 10, 10, 400)  0           concatenate_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 10, 10, 400)  1600        activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 10, 10, 128)  51328       batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 10, 10, 128)  0           conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 10, 10, 128)  512         activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_73 (Concatenate)    (None, 10, 10, 432)  0           conv2d_153[0][0]                 \n",
      "                                                                 concatenate_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 10, 10, 432)  0           concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 10, 10, 432)  1728        activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 10, 10, 128)  55424       batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 10, 10, 128)  0           conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 10, 10, 128)  512         activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_74 (Concatenate)    (None, 10, 10, 464)  0           conv2d_155[0][0]                 \n",
      "                                                                 concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 10, 10, 464)  0           concatenate_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 10, 10, 464)  1856        activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 10, 10, 128)  59520       batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 10, 10, 128)  0           conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 10, 10, 128)  512         activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_75 (Concatenate)    (None, 10, 10, 496)  0           conv2d_157[0][0]                 \n",
      "                                                                 concatenate_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 10, 10, 496)  0           concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 10, 10, 496)  1984        activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 10, 10, 248)  123256      batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 10, 10, 248)  0           conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 10, 10, 248)  992         activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 10, 10, 128)  31872       batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 10, 10, 128)  0           conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 10, 10, 128)  512         activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_76 (Concatenate)    (None, 10, 10, 280)  0           conv2d_160[0][0]                 \n",
      "                                                                 conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 10, 10, 280)  0           concatenate_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 10, 10, 280)  1120        activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 10, 10, 128)  35968       batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 10, 10, 128)  0           conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 10, 10, 128)  512         activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_77 (Concatenate)    (None, 10, 10, 312)  0           conv2d_162[0][0]                 \n",
      "                                                                 concatenate_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 10, 10, 312)  0           concatenate_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 10, 10, 312)  1248        activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 10, 10, 128)  40064       batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 10, 10, 128)  0           conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 10, 10, 128)  512         activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_78 (Concatenate)    (None, 10, 10, 344)  0           conv2d_164[0][0]                 \n",
      "                                                                 concatenate_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 10, 10, 344)  0           concatenate_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 10, 10, 344)  1376        activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 10, 10, 128)  44160       batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 10, 10, 128)  0           conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 10, 10, 128)  512         activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_79 (Concatenate)    (None, 10, 10, 376)  0           conv2d_166[0][0]                 \n",
      "                                                                 concatenate_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 10, 10, 376)  0           concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 10, 10, 376)  1504        activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 10, 10, 128)  48256       batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 10, 10, 128)  0           conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 10, 10, 128)  512         activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_80 (Concatenate)    (None, 10, 10, 408)  0           conv2d_168[0][0]                 \n",
      "                                                                 concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 10, 10, 408)  0           concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 10, 10, 408)  1632        activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 10, 10, 128)  52352       batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 10, 10, 128)  0           conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 10, 10, 128)  512         activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_81 (Concatenate)    (None, 10, 10, 440)  0           conv2d_170[0][0]                 \n",
      "                                                                 concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 10, 10, 440)  0           concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 10, 10, 440)  1760        activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 10, 10, 128)  56448       batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 10, 10, 128)  0           conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 10, 10, 128)  512         activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_82 (Concatenate)    (None, 10, 10, 472)  0           conv2d_172[0][0]                 \n",
      "                                                                 concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 10, 10, 472)  0           concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 10, 10, 472)  1888        activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 10, 10, 128)  60544       batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 10, 10, 128)  0           conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 10, 10, 128)  512         activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_83 (Concatenate)    (None, 10, 10, 504)  0           conv2d_174[0][0]                 \n",
      "                                                                 concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 10, 10, 504)  0           concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 10, 10, 504)  2016        activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 10, 10, 128)  64640       batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 10, 10, 128)  0           conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 10, 10, 128)  512         activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_84 (Concatenate)    (None, 10, 10, 536)  0           conv2d_176[0][0]                 \n",
      "                                                                 concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 10, 10, 536)  0           concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 10, 10, 536)  2144        activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 10, 10, 128)  68736       batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 10, 10, 128)  0           conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 10, 10, 128)  512         activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_85 (Concatenate)    (None, 10, 10, 568)  0           conv2d_178[0][0]                 \n",
      "                                                                 concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 10, 10, 568)  0           concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 10, 10, 568)  2272        activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 10, 10, 128)  72832       batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 10, 10, 128)  0           conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 10, 10, 128)  512         activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_86 (Concatenate)    (None, 10, 10, 600)  0           conv2d_180[0][0]                 \n",
      "                                                                 concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 10, 10, 600)  0           concatenate_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 10, 10, 600)  2400        activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 10, 10, 128)  76928       batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 10, 10, 128)  0           conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 10, 10, 128)  512         activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_87 (Concatenate)    (None, 10, 10, 632)  0           conv2d_182[0][0]                 \n",
      "                                                                 concatenate_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 10, 10, 632)  0           concatenate_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 10, 10, 632)  2528        activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 10, 10, 128)  81024       batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 10, 10, 128)  0           conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 10, 10, 128)  512         activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_88 (Concatenate)    (None, 10, 10, 664)  0           conv2d_184[0][0]                 \n",
      "                                                                 concatenate_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 10, 10, 664)  0           concatenate_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 10, 10, 664)  2656        activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 10, 10, 128)  85120       batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 10, 10, 128)  0           conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 10, 10, 128)  512         activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_89 (Concatenate)    (None, 10, 10, 696)  0           conv2d_186[0][0]                 \n",
      "                                                                 concatenate_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 10, 10, 696)  0           concatenate_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 10, 10, 696)  2784        activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 10, 10, 128)  89216       batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 10, 10, 128)  0           conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 10, 10, 128)  512         activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_90 (Concatenate)    (None, 10, 10, 728)  0           conv2d_188[0][0]                 \n",
      "                                                                 concatenate_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 10, 10, 728)  0           concatenate_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 10, 10, 728)  2912        activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 10, 10, 128)  93312       batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 10, 10, 128)  0           conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 10, 10, 128)  512         activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_91 (Concatenate)    (None, 10, 10, 760)  0           conv2d_190[0][0]                 \n",
      "                                                                 concatenate_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 10, 10, 760)  0           concatenate_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 10, 10, 760)  3040        activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 10, 10, 128)  97408       batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 10, 10, 128)  0           conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 10, 10, 128)  512         activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_92 (Concatenate)    (None, 10, 10, 792)  0           conv2d_192[0][0]                 \n",
      "                                                                 concatenate_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 10, 10, 792)  0           concatenate_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, 10, 10, 792)  3168        activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 10, 10, 128)  101504      batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 10, 10, 128)  0           conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, 10, 10, 128)  512         activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_93 (Concatenate)    (None, 10, 10, 824)  0           conv2d_194[0][0]                 \n",
      "                                                                 concatenate_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 10, 10, 824)  0           concatenate_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 10, 10, 824)  3296        activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 10, 10, 128)  105600      batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 10, 10, 128)  0           conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 10, 10, 128)  512         activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_94 (Concatenate)    (None, 10, 10, 856)  0           conv2d_196[0][0]                 \n",
      "                                                                 concatenate_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 10, 10, 856)  0           concatenate_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 10, 10, 856)  3424        activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 10, 10, 128)  109696      batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 10, 10, 128)  0           conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 10, 10, 128)  512         activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_95 (Concatenate)    (None, 10, 10, 888)  0           conv2d_198[0][0]                 \n",
      "                                                                 concatenate_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 10, 10, 888)  0           concatenate_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, 10, 10, 888)  3552        activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 10, 10, 128)  113792      batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 10, 10, 128)  0           conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 10, 10, 128)  512         activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_96 (Concatenate)    (None, 10, 10, 920)  0           conv2d_200[0][0]                 \n",
      "                                                                 concatenate_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 10, 10, 920)  0           concatenate_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, 10, 10, 920)  3680        activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 10, 10, 128)  117888      batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 10, 10, 128)  0           conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, 10, 10, 128)  512         activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_97 (Concatenate)    (None, 10, 10, 952)  0           conv2d_202[0][0]                 \n",
      "                                                                 concatenate_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 10, 10, 952)  0           concatenate_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, 10, 10, 952)  3808        activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 10, 10, 128)  121984      batch_normalization_203[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 10, 10, 128)  0           conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchN (None, 10, 10, 128)  512         activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_204[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_98 (Concatenate)    (None, 10, 10, 984)  0           conv2d_204[0][0]                 \n",
      "                                                                 concatenate_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 10, 10, 984)  0           concatenate_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, 10, 10, 984)  3936        activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 10, 10, 128)  126080      batch_normalization_205[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, 10, 10, 128)  0           conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchN (None, 10, 10, 128)  512         activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_206[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_99 (Concatenate)    (None, 10, 10, 1016) 0           conv2d_206[0][0]                 \n",
      "                                                                 concatenate_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, 10, 10, 1016) 0           concatenate_99[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, 10, 10, 1016) 4064        activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 10, 10, 508)  516636      batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_206 (Activation)     (None, 10, 10, 508)  0           conv2d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 10, 10, 508)  2032        activation_206[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 10, 10, 128)  65152       batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, 10, 10, 128)  0           conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 10, 10, 128)  512         activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_209[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_100 (Concatenate)   (None, 10, 10, 540)  0           conv2d_209[0][0]                 \n",
      "                                                                 conv2d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, 10, 10, 540)  0           concatenate_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 10, 10, 540)  2160        activation_208[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 10, 10, 128)  69248       batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, 10, 10, 128)  0           conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 10, 10, 128)  512         activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_101 (Concatenate)   (None, 10, 10, 572)  0           conv2d_211[0][0]                 \n",
      "                                                                 concatenate_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, 10, 10, 572)  0           concatenate_101[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 10, 10, 572)  2288        activation_210[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, 10, 10, 128)  73344       batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 10, 10, 128)  0           conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, 10, 10, 128)  512         activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_213 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_213[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_102 (Concatenate)   (None, 10, 10, 604)  0           conv2d_213[0][0]                 \n",
      "                                                                 concatenate_101[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 10, 10, 604)  0           concatenate_102[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, 10, 10, 604)  2416        activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, 10, 10, 128)  77440       batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 10, 10, 128)  0           conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, 10, 10, 128)  512         activation_213[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_215[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_103 (Concatenate)   (None, 10, 10, 636)  0           conv2d_215[0][0]                 \n",
      "                                                                 concatenate_102[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, 10, 10, 636)  0           concatenate_103[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 10, 10, 636)  2544        activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, 10, 10, 128)  81536       batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, 10, 10, 128)  0           conv2d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 10, 10, 128)  512         activation_215[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_104 (Concatenate)   (None, 10, 10, 668)  0           conv2d_217[0][0]                 \n",
      "                                                                 concatenate_103[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, 10, 10, 668)  0           concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 10, 10, 668)  2672        activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)             (None, 10, 10, 128)  85632       batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 10, 10, 128)  0           conv2d_218[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 10, 10, 128)  512         activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_105 (Concatenate)   (None, 10, 10, 700)  0           conv2d_219[0][0]                 \n",
      "                                                                 concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 10, 10, 700)  0           concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 10, 10, 700)  2800        activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_220 (Conv2D)             (None, 10, 10, 128)  89728       batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 10, 10, 128)  0           conv2d_220[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 10, 10, 128)  512         activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_221 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_106 (Concatenate)   (None, 10, 10, 732)  0           conv2d_221[0][0]                 \n",
      "                                                                 concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 10, 10, 732)  0           concatenate_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 10, 10, 732)  2928        activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_222 (Conv2D)             (None, 10, 10, 128)  93824       batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, 10, 10, 128)  0           conv2d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, 10, 10, 128)  512         activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_223 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_107 (Concatenate)   (None, 10, 10, 764)  0           conv2d_223[0][0]                 \n",
      "                                                                 concatenate_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, 10, 10, 764)  0           concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 10, 10, 764)  3056        activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_224 (Conv2D)             (None, 10, 10, 128)  97920       batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, 10, 10, 128)  0           conv2d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 10, 10, 128)  512         activation_223[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_225 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_108 (Concatenate)   (None, 10, 10, 796)  0           conv2d_225[0][0]                 \n",
      "                                                                 concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, 10, 10, 796)  0           concatenate_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 10, 10, 796)  3184        activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_226 (Conv2D)             (None, 10, 10, 128)  102016      batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_225 (Activation)     (None, 10, 10, 128)  0           conv2d_226[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, 10, 10, 128)  512         activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_227 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_109 (Concatenate)   (None, 10, 10, 828)  0           conv2d_227[0][0]                 \n",
      "                                                                 concatenate_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_226 (Activation)     (None, 10, 10, 828)  0           concatenate_109[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 10, 10, 828)  3312        activation_226[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_228 (Conv2D)             (None, 10, 10, 128)  106112      batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_227 (Activation)     (None, 10, 10, 128)  0           conv2d_228[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 10, 10, 128)  512         activation_227[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_110 (Concatenate)   (None, 10, 10, 860)  0           conv2d_229[0][0]                 \n",
      "                                                                 concatenate_109[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_228 (Activation)     (None, 10, 10, 860)  0           concatenate_110[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_230 (BatchN (None, 10, 10, 860)  3440        activation_228[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, 10, 10, 128)  110208      batch_normalization_230[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_229 (Activation)     (None, 10, 10, 128)  0           conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchN (None, 10, 10, 128)  512         activation_229[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_231[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_111 (Concatenate)   (None, 10, 10, 892)  0           conv2d_231[0][0]                 \n",
      "                                                                 concatenate_110[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_230 (Activation)     (None, 10, 10, 892)  0           concatenate_111[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, 10, 10, 892)  3568        activation_230[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 10, 10, 128)  114304      batch_normalization_232[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_231 (Activation)     (None, 10, 10, 128)  0           conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_233 (BatchN (None, 10, 10, 128)  512         activation_231[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_233[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_112 (Concatenate)   (None, 10, 10, 924)  0           conv2d_233[0][0]                 \n",
      "                                                                 concatenate_111[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_232 (Activation)     (None, 10, 10, 924)  0           concatenate_112[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchN (None, 10, 10, 924)  3696        activation_232[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, 10, 10, 128)  118400      batch_normalization_234[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, 10, 10, 128)  0           conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_235 (BatchN (None, 10, 10, 128)  512         activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_235[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_113 (Concatenate)   (None, 10, 10, 956)  0           conv2d_235[0][0]                 \n",
      "                                                                 concatenate_112[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_234 (Activation)     (None, 10, 10, 956)  0           concatenate_113[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_236 (BatchN (None, 10, 10, 956)  3824        activation_234[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)             (None, 10, 10, 128)  122496      batch_normalization_236[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_235 (Activation)     (None, 10, 10, 128)  0           conv2d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_237 (BatchN (None, 10, 10, 128)  512         activation_235[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_237[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_114 (Concatenate)   (None, 10, 10, 988)  0           conv2d_237[0][0]                 \n",
      "                                                                 concatenate_113[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_236 (Activation)     (None, 10, 10, 988)  0           concatenate_114[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_238 (BatchN (None, 10, 10, 988)  3952        activation_236[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)             (None, 10, 10, 128)  126592      batch_normalization_238[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_237 (Activation)     (None, 10, 10, 128)  0           conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_239 (BatchN (None, 10, 10, 128)  512         activation_237[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)             (None, 10, 10, 32)   36896       batch_normalization_239[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_115 (Concatenate)   (None, 10, 10, 1020) 0           conv2d_239[0][0]                 \n",
      "                                                                 concatenate_114[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10, 10, 361)  368581      concatenate_115[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 7,311,273\n",
      "Trainable params: 7,231,129\n",
      "Non-trainable params: 80,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "opt = Nadam(learning_rate = 0.001)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c7e38088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_model(img_shape, num_classes):\n",
    "    model_resnet = ResNet50(include_top=False, weights=None)\n",
    "\n",
    "    img_input = Input(shape=img_shape)\n",
    "    img_model_resnet = model_resnet(img_input)\n",
    "\n",
    "    x = Flatten(name = 'flatten')(img_model_resnet)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    modified_model = Model(inputs=img_input, outputs=x)\n",
    "    opt = Nadam(learning_rate = 0.001)\n",
    "    modified_model.compile(loss='categorical_crossentropy',\n",
    "                            optimizer=opt, metrics=['acc'])\n",
    "\n",
    "    return modified_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d4cd9eac",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='input_27'), name='input_27', description=\"created by layer 'input_27'\"), but it was called on an input with incompatible shape (None, 19, 19, 11).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv1_conv is incompatible with the layer: expected axis -1 of input shape to have value 3 but received input with shape (None, 25, 25, 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[81], line 5\u001b[0m, in \u001b[0;36mmodified_model\u001b[1;34m(img_shape, num_classes)\u001b[0m\n\u001b[0;32m      2\u001b[0m model_resnet \u001b[38;5;241m=\u001b[39m ResNet50(include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m img_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39mimg_shape)\n\u001b[1;32m----> 5\u001b[0m img_model_resnet \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_resnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m Flatten(name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflatten\u001b[39m\u001b[38;5;124m'\u001b[39m)(img_model_resnet)\n\u001b[0;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m256\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\engine\\base_layer.py:976\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# Functional Model construction mode is invoked when `Layer`s are called on\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _in_functional_construction_mode(\u001b[38;5;28mself\u001b[39m, inputs, args, kwargs, input_list):\n\u001b[1;32m--> 976\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_construction_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[0;32m    980\u001b[0m call_context \u001b[38;5;241m=\u001b[39m base_layer_utils\u001b[38;5;241m.\u001b[39mcall_context()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\engine\\base_layer.py:1114\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1109\u001b[0m     training_arg_passed_by_framework \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m call_context\u001b[38;5;241m.\u001b[39menter(\n\u001b[0;32m   1112\u001b[0m     layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, inputs\u001b[38;5;241m=\u001b[39minputs, build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39mtraining_value):\n\u001b[0;32m   1113\u001b[0m   \u001b[38;5;66;03m# Check input assumptions set after layer building, e.g. input shape.\u001b[39;00m\n\u001b[1;32m-> 1114\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_tensor_symbolic_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1117\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA layer\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms `call` method should return a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1119\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor or a list of Tensors, not None \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1120\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(layer: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\engine\\base_layer.py:848\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    846\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(keras_tensor\u001b[38;5;241m.\u001b[39mKerasTensor, output_signature)\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 848\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_output_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\engine\\base_layer.py:888\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_build(inputs)\n\u001b[0;32m    887\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[1;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_mask_metadata(inputs, outputs, input_masks,\n\u001b[0;32m    892\u001b[0m                         build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\engine\\functional.py:414\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    397\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m  In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m      a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_internal_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\engine\\functional.py:550\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    547\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    549\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 550\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_id, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39mflat_output_ids, tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\engine\\base_layer.py:1020\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1013\u001b[0m eager \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m call_context\u001b[38;5;241m.\u001b[39menter(\n\u001b[0;32m   1015\u001b[0m     layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1016\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m   1017\u001b[0m     build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m eager,\n\u001b[0;32m   1018\u001b[0m     training\u001b[38;5;241m=\u001b[39mtraining_mode):\n\u001b[1;32m-> 1020\u001b[0m   \u001b[43minput_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_input_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m eager:\n\u001b[0;32m   1022\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\engine\\input_spec.py:250\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    248\u001b[0m       value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape_as_list[\u001b[38;5;28mint\u001b[39m(axis)] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {value, \u001b[38;5;28;01mNone\u001b[39;00m}:\n\u001b[1;32m--> 250\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(input_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m layer_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    252\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m incompatible with the layer: expected axis \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(axis) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    253\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of input shape to have value \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(value) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    254\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but received input with shape \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m display_shape(x\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer conv1_conv is incompatible with the layer: expected axis -1 of input shape to have value 3 but received input with shape (None, 25, 25, 11)"
     ]
    }
   ],
   "source": [
    "model = modified_model((19, 19, 11), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39553a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/model_kyu_10_14.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a4d7f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fq]', 'W[nd]', 'B[nc]', 'W[mc]', 'B[oc]', 'W[md]', 'B[qf]', 'W[iq]', 'B[cn]', 'W[fp]', 'B[cq]', 'W[cp]', 'B[dq]', 'W[eq]', 'B[er]', 'W[ep]', 'B[bp]', 'W[fr]', 'B[bo]', 'W[dr]', 'B[cr]', 'W[es]', 'B[dj]', 'W[po]', 'B[qo]', 'W[pp]', 'B[qp]', 'W[oq]', 'B[qq]', 'W[np]', 'B[pn]', 'W[on]', 'B[pm]', 'W[om]', 'B[cf]', 'W[pl]', 'B[ql]', 'W[qm]', 'B[qn]', 'W[qk]', 'B[rm]', 'W[rl]', 'B[qm]', 'W[ok]', 'B[fc]', 'W[df]', 'B[dg]', 'W[ef]', 'B[eg]', 'W[ce]', 'B[bf]', 'W[fg]', 'B[db]', 'W[cc]', 'B[cb]', 'W[bb]', 'B[ic]', 'W[gd]', 'B[gc]', 'W[hd]', 'B[hc]', 'W[id]', 'B[jd]', 'W[je]', 'B[kd]', 'W[ke]', 'B[qi]', 'W[ld]', 'B[fh]', 'W[kc]', 'B[jc]', 'W[kb]', 'B[jb]', 'W[gh]', 'B[fi]', 'W[gi]', 'B[km]', 'W[fj]', 'B[ej]', 'W[fk]', 'B[dl]', 'W[em]', 'B[kp]', 'W[dm]', 'B[cm]', 'W[el]', 'B[dk]', 'W[lo]', 'B[ko]', 'W[ln]', 'B[kn]', 'W[lp]', 'B[lm]', 'W[kq]', 'B[lj]', 'W[jp]', 'B[hn]', 'W[io]', 'B[in]', 'W[go]', 'B[hk]', 'W[gk]', 'B[hj]', 'W[gj]', 'B[ii]', 'W[hl]', 'B[il]', 'W[hm]', 'B[im]', 'W[gn]', 'B[lg]', 'W[mi]', 'B[li]', 'W[mh]', 'B[lh]', 'W[mf]', 'B[ig]', 'W[hg]', 'B[ih]', 'W[if]', 'B[rk]', 'W[rj]', 'B[sl]', 'W[qj]', 'B[ri]', 'W[pi]', 'B[ph]', 'W[oh]', 'B[pg]', 'W[og]', 'B[or]', 'W[nr]', 'B[os]', 'W[ns]', 'B[pr]', 'W[jg]', 'B[jh]', 'W[mm]', 'B[ml]', 'W[nl]', 'B[mk]', 'W[nj]', 'B[fd]', 'W[fe]', 'B[ac]', 'W[bc]', 'B[ae]', 'W[ab]', 'B[ad]', 'W[ca]', 'B[eb]', 'W[da]', 'B[ea]', 'W[ba]', 'B[gb]', 'W[qc]', 'B[qd]', 'W[pc]', 'B[od]', 'W[rd]', 'B[re]', 'W[rb]', 'B[sc]', 'W[sd]', 'B[pb]', 'W[qb]', 'B[ob]', 'W[sb]', 'B[se]', 'W[rc]', 'B[kg]', 'W[jf]', 'B[mg]', 'W[ng]', 'B[ne]', 'W[nf]', 'B[me]', 'W[oe]', 'B[pe]', 'W[le]', 'B[dn]', 'W[en]', 'B[ho]', 'W[hp]', 'B[jo]', 'W[ip]', 'B[sj]', 'W[cs]', 'B[bq]', 'W[bs]', 'B[pj]', 'W[pk]', 'B[oi]', 'W[ni]', 'B[nk]', 'W[oj]', 'B[pi]', 'W[br]', 'B[ed]', 'W[ee]', 'B[dc]', 'W[cd]', 'B[ka]', 'W[la]', 'B[ja]', 'W[mb]', 'B[qa]', 'W[ra]', 'B[pa]', 'W[aq]', 'B[ap]', 'W[ar]', 'B[co]', 'W[do]', 'B[bl]', 'W[na]', 'B[be]', 'W[nb]', 'B[pf]', 'W[of]', 'B[mj]', 'W[ek]', 'B[dh]', 'W[hh]', 'B[hi]', 'W[ei]', 'B[eh]', 'W[bd]', 'B[ch]']\n",
      "['B[pd]', 'W[dp]', 'B[pq]', 'W[dc]', 'B[de]', 'W[po]', 'B[qo]', 'W[qn]', 'B[qp]', 'W[pm]', 'B[nq]', 'W[qi]', 'B[cn]', 'W[gq]', 'B[dj]', 'W[ce]', 'B[cf]', 'W[cd]', 'B[dg]', 'W[fc]', 'B[kd]', 'W[co]', 'B[dn]', 'W[jq]', 'B[qg]', 'W[oi]', 'B[pc]', 'W[ic]', 'B[jc]', 'W[jb]', 'B[kb]', 'W[jd]', 'B[kc]', 'W[ib]', 'B[je]', 'W[id]', 'B[og]', 'W[mf]', 'B[md]', 'W[ne]', 'B[nd]', 'W[oe]', 'B[pe]', 'W[od]', 'B[oc]', 'W[kf]', 'B[ke]', 'W[lh]', 'B[bo]', 'W[bp]', 'B[do]', 'W[cp]', 'B[bn]', 'W[eo]', 'B[en]', 'W[fo]', 'B[mo]', 'W[mm]', 'B[gm]', 'W[nh]', 'B[ri]', 'W[rj]', 'B[rh]', 'W[qj]', 'B[gg]', 'W[be]', 'B[bf]', 'W[af]', 'B[ag]', 'W[ae]', 'B[bh]', 'W[gj]', 'B[ap]', 'W[aq]', 'B[ao]', 'W[bq]', 'B[kq]', 'W[jp]', 'B[kp]', 'W[jo]', 'B[ko]', 'W[fl]', 'B[el]', 'W[fm]', 'B[fn]', 'W[gl]', 'B[gn]', 'W[ek]', 'B[dl]', 'W[dk]', 'B[ck]', 'W[ej]', 'B[di]', 'W[em]', 'B[dm]', 'W[ei]', 'B[eh]', 'W[fh]', 'B[fg]', 'W[gh]', 'B[lm]', 'W[ll]', 'B[ml]', 'W[nm]', 'B[kl]', 'W[lk]', 'B[km]', 'W[kk]', 'B[jf]', 'W[kg]', 'B[hh]', 'W[hj]', 'B[jk]', 'W[jj]', 'B[ik]', 'W[ij]', 'B[jr]', 'W[ir]', 'B[kr]', 'W[rn]', 'B[ro]', 'W[so]', 'B[sp]', 'W[sn]', 'B[rq]', 'W[no]', 'B[np]', 'W[oo]', 'B[sj]', 'W[sk]', 'B[si]', 'W[rk]', 'B[is]', 'W[hr]', 'B[go]', 'W[gp]', 'B[jh]', 'W[ki]', 'B[hp]', 'W[iq]', 'B[ho]', 'W[ka]', 'B[la]', 'W[ja]', 'B[mb]', 'W[fe]', 'B[ed]', 'W[fd]', 'B[ec]', 'W[eb]', 'B[dd]', 'W[cb]', 'B[he]', 'W[ef]', 'B[df]', 'W[eg]', 'B[dh]', 'W[hd]', 'B[hf]', 'W[qh]', 'B[rg]', 'W[ng]', 'B[of]', 'W[hs]', 'B[js]', 'W[eq]', 'B[hk]', 'W[hl]', 'B[il]', 'W[hm]', 'B[im]', 'W[mn]', 'B[ln]', 'W[jn]', 'B[gk]', 'W[fk]', 'B[hn]', 'W[gi]', 'B[ph]', 'W[oh]', 'B[pi]', 'W[pj]', 'B[pg]', 'W[me]', 'B[le]', 'W[lf]', 'B[jg]', 'W[ge]', 'B[ie]', 'W[hi]', 'B[ih]', 'W[ji]', 'B[gf]', 'W[hq]', 'B[jm]', 'W[pp]', 'B[op]', 'W[nf]', 'B[kn]', 'W[kh]', 'B[in]', 'W[mp]', 'B[lo]', 'W[mq]', 'B[nr]', 'W[pr]', 'B[qq]', 'W[or]', 'B[oq]', 'W[ee]']\n",
      "['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fq]', 'W[jr]', 'B[cq]', 'W[dq]', 'B[cp]', 'W[do]', 'B[dr]', 'W[er]', 'B[cr]', 'W[eq]', 'B[cn]', 'W[co]', 'B[bo]', 'W[dn]', 'B[cm]', 'W[el]', 'B[jp]', 'W[lq]', 'B[jm]', 'W[nq]', 'B[qo]', 'W[ck]', 'B[bl]', 'W[bk]', 'B[fc]', 'W[hc]', 'B[cf]', 'W[cd]', 'B[ci]', 'W[fd]', 'B[ed]', 'W[ec]', 'B[gd]', 'W[ee]', 'B[gc]', 'W[nc]', 'B[oc]', 'W[nd]', 'B[qf]', 'W[ge]', 'B[hd]', 'W[he]', 'B[id]', 'W[ie]', 'B[kc]', 'W[jd]', 'B[jc]', 'W[je]', 'B[ld]', 'W[le]', 'B[ke]', 'W[kd]', 'B[mc]', 'W[md]', 'B[lc]', 'W[nb]', 'B[mb]', 'W[kf]', 'B[ob]', 'W[oa]', 'B[pa]', 'W[na]', 'B[qb]', 'W[eb]', 'B[hb]', 'W[ma]', 'B[la]', 'W[ka]', 'B[lb]', 'W[ib]', 'B[ic]', 'W[ej]', 'B[bd]', 'W[bc]', 'B[be]', 'W[ac]', 'B[ei]', 'W[fi]', 'B[eg]', 'W[di]', 'B[dh]', 'W[eh]', 'B[cg]', 'W[fg]', 'B[ef]', 'W[ff]', 'B[df]', 'W[qg]', 'B[rg]', 'W[rf]', 'B[re]', 'W[rh]', 'B[sf]', 'W[qh]', 'B[ql]', 'W[qn]', 'B[rn]', 'W[ro]', 'B[rp]', 'W[rm]', 'B[so]', 'W[qm]', 'B[pl]', 'W[rl]', 'B[qj]', 'W[rk]', 'B[rj]', 'W[qk]', 'B[pk]', 'W[pj]', 'B[pi]', 'W[oj]', 'B[qi]', 'W[og]', 'B[oi]', 'W[nj]', 'B[ni]', 'W[om]', 'B[ol]', 'W[nl]', 'B[pm]', 'W[pn]', 'B[nf]', 'W[ng]', 'B[me]', 'W[mf]', 'B[ne]', 'W[mh]', 'B[mi]', 'W[jk]', 'B[lj]', 'W[kh]', 'B[ll]', 'W[kl]', 'B[km]', 'W[lm]', 'B[kk]', 'W[jl]', 'B[jj]', 'W[hk]', 'B[hj]', 'W[gj]', 'B[ik]', 'W[il]', 'B[ij]', 'W[gl]', 'B[ki]', 'W[lh]', 'B[li]', 'W[ml]', 'B[jh]', 'W[lk]', 'B[pg]', 'W[ph]', 'B[sh]', 'W[si]', 'B[ri]', 'W[sg]', 'B[pf]', 'W[rf]', 'B[oh]', 'W[se]', 'B[sj]', 'W[qe]', 'B[sh]', 'W[rd]', 'B[rg]', 'W[of]', 'B[od]', 'W[pe]', 'B[oe]', 'W[qd]', 'B[sf]', 'W[rb]', 'B[ra]', 'W[sg]', 'B[qh]', 'W[qc]', 'B[sb]']\n",
      "      1/Unknown - 2s 2s/step - loss: 6.8673 - accuracy: 0.0078['B[pd]', 'W[dq]', 'B[pq]', 'W[dd]', 'B[do]', 'W[dm]', 'B[cq]', 'W[dp]', 'B[cp]', 'W[eo]', 'B[dn]', 'W[en]', 'B[cm]', 'W[cl]', 'B[cn]', 'W[el]', 'B[dr]', 'W[er]', 'B[cr]', 'W[fq]', 'B[jp]', 'W[iq]', 'B[jq]', 'W[ip]', 'B[jo]', 'W[io]', 'B[qo]', 'W[bl]', 'B[bm]', 'W[bo]', 'B[co]', 'W[am]', 'B[an]', 'W[al]', 'B[bn]', 'W[dg]', 'B[fc]', 'W[ec]', 'B[fd]', 'W[eb]', 'B[jd]', 'W[gb]', 'B[mc]', 'W[hd]', 'B[ff]', 'W[ge]', 'B[gg]', 'W[fe]', 'B[ig]', 'W[ie]', 'B[gk]', 'W[gi]', 'B[hi]', 'W[hh]', 'B[hg]', 'W[hj]', 'B[ii]', 'W[gj]', 'B[ih]', 'W[hk]', 'B[gh]', 'W[eh]', 'B[qi]', 'W[qc]', 'B[qd]', 'W[pc]', 'B[oc]', 'W[ob]', 'B[nb]', 'W[od]', 'B[nc]', 'W[pa]', 'B[rc]', 'W[rb]', 'B[rd]', 'W[qb]', 'B[ne]', 'W[kc]', 'B[kd]', 'W[jc]', 'B[ql]', 'W[ld]', 'B[le]', 'W[md]', 'B[nd]', 'W[me]', 'B[mf]', 'W[lc]', 'B[kf]', 'W[je]', 'B[ke]', 'W[id]', 'B[og]', 'W[kk]', 'B[mn]', 'W[mk]', 'B[ok]', 'W[ir]', 'B[jr]', 'W[ol]', 'B[pl]', 'W[nl]', 'B[on]', 'W[ni]', 'B[oi]', 'W[oj]', 'B[pj]', 'W[oh]', 'B[pi]', 'W[ph]', 'B[nh]', 'W[ng]', 'B[mh]', 'W[mg]', 'B[lg]', 'W[pg]', 'B[of]', 'W[mi]', 'B[nf]', 'W[nk]', 'B[nj]', 'W[mj]', 'B[oj]', 'W[rg]', 'B[rh]', 'W[ef]', 'B[li]', 'W[ki]', 'B[kh]', 'W[kj]', 'B[in]', 'W[jn]', 'B[kn]', 'W[jm]', 'B[om]', 'W[km]', 'B[ln]', 'W[js]', 'B[ks]', 'W[is]', 'B[lr]', 'W[fh]', 'B[ij]', 'W[ik]', 'B[lb]', 'W[kb]', 'B[ka]', 'W[ja]', 'B[ic]', 'W[la]', 'B[ma]', 'W[mb]', 'B[sb]', 'W[na]', 'B[es]', 'W[fs]', 'B[ds]', 'W[nm]', 'B[nn]', 'W[lm]', 'B[mm]', 'W[lj]', 'B[lh]', 'W[ml]', 'B[fg]', 'W[jf]', 'B[jg]', 'W[ed]', 'B[eg]', 'W[df]', 'B[jj]', 'W[ji]', 'B[jk]', 'W[jl]', 'B[jh]', 'W[hf]', 'B[gf]', 'W[if]', 'B[sa]', 'W[ra]', 'B[sc]', 'W[br]', 'B[bq]', 'W[aq]', 'B[ap]', 'W[ar]', 'B[bs]', 'W[pr]', 'B[qr]', 'W[qq]', 'B[rr]', 'W[oq]', 'B[pp]']\n",
      "      2/Unknown - 3s 92ms/step - loss: 6.8656 - accuracy: 0.0039['B[pd]', 'W[dp]', 'B[pp]', 'W[dd]', 'B[fq]', 'W[hq]', 'B[cq]', 'W[dq]', 'B[cp]', 'W[do]', 'B[dr]', 'W[er]', 'B[cr]', 'W[co]', 'B[fr]', 'W[eq]', 'B[es]', 'W[fp]', 'B[hr]', 'W[iq]', 'B[ir]', 'W[gq]', 'B[gr]', 'W[jr]', 'B[dj]', 'W[jq]', 'B[cf]', 'W[df]', 'B[dg]', 'W[ce]', 'B[ef]', 'W[ee]', 'B[de]', 'W[ff]', 'B[df]', 'W[fd]', 'B[cd]', 'W[cc]', 'B[be]', 'W[dc]', 'B[bc]', 'W[nc]', 'B[oc]', 'W[nd]', 'B[qf]', 'W[of]', 'B[je]', 'W[qg]', 'B[pf]', 'W[pg]', 'B[lc]', 'W[mf]', 'B[gc]', 'W[kd]', 'B[jd]', 'W[kc]', 'B[jc]', 'W[kb]', 'B[ke]', 'W[le]', 'B[jg]', 'W[ib]', 'B[ic]', 'W[hb]', 'B[hc]', 'W[fb]', 'B[kh]', 'W[qc]', 'B[qd]', 'W[pb]', 'B[ob]', 'W[rd]', 'B[re]', 'W[rb]', 'B[sd]', 'W[rc]', 'B[rg]', 'W[rh]', 'B[sg]', 'W[qi]', 'B[oe]', 'W[od]', 'B[pc]', 'W[qa]', 'B[nb]', 'W[mb]', 'B[mc]', 'W[lb]', 'B[ne]', 'W[md]', 'B[nf]', 'W[og]', 'B[ng]', 'W[qn]', 'B[qo]', 'W[pn]', 'B[nq]', 'W[mo]', 'B[lp]', 'W[mp]', 'B[mq]', 'W[ko]', 'B[lo]', 'W[ln]', 'B[kp]', 'W[jo]', 'B[mn]', 'W[no]', 'B[nn]', 'W[oo]', 'B[lq]', 'W[kn]', 'B[mk]', 'W[nh]', 'B[mg]', 'W[lg]', 'B[mh]', 'W[lh]', 'B[mi]', 'W[li]', 'B[lj]', 'W[ki]', 'B[ji]', 'W[kj]', 'B[kk]', 'W[jj]', 'B[ii]', 'W[ij]', 'B[hi]', 'W[nj]', 'B[mj]', 'W[ll]', 'B[lk]', 'W[pe]', 'B[qe]', 'W[ok]', 'B[ni]', 'W[oh]', 'B[oi]', 'W[pi]', 'B[oj]', 'W[nk]', 'B[pk]', 'W[pj]', 'B[ol]', 'W[nl]', 'B[ml]', 'W[qk]', 'B[nm]', 'W[ql]', 'B[cm]', 'W[bo]', 'B[fi]', 'W[fs]', 'B[gs]', 'W[ds]', 'B[cs]', 'W[is]', 'B[bp]', 'W[ar]', 'B[aq]', 'W[br]', 'B[bs]', 'W[ao]', 'B[ap]', 'W[jp]', 'B[cb]', 'W[db]', 'B[ca]', 'W[da]', 'B[ab]', 'W[fg]', 'B[fh]', 'W[bl]', 'B[bk]', 'W[bm]', 'B[cl]', 'W[cn]', 'B[ak]', 'W[dm]', 'B[dl]', 'W[em]', 'B[rn]', 'W[rm]', 'B[ro]', 'W[ms]', 'B[mr]', 'W[ls]', 'B[ns]', 'W[ks]', 'B[or]', 'W[sm]', 'B[jl]', 'W[im]', 'B[il]', 'W[hl]', 'B[hk]', 'W[gl]', 'B[gk]', 'W[fl]', 'B[fk]', 'W[jm]', 'B[lm]', 'W[ik]', 'B[km]', 'W[jk]', 'B[kl]', 'W[hj]', 'B[gj]', 'W[kg]', 'B[jh]', 'W[me]', 'B[lf]', 'W[kf]', 'B[jf]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3/Unknown - 3s 92ms/step - loss: 6.8218 - accuracy: 0.0052['B[pd]', 'W[dp]', 'B[pp]', 'W[dd]', 'B[pj]', 'W[nc]', 'B[nd]', 'W[md]', 'B[ne]', 'W[oc]', 'B[pc]', 'W[lc]', 'B[pf]', 'W[nq]', 'B[np]', 'W[mp]', 'B[no]', 'W[oq]', 'B[pq]', 'W[op]', 'B[oo]', 'W[lq]', 'B[pn]', 'W[ln]', 'B[cn]', 'W[fp]', 'B[dj]', 'W[cg]', 'B[fc]', 'W[ec]', 'B[fd]', 'W[ic]', 'B[cc]', 'W[eb]', 'B[ce]', 'W[de]', 'B[bf]', 'W[cf]', 'B[cd]', 'W[bg]', 'B[ae]', 'W[ef]', 'B[cb]', 'W[gf]', 'B[bp]', 'W[cq]', 'B[bq]', 'W[br]', 'B[cp]', 'W[dq]', 'B[do]', 'W[eo]', 'B[en]', 'W[fn]', 'B[em]', 'W[fm]', 'B[jm]', 'W[ll]', 'B[jk]', 'W[jo]', 'B[hn]', 'W[ho]', 'B[go]', 'W[gn]', 'B[io]', 'W[hp]', 'B[in]', 'W[ip]', 'B[ji]', 'W[lj]', 'B[fk]', 'W[fl]', 'B[el]', 'W[gk]', 'B[fj]', 'W[gj]', 'B[gi]', 'W[hi]', 'B[jg]', 'W[gh]', 'B[lf]', 'W[fi]', 'B[bi]', 'W[ci]', 'B[cj]', 'W[di]', 'B[ei]', 'W[eh]', 'B[ej]', 'W[bh]', 'B[bj]', 'W[kh]', 'B[jh]', 'W[kg]', 'B[kf]', 'W[jf]', 'B[ki]', 'W[li]', 'B[lh]', 'W[mh]', 'B[lg]', 'W[ni]', 'B[kk]', 'W[lk]', 'B[ph]', 'W[pl]', 'B[ql]', 'W[qm]', 'B[qk]', 'W[pm]', 'B[qn]', 'W[rm]', 'B[rn]', 'W[sn]', 'B[so]', 'W[sm]', 'B[rp]', 'W[pk]', 'B[qj]', 'W[oj]', 'B[rj]', 'W[nm]', 'B[ob]', 'W[nb]', 'B[pb]', 'W[je]', 'B[kd]', 'W[kc]', 'B[jd]', 'W[id]', 'B[ld]', 'W[jc]', 'B[me]', 'W[mc]', 'B[pr]', 'W[or]', 'B[os]', 'W[ns]', 'B[ps]', 'W[mr]', 'B[oh]', 'W[oi]', 'B[pi]', 'W[jn]', 'B[il]', 'W[hm]', 'B[im]', 'W[hl]', 'B[km]', 'W[lm]', 'B[kn]', 'W[ko]', 'B[hg]', 'W[hf]', 'B[hh]', 'W[gg]', 'B[mg]', 'W[nh]', 'B[ng]', 'W[mo]', 'B[om]', 'W[ol]', 'B[on]', 'W[nn]', 'B[qo]', 'W[od]', 'B[oe]', 'W[ke]', 'B[le]', 'W[if]', 'B[ij]', 'W[kj]', 'B[jj]', 'W[kl]', 'B[jl]', 'W[hj]', 'B[da]', 'W[ea]', 'B[ag]', 'W[ah]', 'B[af]', 'W[ad]', 'B[be]', 'W[db]', 'B[ca]', 'W[bc]', 'B[ab]', 'W[bb]', 'B[ba]', 'W[dc]', 'B[na]', 'W[ma]', 'B[oa]', 'W[ai]', 'B[aj]', 'W[ig]', 'B[ih]', 'W[cr]', 'B[sk]', 'W[rl]', 'B[rk]', 'W[aq]', 'B[ap]', 'W[ar]', 'B[sl]', 'W[hk]', 'B[nl]', 'W[ok]']\n",
      "['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[qk]', 'W[jj]', 'B[cj]', 'W[rq]', 'B[rp]', 'W[qp]', 'B[qq]', 'W[ro]', 'B[pp]', 'W[qo]', 'B[po]', 'W[rr]', 'B[jp]', 'W[jq]', 'B[kq]', 'W[iq]', 'B[kp]', 'W[gp]', 'B[cq]', 'W[dq]', 'B[cp]', 'W[co]', 'B[bo]', 'W[cn]', 'B[bn]', 'W[cm]', 'B[bm]', 'W[cr]', 'B[cl]', 'W[br]', 'B[bp]', 'W[pc]', 'B[qc]', 'W[od]', 'B[oc]', 'W[pb]', 'B[qb]', 'W[qd]', 'B[pe]', 'W[ob]', 'B[qe]', 'W[nc]', 'B[rd]', 'W[jd]', 'B[fc]', 'W[ch]', 'B[cc]', 'W[cd]', 'B[dc]', 'W[bc]', 'B[ed]', 'W[cb]', 'B[db]', 'W[bb]', 'B[de]', 'W[ce]', 'B[df]', 'W[cf]', 'B[ic]', 'W[jc]', 'B[id]', 'W[dg]', 'B[je]', 'W[ke]', 'B[jf]', 'W[qh]', 'B[pi]', 'W[pg]', 'B[nh]', 'W[of]', 'B[oe]', 'W[ne]', 'B[nf]', 'W[ng]', 'B[mf]', 'W[mg]', 'B[me]', 'W[nd]', 'B[kf]', 'W[mh]', 'B[ni]', 'W[le]', 'B[lf]', 'W[md]', 'B[jb]', 'W[kc]', 'B[kb]', 'W[lc]', 'B[lj]', 'W[kh]', 'B[km]', 'W[ki]', 'B[qm]', 'W[qn]', 'B[pn]', 'W[rm]', 'B[ql]', 'W[rl]', 'B[jr]', 'W[ir]', 'B[kr]', 'W[os]', 'B[or]', 'W[qs]', 'B[ns]', 'W[ps]', 'B[nr]', 'W[lk]', 'B[mk]', 'W[kk]', 'B[ll]', 'W[kn]', 'B[ln]', 'W[jn]', 'B[jm]', 'W[in]', 'B[im]', 'W[hj]', 'B[hn]', 'W[io]', 'B[ip]', 'W[ho]', 'B[hq]', 'W[hr]', 'B[gq]', 'W[gr]', 'B[hp]', 'W[gn]', 'B[hm]', 'W[fq]', 'B[ff]', 'W[fh]', 'B[hg]', 'W[lb]', 'B[qi]', 'W[rh]', 'B[rf]', 'W[sj]', 'B[rj]', 'W[sk]', 'B[bh]', 'W[bg]', 'B[bi]', 'W[ka]', 'B[ib]', 'W[da]', 'B[ea]', 'W[ca]', 'B[fb]', 'W[gm]', 'B[el]', 'W[gk]', 'B[dm]', 'W[do]', 'B[en]', 'W[fo]', 'B[gi]', 'W[dk]', 'B[dl]', 'W[ck]', 'B[bk]', 'W[dj]', 'B[di]', 'W[ci]', 'B[bj]', 'W[ei]', 'B[ek]', 'W[ej]', 'B[fj]', 'W[hl]', 'B[fi]', 'W[dh]', 'B[gg]', 'W[gh]', 'B[qa]', 'W[fk]', 'B[hh]', 'W[hi]', 'B[mi]', 'W[li]', 'B[jh]', 'W[ji]', 'B[ih]', 'W[ef]', 'B[ee]', 'W[eg]', 'B[sg]', 'W[ri]', 'B[ja]', 'W[la]', 'B[pf]', 'W[og]', 'B[rg]', 'W[rk]', 'B[qj]', 'W[pa]', 'B[ik]', 'W[hk]', 'B[jk]', 'W[kl]', 'B[mj]', 'W[kj]', 'B[ml]', 'W[oh]', 'B[oi]', 'W[kg]', 'B[jg]', 'W[lg]', 'B[qg]', 'W[ph]', 'B[is]', 'W[fr]', 'B[hs]', 'W[gs]', 'B[js]', 'W[bq]', 'B[ag]', 'W[af]', 'B[ah]', 'W[ap]', 'B[ao]', 'W[aq]', 'B[sh]', 'W[si]', 'B[eo]', 'W[ep]', 'B[ko]', 'W[fg]', 'B[ge]', 'W[ij]', 'B[lm]', 'W[go]', 'B[fl]', 'W[gl]', 'B[qr]', 'W[rs]', 'B[fm]', 'W[dn]', 'B[fn]', 'W[il]', 'B[jl]', 'W[pr]', 'B[np]']\n",
      "      4/Unknown - 3s 89ms/step - loss: 6.7322 - accuracy: 0.0078['B[pd]', 'W[dp]', 'B[pp]', 'W[dc]', 'B[de]', 'W[ce]', 'B[cf]', 'W[cd]', 'B[dg]', 'W[fc]', 'B[fq]', 'W[cn]', 'B[pj]', 'W[ck]', 'B[jp]', 'W[nq]', 'B[oq]', 'W[np]', 'B[pn]', 'W[kq]', 'B[jq]', 'W[kp]', 'B[jo]', 'W[ko]', 'B[dr]', 'W[cq]', 'B[nc]', 'W[qf]', 'B[qe]', 'W[pf]', 'B[qh]', 'W[nf]', 'B[kd]', 'W[ic]', 'B[ci]', 'W[ek]', 'B[kf]', 'W[oh]', 'B[oj]', 'W[mh]', 'B[fo]', 'W[en]', 'B[nn]', 'W[kn]', 'B[kr]', 'W[lr]', 'B[jr]', 'W[or]', 'B[pr]', 'W[op]', 'B[pq]', 'W[nr]', 'B[oo]', 'W[jn]', 'B[in]', 'W[im]', 'B[hn]', 'W[hm]', 'B[gn]', 'W[gm]', 'B[kl]', 'W[jl]', 'B[kk]', 'W[kh]', 'B[bj]', 'W[bk]', 'B[ed]', 'W[ec]', 'B[gd]', 'W[gc]', 'B[hd]', 'W[hc]', 'B[fi]', 'W[fd]', 'B[fe]', 'W[id]', 'B[ie]', 'W[je]', 'B[ke]', 'W[he]', 'B[ge]', 'W[if]', 'B[hf]', 'W[ie]', 'B[gg]', 'W[hg]', 'B[gf]', 'W[hh]', 'B[gh]', 'W[hi]', 'B[jb]', 'W[ib]', 'B[jc]', 'W[qc]', 'B[pc]', 'W[re]', 'B[qd]', 'W[rd]', 'B[rc]', 'W[qb]', 'B[rb]', 'W[pb]', 'B[ob]', 'W[ra]', 'B[pa]', 'W[sc]', 'B[qa]', 'W[rg]', 'B[sb]', 'W[rh]', 'B[qi]', 'W[ri]', 'B[rj]', 'W[sj]', 'B[sk]', 'W[si]', 'B[rk]', 'W[mk]', 'B[ml]', 'W[nk]', 'B[nl]', 'W[ok]', 'B[pk]', 'W[ol]', 'B[pl]', 'W[om]', 'B[on]', 'W[nm]', 'B[mm]', 'W[mn]', 'B[mj]', 'W[nj]', 'B[ni]', 'W[mi]', 'B[lj]', 'W[oi]', 'B[jg]', 'W[ig]', 'B[jh]', 'W[ki]', 'B[jj]', 'W[ji]', 'B[hj]', 'W[gj]', 'B[fj]', 'W[gk]', 'B[fk]', 'W[fl]', 'B[dj]', 'W[dk]', 'B[hk]', 'W[gi]', 'B[ej]', 'W[cr]', 'B[er]', 'W[ps]', 'B[qs]', 'W[os]', 'B[rr]', 'W[mo]', 'B[og]', 'W[ng]', 'B[of]', 'W[oe]', 'B[pg]', 'W[pe]', 'B[ne]', 'W[qg]', 'B[ph]', 'W[nd]', 'B[me]', 'W[md]', 'B[le]', 'W[od]', 'B[sd]', 'W[sf]', 'B[ii]', 'W[ih]', 'B[ij]', 'W[kg]', 'B[jf]', 'W[lm]', 'B[ll]', 'W[kj]', 'B[lk]', 'W[nh]', 'B[pm]', 'W[ni]', 'B[jk]', 'W[hl]', 'B[lg]', 'W[km]', 'B[lh]', 'W[li]', 'B[mf]', 'W[ik]']\n",
      "      5/Unknown - 3s 90ms/step - loss: 6.6919 - accuracy: 0.0109['B[pd]', 'W[cp]', 'B[pq]', 'W[dc]', 'B[de]', 'W[fd]', 'B[eq]', 'W[dq]', 'B[ep]', 'W[cm]', 'B[iq]', 'W[nc]', 'B[nd]', 'W[md]', 'B[ne]', 'W[oc]', 'B[pc]', 'W[lc]', 'B[pg]', 'W[fg]', 'B[dh]', 'W[cd]', 'B[dk]', 'W[bk]', 'B[cj]', 'W[bj]', 'B[ci]', 'W[bf]', 'B[qo]', 'W[qj]', 'B[ql]', 'W[rg]', 'B[rf]', 'W[qg]', 'B[qf]', 'W[ph]', 'B[og]', 'W[pi]', 'B[pb]', 'W[lf]', 'B[if]', 'W[mh]', 'B[ih]', 'W[qm]', 'B[rm]', 'W[qn]', 'B[rn]', 'W[pl]', 'B[po]', 'W[qk]', 'B[rl]', 'W[om]', 'B[lp]', 'W[no]', 'B[np]', 'W[op]', 'B[oq]', 'W[mp]', 'B[nq]', 'W[mq]', 'B[mo]', 'W[lo]', 'B[mn]', 'W[kp]', 'B[lq]', 'W[kq]', 'B[mr]', 'W[jr]', 'B[ko]', 'W[jo]', 'B[ln]', 'W[ip]', 'B[ir]', 'W[im]', 'B[jq]', 'W[jp]', 'B[kr]', 'W[hp]', 'B[gq]', 'W[em]', 'B[fk]', 'W[gl]', 'B[gj]', 'W[hi]', 'B[hj]', 'W[ii]', 'B[gh]', 'W[hh]', 'B[gg]', 'W[hg]', 'B[gf]', 'W[hf]', 'B[ge]', 'W[he]', 'B[gd]', 'W[fe]', 'B[hd]', 'W[ie]', 'B[id]', 'W[je]', 'B[fc]', 'W[ec]', 'B[fb]', 'W[ef]', 'B[jc]', 'W[ij]', 'B[kb]', 'W[kd]', 'B[lk]', 'W[mj]', 'B[lj]', 'W[li]', 'B[mk]', 'W[hk]', 'B[fh]', 'W[cg]', 'B[eg]', 'W[dr]', 'B[er]', 'W[dg]', 'B[ff]', 'W[df]', 'B[bi]', 'W[nk]', 'B[nj]', 'W[mi]', 'B[nl]', 'W[ok]', 'B[nm]', 'W[on]', 'B[oo]', 'W[sf]', 'B[se]', 'W[sg]', 'B[rd]', 'W[rk]', 'B[nn]', 'W[da]', 'B[ea]', 'W[eb]', 'B[gb]', 'W[fo]', 'B[gp]', 'W[go]', 'B[hq]', 'W[eo]', 'B[dp]', 'W[cq]', 'B[jn]', 'W[in]', 'B[mg]', 'W[nh]', 'B[lg]', 'W[kg]', 'B[mf]', 'W[le]', 'B[kh]', 'W[lh]', 'B[kf]', 'W[jg]', 'B[oh]', 'W[oi]', 'B[ki]', 'W[kj]', 'B[jj]', 'W[kk]', 'B[jk]', 'W[kl]', 'B[jl]', 'W[jm]', 'B[km]', 'W[jh]', 'B[ll]', 'W[ik]', 'B[ng]', 'W[ni]', 'B[pm]', 'W[pn]', 'B[ol]', 'W[pk]', 'B[pm]', 'W[gi]', 'B[fi]', 'W[cl]', 'B[ma]', 'W[lb]', 'B[la]', 'W[fa]', 'B[ga]', 'W[do]', 'B[fp]', 'W[nb]', 'B[na]', 'W[mb]', 'B[oa]', 'W[kc]', 'B[jb]', 'W[fl]', 'B[ek]', 'W[gk]', 'B[fj]', 'W[ag]', 'B[ah]', 'W[ji]', 'B[kj]', 'W[ck]', 'B[dl]', 'W[dm]', 'B[od]', 'W[ds]', 'B[es]', 'W[sk]', 'B[sl]', 'W[kn]', 'B[lo]', 'W[jn]', 'B[ob]', 'W[il]', 'B[aj]', 'W[ak]', 'B[ai]', 'W[ch]', 'B[di]', 'W[bh]', 'B[ea]']\n",
      "      6/Unknown - 3s 88ms/step - loss: 6.6813 - accuracy: 0.0104['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fd]', 'W[cg]', 'B[cc]', 'W[dc]', 'B[cd]', 'W[ce]', 'B[db]', 'W[eb]', 'B[cb]', 'W[fc]', 'B[be]', 'W[cf]', 'B[bf]', 'W[gd]', 'B[fq]', 'W[dn]', 'B[dr]', 'W[hq]', 'B[cq]', 'W[eq]', 'B[er]', 'W[fp]', 'B[gp]', 'W[gq]', 'B[fr]', 'W[cp]', 'B[bp]', 'W[bo]', 'B[bq]', 'W[bg]', 'B[ad]', 'W[go]', 'B[dj]', 'W[nd]', 'B[nc]', 'W[mc]', 'B[oc]', 'W[md]', 'B[qf]', 'W[pg]', 'B[gj]', 'W[qg]', 'B[pf]', 'W[of]', 'B[pj]', 'W[oh]', 'B[pm]', 'W[nj]', 'B[jd]', 'W[nm]', 'B[qo]', 'W[np]', 'B[jg]', 'W[ji]', 'B[hh]', 'W[hi]', 'B[gi]', 'W[ih]', 'B[hg]', 'W[ig]', 'B[if]', 'W[hf]', 'B[he]', 'W[gf]', 'B[je]', 'W[ge]', 'B[hd]', 'W[hc]', 'B[ic]', 'W[gh]', 'B[fh]', 'W[fg]', 'B[eh]', 'W[hj]', 'B[gk]', 'W[kf]', 'B[jf]', 'W[kc]', 'B[jh]', 'W[ii]', 'B[ki]', 'W[kj]', 'B[jj]', 'W[hk]', 'B[lj]', 'W[kk]', 'B[lh]', 'W[hl]', 'B[kg]', 'W[lf]', 'B[lg]', 'W[mf]', 'B[dl]', 'W[gl]', 'B[fl]', 'W[mi]', 'B[li]', 'W[lk]', 'B[hb]', 'W[gc]', 'B[jc]', 'W[fm]', 'B[el]', 'W[nr]', 'B[hr]', 'W[gr]', 'B[gs]', 'W[ir]', 'B[hs]', 'W[is]', 'B[fs]', 'W[ip]', 'B[rg]', 'W[rh]', 'B[rf]', 'W[qi]', 'B[qj]', 'W[rj]', 'B[rk]', 'W[sk]', 'B[rl]', 'W[sl]', 'B[sm]', 'W[sj]', 'B[rm]', 'W[bj]', 'B[ck]', 'W[bk]', 'B[bl]', 'W[cm]', 'B[cl]', 'W[ps]', 'B[qr]', 'W[mb]', 'B[qs]', 'W[os]', 'B[em]', 'W[en]', 'B[bn]', 'W[ao]', 'B[cn]', 'W[co]', 'B[bm]', 'W[dm]', 'B[eg]', 'W[ef]', 'B[mk]', 'W[mj]', 'B[nk]', 'W[ok]', 'B[oj]', 'W[ml]', 'B[ni]', 'W[mh]', 'B[nh]', 'W[mg]', 'B[oi]', 'W[og]', 'B[no]', 'W[mo]', 'B[mp]', 'W[mq]', 'B[mn]', 'W[lp]', 'B[lo]', 'W[mp]', 'B[nn]', 'W[ko]', 'B[ln]', 'W[kn]', 'B[lm]', 'W[km]', 'B[ll]', 'W[nl]', 'B[kl]', 'W[jl]', 'B[on]', 'W[pk]', 'B[qk]', 'W[pp]', 'B[qp]', 'W[op]', 'B[po]', 'W[od]', 'B[pc]', 'W[pb]', 'B[qb]', 'W[oa]', 'B[ob]', 'W[na]', 'B[pa]', 'W[gb]', 'B[ib]', 'W[kb]', 'B[bi]', 'W[ci]', 'B[cj]', 'W[bh]', 'B[di]', 'W[ai]', 'B[or]', 'W[oq]', 'B[pr]', 'W[ns]', 'B[pi]', 'W[sh]', 'B[ph]', 'W[qh]', 'B[ol]', 'W[nk]', 'B[pl]', 'W[sg]', 'B[sf]', 'W[ap]', 'B[aq]', 'W[dq]', 'B[cs]', 'W[da]', 'B[ca]', 'W[ea]', 'B[oe]', 'W[ne]', 'B[pe]', 'W[ng]', 'B[kd]', 'W[ld]', 'B[ga]', 'W[fa]', 'B[ha]', 'W[ja]', 'B[af]', 'W[ag]', 'B[ak]', 'W[aj]', 'B[al]', 'W[an]', 'B[dg]', 'W[df]', 'B[jb]', 'W[ch]', 'B[ia]', 'W[ka]', 'B[nb]', 'W[ma]', 'B[dh]', 'W[ke]', 'B[am]', 'W[hp]', 'B[fn]', 'W[eo]', 'B[gm]', 'W[hm]', 'B[gn]', 'W[hn]', 'B[fm]', 'W[fo]', 'B[fj]', 'B[om]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7/Unknown - 3s 88ms/step - loss: 6.6609 - accuracy: 0.0112['B[pd]', 'W[dp]', 'B[pp]', 'W[dc]', 'B[pj]', 'W[nc]', 'B[nd]', 'W[md]', 'B[ne]', 'W[oc]', 'B[pc]', 'W[ic]', 'B[de]', 'W[dh]', 'B[cc]', 'W[dd]', 'B[cd]', 'W[ee]', 'B[df]', 'W[ef]', 'B[dg]', 'W[eg]', 'B[db]', 'W[eb]', 'B[cb]', 'W[fc]', 'B[ch]', 'W[ci]', 'B[cg]', 'W[bi]', 'B[bh]', 'W[ei]', 'B[jd]', 'W[id]', 'B[je]', 'W[kc]', 'B[ie]', 'W[he]', 'B[hf]', 'W[gf]', 'B[jc]', 'W[jb]', 'B[ib]', 'W[hb]', 'B[hd]', 'W[ia]', 'B[ge]', 'W[fe]', 'B[hc]', 'W[ib]', 'B[gg]', 'W[fg]', 'B[ig]', 'W[gd]', 'B[he]', 'W[gc]', 'B[gh]', 'W[jq]', 'B[cn]', 'W[co]', 'B[dn]', 'W[fp]', 'B[ck]', 'W[bo]', 'B[bn]', 'W[an]', 'B[am]', 'W[ao]', 'B[bl]', 'W[dj]', 'B[fn]', 'W[ho]', 'B[fk]', 'W[gi]', 'B[hi]', 'W[gj]', 'B[gk]', 'W[hj]', 'B[ii]', 'W[ij]', 'B[ji]', 'W[ej]', 'B[dk]', 'W[ek]', 'B[el]', 'W[jj]', 'B[ki]', 'W[nq]', 'B[oq]', 'W[np]', 'B[pn]', 'W[nn]', 'B[hn]', 'W[in]', 'B[hm]', 'W[im]', 'B[go]', 'W[gp]', 'B[nr]', 'W[mr]', 'B[or]', 'W[lq]', 'B[pg]', 'W[ob]', 'B[pb]', 'W[pa]', 'B[qa]', 'W[oa]', 'B[rb]', 'W[ah]', 'B[ag]', 'W[ai]', 'B[bf]', 'W[pk]', 'B[qk]', 'W[qj]', 'B[ql]', 'W[pi]', 'B[oj]', 'W[oi]', 'B[qi]', 'W[qh]', 'B[rj]', 'W[ph]', 'B[qg]', 'W[ri]', 'B[qj]', 'W[rg]', 'B[si]', 'W[rh]', 'B[rf]', 'W[og]', 'B[pf]', 'W[of]', 'B[oe]', 'W[ni]', 'B[nj]', 'W[mi]', 'B[mj]', 'W[li]', 'B[lj]', 'W[me]', 'B[nf]', 'W[mf]', 'B[ng]', 'W[mg]', 'B[mh]', 'W[lh]', 'B[kj]', 'W[om]', 'B[ol]', 'W[pm]', 'B[pl]', 'W[qm]', 'B[qn]', 'W[rm]', 'B[rn]', 'W[nl]', 'B[nk]', 'W[ml]', 'B[kl]', 'W[lm]', 'B[km]', 'W[kn]', 'B[ln]', 'W[ko]', 'B[io]', 'W[hp]', 'B[jn]', 'W[il]', 'B[jm]', 'W[jo]', 'B[ll]', 'W[mm]', 'B[hl]', 'W[jl]', 'B[lc]', 'W[ld]', 'B[kb]', 'W[kd]', 'B[mc]', 'W[mb]', 'B[lb]', 'W[la]', 'B[ma]', 'W[ka]', 'B[kf]', 'W[kg]', 'B[jg]', 'W[kh]', 'B[jh]', 'W[lf]', 'B[ke]', 'W[nh]', 'B[sf]', 'W[od]', 'B[oh]', 'W[og]', 'B[do]', 'W[cq]', 'B[fo]', 'W[eo]', 'B[en]', 'W[ep]', 'B[fh]', 'W[eh]', 'B[sg]', 'W[sh]', 'B[sj]', 'W[sn]', 'B[so]', 'W[sm]', 'B[rp]', 'W[on]', 'B[oo]', 'W[no]', 'B[op]', 'W[hk]', 'B[gm]', 'W[jk]', 'B[ea]', 'W[fa]', 'B[da]', 'W[cj]', 'B[bk]', 'W[ak]', 'B[al]', 'W[aj]', 'B[of]', 'W[rl]', 'B[qe]', 'W[rk]', 'B[sk]', 'W[ns]', 'B[os]', 'W[ms]', 'B[mk]', 'W[kk]', 'B[lk]', 'W[ff]', 'B[fi]', 'W[fj]', 'B[mn]', 'W[nm]']\n",
      "      8/Unknown - 3s 87ms/step - loss: 6.6241 - accuracy: 0.0117['B[dd]', 'W[pq]', 'B[cn]', 'W[dn]', 'B[dm]', 'W[en]', 'B[co]', 'W[cp]', 'B[em]', 'W[gp]', 'B[fn]', 'W[ep]', 'B[fo]', 'W[fp]', 'B[gm]', 'W[cf]', 'B[ch]', 'W[cc]', 'B[cd]', 'W[dc]', 'B[ed]', 'W[bd]', 'B[be]', 'W[bc]', 'B[ce]', 'W[fc]', 'B[ec]', 'W[eb]', 'B[fd]', 'W[gb]', 'B[gc]', 'W[fb]', 'B[hc]', 'W[gi]', 'B[ei]', 'W[ge]', 'B[gd]', 'W[gg]', 'B[he]', 'W[hf]', 'B[fe]', 'W[gf]', 'B[ie]', 'W[gk]', 'B[fj]', 'W[gj]', 'B[fk]', 'W[jk]', 'B[nc]', 'W[qf]', 'B[pb]', 'W[qc]', 'B[qb]', 'W[qo]', 'B[qj]', 'W[ql]', 'B[qg]', 'W[pf]', 'B[pg]', 'W[rb]', 'B[ra]', 'W[rc]', 'B[of]', 'W[rg]', 'B[rh]', 'W[rf]', 'B[oh]', 'W[oe]', 'B[nf]', 'W[me]', 'B[ne]', 'W[nd]', 'B[md]', 'W[mc]', 'B[od]', 'W[pe]', 'B[pc]', 'W[rd]', 'B[hb]', 'W[da]', 'B[jg]', 'W[kq]', 'B[jm]', 'W[jh]', 'B[kh]', 'W[ji]', 'B[ig]', 'W[hh]', 'B[ih]', 'W[hi]', 'B[ii]', 'W[ij]', 'B[ki]', 'W[lk]', 'B[jj]', 'W[il]', 'B[im]', 'W[kj]', 'B[ji]', 'W[li]', 'B[lh]', 'W[mi]', 'B[kl]', 'W[kk]', 'B[hl]', 'W[hk]', 'B[jl]', 'W[ik]', 'B[mm]', 'W[nk]', 'B[ol]', 'W[mp]', 'B[ok]', 'W[nj]', 'B[oj]', 'W[ip]', 'B[bp]', 'W[bq]', 'B[bo]', 'W[aq]', 'B[eg]', 'W[on]', 'B[om]', 'W[pn]', 'B[nn]', 'W[no]', 'B[mo]', 'W[np]', 'B[lo]', 'W[lp]', 'B[ko]', 'W[kp]', 'B[rk]', 'W[rl]', 'B[sl]', 'W[sm]', 'B[sk]', 'W[rn]', 'B[qk]', 'W[ha]', 'B[ia]', 'W[ga]', 'B[jb]', 'W[ae]', 'B[af]', 'W[ad]', 'B[bf]', 'W[gl]', 'B[fl]', 'W[hm]', 'B[hn]', 'W[hl]', 'B[io]', 'W[jo]', 'B[jn]', 'W[jp]', 'B[ho]', 'W[hp]', 'B[ll]', 'W[mk]', 'B[nh]', 'W[mh]', 'B[mg]', 'W[eh]', 'B[fh]', 'W[ef]', 'B[ff]', 'W[fg]', 'B[dh]', 'W[if]', 'B[jf]', 'W[sh]', 'B[ri]', 'W[nl]', 'B[nm]', 'W[pm]', 'B[pl]', 'W[go]', 'B[gn]', 'W[ni]', 'B[oi]', 'W[fi]', 'B[eh]', 'W[ap]', 'B[ao]', 'W[do]', 'B[cm]', 'W[si]', 'B[sj]', 'W[sg]', 'B[sb]', 'W[sc]', 'B[qa]', 'W[sa]', 'B[gh]', 'W[hg]', 'B[sb]', 'W[oc]', 'B[ob]', 'W[sa]', 'B[se]', 'W[re]', 'B[sb]', 'W[ld]', 'B[nd]', 'W[sa]', 'B[ml]', 'W[sb]', 'B[cq]', 'W[cr]', 'B[br]', 'W[dq]', 'B[ar]', 'W[cq]', 'B[nr]', 'W[oq]', 'B[or]', 'W[pr]', 'B[lr]', 'W[kr]', 'B[ks]', 'W[js]', 'B[ls]', 'W[ir]', 'B[ps]', 'W[qs]', 'B[os]', 'W[rr]', 'B[mq]', 'W[ms]', 'B[nq]', 'W[op]', 'B[lq]']\n",
      "      9/Unknown - 3s 87ms/step - loss: 6.5787 - accuracy: 0.0130['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[qk]', 'W[dj]', 'B[fq]', 'W[hq]', 'B[cq]', 'W[cp]', 'B[dq]', 'W[ep]', 'B[fr]', 'W[go]', 'B[fc]', 'W[hc]', 'B[cf]', 'W[fd]', 'B[cc]', 'W[cd]', 'B[dc]', 'W[ed]', 'B[ec]', 'W[ch]', 'B[gc]', 'W[gd]', 'B[bd]', 'W[be]', 'B[bc]', 'W[bf]', 'B[gj]', 'W[jj]', 'B[gm]', 'W[fk]', 'B[gk]', 'W[fl]', 'B[gl]', 'W[fm]', 'B[jl]', 'W[qc]', 'B[qd]', 'W[pc]', 'B[oc]', 'W[ob]', 'B[nc]', 'W[nb]', 'B[mc]', 'W[rd]', 'B[re]', 'W[rc]', 'B[qf]', 'W[qn]', 'B[qp]', 'W[kq]', 'B[mq]', 'W[ln]', 'B[ll]', 'W[mp]', 'B[nq]', 'W[nm]', 'B[nl]', 'W[om]', 'B[ol]', 'W[ig]', 'B[gh]', 'W[hi]', 'B[gi]', 'W[eg]', 'B[lg]', 'W[kk]', 'B[kl]', 'W[mj]', 'B[mk]', 'W[mh]', 'B[mg]', 'W[oi]', 'B[qi]', 'W[kh]', 'B[lh]', 'W[li]', 'B[nh]', 'W[nj]', 'B[oh]', 'W[lq]', 'B[pn]', 'W[pm]', 'B[qm]', 'W[ql]', 'B[pl]', 'W[rm]', 'B[po]', 'W[bp]', 'B[bq]', 'W[aq]', 'B[ar]', 'W[ap]', 'B[bs]', 'W[ke]', 'B[mb]', 'W[me]', 'B[ne]', 'W[gf]', 'B[jc]', 'W[id]', 'B[na]', 'W[pa]', 'B[sd]', 'W[sc]', 'B[jn]', 'W[jp]', 'B[pj]', 'W[rk]', 'B[rj]', 'W[ro]', 'B[rp]', 'W[sp]', 'B[sq]', 'W[so]', 'B[rr]', 'W[gp]', 'B[ko]', 'W[lo]', 'B[ii]', 'W[ih]', 'B[ij]', 'W[ji]', 'B[jk]', 'W[hb]', 'B[gb]', 'W[mf]', 'B[nf]', 'W[ld]', 'B[kj]', 'W[ki]', 'B[lk]', 'W[pi]', 'B[ph]', 'W[hr]', 'B[kf]', 'W[jf]', 'B[lr]', 'W[kr]', 'B[mr]', 'W[kb]', 'B[kc]', 'W[lc]', 'B[lb]', 'W[jb]', 'B[md]', 'W[le]', 'B[je]', 'W[jd]', 'B[jg]', 'W[ie]', 'B[kg]', 'W[jh]', 'B[mi]', 'W[lj]', 'B[fg]', 'W[ff]', 'B[eh]', 'W[dg]', 'B[di]', 'W[ci]', 'B[ej]', 'W[ek]', 'B[fj]', 'W[dh]', 'B[ei]', 'W[gg]', 'B[hh]', 'W[hg]', 'B[fn]', 'W[dm]', 'B[em]', 'W[dl]', 'B[dn]', 'W[cn]', 'B[gn]', 'W[do]', 'B[en]', 'W[el]', 'B[eq]', 'W[sk]', 'B[sj]', 'W[io]', 'B[in]', 'W[gr]', 'B[fs]', 'W[ks]', 'B[ni]', 'W[oj]', 'B[np]', 'W[no]', 'B[oo]', 'W[nn]', 'B[se]', 'W[kp]', 'B[jo]', 'W[ho]', 'B[hn]', 'W[ls]', 'B[ms]', 'W[ha]', 'B[ga]', 'W[ae]', 'B[ka]', 'W[ja]', 'B[la]', 'W[ra]', 'B[qo]', 'W[ad]', 'B[ac]', 'W[km]', 'B[jm]', 'W[lm]', 'B[sm]', 'W[rn]', 'B[rl]', 'W[qm]', 'B[sl]', 'W[mm]', 'B[gs]', 'W[hs]', 'B[fp]', 'W[ok]', 'B[pk]', 'W[fh]', 'B[fi]', 'W[fg]', 'B[gq]', 'W[kk]', 'B[fo]', 'W[cm]', 'B[ip]', 'W[hp]', 'B[iq]', 'W[ir]', 'B[od]', 'W[kj]', 'B[ma]', 'W[oa]', 'B[lf]']\n",
      "     10/Unknown - 3s 86ms/step - loss: 6.5481 - accuracy: 0.0125['B[pd]', 'W[dp]', 'B[pq]', 'W[dc]', 'B[de]', 'W[fd]', 'B[fq]', 'W[cn]', 'B[jp]', 'W[dr]', 'B[dj]', 'W[dg]', 'B[ef]', 'W[eg]', 'B[gf]', 'W[fg]', 'B[ff]', 'W[bd]', 'B[gg]', 'W[gh]', 'B[hh]', 'W[gi]', 'B[gd]', 'W[gc]', 'B[hd]', 'W[hc]', 'B[ig]', 'W[hi]', 'B[ii]', 'W[ij]', 'B[ji]', 'W[hk]', 'B[el]', 'W[en]', 'B[gm]', 'W[go]', 'B[im]', 'W[jj]', 'B[hp]', 'W[ho]', 'B[io]', 'W[hn]', 'B[hm]', 'W[in]', 'B[jn]', 'W[jm]', 'B[jl]', 'W[km]', 'B[il]', 'W[kl]', 'B[fo]', 'W[gp]', 'B[fp]', 'W[gq]', 'B[hq]', 'W[gr]', 'B[fr]', 'W[hr]', 'B[jq]', 'W[fn]', 'B[gn]', 'W[fl]', 'B[fm]', 'W[em]', 'B[gl]', 'W[fk]', 'B[gk]', 'W[gj]', 'B[ir]', 'W[kn]', 'B[jo]', 'W[ki]', 'B[jg]', 'W[jh]', 'B[ih]', 'W[kh]', 'B[kg]', 'W[lg]', 'B[lf]', 'W[mg]', 'B[nc]', 'W[mf]', 'B[le]', 'W[me]', 'B[ld]', 'W[md]', 'B[mc]', 'W[id]', 'B[ie]', 'W[jd]', 'B[je]', 'W[cf]', 'B[qo]', 'W[qm]', 'B[qk]', 'W[qi]', 'B[ol]', 'W[om]', 'B[nm]', 'W[on]', 'B[nn]', 'W[oo]', 'B[no]', 'W[pl]', 'B[nk]', 'W[pk]', 'B[op]', 'W[qf]', 'B[qe]', 'W[pf]', 'B[rf]', 'W[rg]', 'B[re]', 'W[qh]', 'B[rn]', 'W[rm]', 'B[qn]', 'W[ok]', 'B[nl]', 'W[nj]', 'B[mj]', 'W[ni]', 'B[lj]', 'W[kk]', 'B[kj]', 'W[jk]', 'B[hs]', 'W[lh]', 'B[sm]', 'W[sl]', 'B[sn]', 'W[rl]', 'B[kc]', 'W[jc]', 'B[ce]', 'W[be]', 'B[cc]', 'W[cd]', 'B[ec]', 'W[ed]', 'B[dd]', 'W[db]', 'B[eb]', 'W[cb]', 'B[ib]', 'W[ic]', 'B[jb]', 'W[hb]', 'B[kd]', 'W[fc]', 'B[ne]', 'W[of]', 'B[oe]', 'W[ia]', 'B[kb]', 'W[ge]', 'B[he]', 'W[fe]', 'B[nf]', 'W[ng]', 'B[nd]', 'W[es]', 'B[fs]', 'W[lo]', 'B[lp]', 'W[mo]', 'B[mp]', 'W[mk]', 'B[lk]', 'W[ml]', 'B[ll]', 'W[mm]', 'B[lm]', 'W[mn]', 'B[ln]', 'W[np]', 'B[nq]', 'W[no]', 'B[ko]', 'W[fj]', 'B[gs]', 'W[pp]', 'B[qp]', 'W[oq]', 'B[or]', 'W[qq]', 'B[op]', 'W[po]', 'B[oq]', 'W[rq]', 'B[qr]', 'W[so]', 'B[sp]', 'W[rp]', 'B[ro]', 'W[pn]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11/Unknown - 3s 85ms/step - loss: 6.4983 - accuracy: 0.0142['B[dd]', 'W[pq]', 'B[po]', 'W[qo]', 'B[qn]', 'W[qp]', 'B[pn]', 'W[nq]', 'B[qj]', 'W[jp]', 'B[qf]', 'W[qe]', 'B[pf]', 'W[nc]', 'B[cn]', 'W[co]', 'B[dn]', 'W[fq]', 'B[dj]', 'W[fc]', 'B[hc]', 'W[cc]', 'B[dc]', 'W[cd]', 'B[de]', 'W[db]', 'B[eb]', 'W[cb]', 'B[ec]', 'W[cf]', 'B[ce]', 'W[be]', 'B[df]', 'W[cg]', 'B[dg]', 'W[ch]', 'B[dh]', 'W[id]', 'B[hd]', 'W[if]', 'B[ic]', 'W[he]', 'B[jd]', 'W[ie]', 'B[fd]', 'W[kg]', 'B[ci]', 'W[bi]', 'B[bj]', 'W[bh]', 'B[lc]', 'W[kj]', 'B[oj]', 'W[fn]', 'B[eo]', 'W[fo]', 'B[ep]', 'W[eq]', 'B[fp]', 'W[gp]', 'B[gq]', 'W[hq]', 'B[gr]', 'W[hr]', 'B[dq]', 'W[fr]', 'B[cp]', 'W[dr]', 'B[cq]', 'W[cr]', 'B[br]', 'W[gs]', 'B[qc]', 'W[pc]', 'B[re]', 'W[qd]', 'B[rd]', 'W[rc]', 'B[qb]', 'W[rb]', 'B[pb]', 'W[ob]', 'B[ra]', 'W[pa]', 'B[sb]', 'W[qa]', 'B[sc]', 'W[sa]', 'B[sd]', 'W[rn]', 'B[rm]', 'W[ro]', 'B[ql]', 'W[oh]', 'B[ph]', 'W[pi]', 'B[qh]', 'W[pj]', 'B[pk]', 'W[oi]', 'B[nj]', 'W[mh]', 'B[og]', 'W[ng]', 'B[nf]', 'W[mf]', 'B[ne]', 'W[md]', 'B[me]', 'W[ld]', 'B[lf]', 'W[mg]', 'B[le]', 'W[kh]', 'B[kd]', 'W[mc]', 'B[lb]', 'W[mb]', 'B[rf]', 'W[mn]', 'B[nm]', 'W[mm]', 'B[ml]', 'W[ll]', 'B[mk]', 'W[lk]', 'B[nn]', 'W[no]', 'B[oo]', 'W[op]', 'B[mo]', 'W[np]', 'B[lo]', 'W[lm]', 'B[jo]', 'W[io]', 'B[jn]', 'W[in]', 'B[jm]', 'W[il]', 'B[im]', 'W[hm]', 'B[jl]', 'W[ik]', 'B[jk]', 'W[jj]', 'B[ij]', 'W[hj]', 'B[ii]', 'W[hk]', 'B[hi]', 'W[gi]', 'B[gh]', 'W[fi]', 'B[hg]', 'W[fh]', 'B[fg]', 'W[gf]', 'B[gg]', 'W[jh]', 'B[ig]', 'W[jf]', 'B[kf]', 'W[jg]', 'B[lj]', 'W[li]', 'B[mj]', 'W[ji]', 'B[kk]', 'W[lg]', 'B[kp]', 'W[jq]', 'B[kq]', 'W[jr]', 'B[kr]', 'W[js]', 'B[mr]', 'W[nr]', 'B[ns]', 'W[os]', 'B[ms]', 'W[pr]', 'B[fk]', 'W[gj]', 'B[gl]', 'W[hl]', 'B[gm]', 'W[gn]', 'B[em]', 'W[ea]', 'B[fa]', 'W[da]', 'B[fb]', 'W[la]', 'B[kb]', 'W[ka]', 'B[ja]', 'W[ma]', 'B[jb]', 'W[aj]', 'B[bk]', 'W[ak]', 'B[al]', 'W[ai]', 'B[bl]', 'W[bf]', 'B[ge]', 'W[hf]', 'B[ff]', 'W[mi]', 'B[ni]', 'W[nh]', 'B[qi]', 'W[ei]', 'B[di]', 'W[ek]', 'B[el]', 'W[ej]', 'B[dk]', 'W[fm]', 'B[fl]', 'W[ar]', 'B[aq]', 'W[oe]', 'B[of]', 'W[nd]', 'B[bq]', 'W[pe]', 'B[cs]', 'W[ds]', 'B[bs]', 'W[en]', 'B[do]', 'W[ks]', 'B[ls]', 'W[mq]', 'B[lr]', 'W[lp]', 'B[ko]', 'W[mp]', 'B[nl]', 'W[pp]', 'B[je]', 'W[ih]', 'B[hh]', 'W[eh]', 'B[eg]', 'W[sm]', 'B[sl]', 'W[sn]', 'B[gk]', 'W[hn]', 'B[ra]', 'W[rk]', 'B[rl]', 'W[sa]', 'B[kl]', 'W[ra]', 'B[km]']\n",
      "     12/Unknown - 3s 86ms/step - loss: 6.4712 - accuracy: 0.0150['B[pd]', 'W[cp]', 'B[pq]', 'W[dd]', 'B[ep]', 'W[qo]', 'B[qp]', 'W[po]', 'B[nq]', 'W[pp]', 'B[rp]', 'W[ro]', 'B[qr]', 'W[lq]', 'B[jq]', 'W[mp]', 'B[op]', 'W[oo]', 'B[np]', 'W[no]', 'B[mo]', 'W[mq]', 'B[lo]', 'W[nr]', 'B[oq]', 'W[kp]', 'B[ko]', 'W[jp]', 'B[iq]', 'W[ip]', 'B[hq]', 'W[hp]', 'B[gq]', 'W[dq]', 'B[eq]', 'W[cn]', 'B[gp]', 'W[hn]', 'B[mm]', 'W[om]', 'B[mk]', 'W[ok]', 'B[or]', 'W[go]', 'B[fo]', 'W[fn]', 'B[en]', 'W[em]', 'B[fm]', 'W[gn]', 'B[dm]', 'W[el]', 'B[dn]', 'W[do]', 'B[eo]', 'W[dl]', 'B[cm]', 'W[bn]', 'B[dp]', 'W[co]', 'B[cq]', 'W[bq]', 'B[cr]', 'W[br]', 'B[dr]', 'W[bl]', 'B[fc]', 'W[gl]', 'B[cf]', 'W[fd]', 'B[cc]', 'W[cd]', 'B[dc]', 'W[bc]', 'B[ed]', 'W[ee]', 'B[ec]', 'W[de]', 'B[gd]', 'W[fe]', 'B[ge]', 'W[jd]', 'B[bb]', 'W[bd]', 'B[ab]', 'W[dg]', 'B[gg]', 'W[eg]', 'B[gi]', 'W[jf]', 'B[qf]', 'W[qh]', 'B[oc]', 'W[md]', 'B[mc]', 'W[lc]', 'B[nc]', 'W[ld]', 'B[of]', 'W[oi]', 'B[mi]', 'W[cl]', 'B[ei]', 'W[fh]', 'B[fi]', 'W[gh]', 'B[hh]', 'W[hi]', 'B[hj]', 'W[ii]', 'B[gk]', 'W[ki]', 'B[jk]', 'W[lj]', 'B[lk]', 'W[mj]', 'B[kj]', 'W[li]', 'B[ji]', 'W[jh]', 'B[jj]', 'W[ih]', 'B[hl]', 'W[kl]', 'B[jm]', 'W[hm]', 'B[il]', 'W[nk]', 'B[ml]', 'W[ij]', 'B[ik]', 'W[jn]', 'B[kn]', 'W[im]', 'B[km]', 'W[mn]', 'B[nn]', 'W[nm]', 'B[ln]', 'W[ll]', 'B[fk]', 'W[fl]', 'B[ek]', 'W[hg]', 'B[gf]', 'W[hc]', 'B[gc]', 'W[hd]', 'B[so]', 'W[sn]', 'B[sp]', 'W[rm]', 'B[oh]', 'W[ph]', 'B[pg]', 'W[nh]', 'B[og]', 'W[mh]', 'B[mf]', 'W[lg]', 'B[me]', 'W[ci]', 'B[dj]', 'W[cj]', 'B[di]', 'W[ch]', 'B[mr]', 'W[lr]', 'B[ns]', 'W[rg]', 'B[rf]', 'W[lb]', 'B[mb]', 'W[sf]', 'B[se]', 'W[sg]', 'B[rd]', 'W[hb]', 'B[gb]', 'W[lf]', 'B[le]', 'W[ke]', 'B[nd]', 'W[mg]', 'B[qg]', 'W[ri]', 'B[ha]', 'W[ia]', 'B[ga]', 'W[jb]', 'B[la]', 'W[ka]', 'B[ma]', 'W[bs]', 'B[kr]', 'W[ls]', 'B[ks]', 'W[kq]', 'B[ir]', 'W[on]', 'B[jo]', 'W[in]', 'B[io]', 'W[ho]', 'B[nl]', 'W[ol]', 'B[if]', 'W[hf]', 'B[he]', 'W[ie]', 'B[dk]', 'W[ck]', 'B[cs]', 'W[mn]', 'B[si]', 'W[rh]', 'B[nn]', 'W[jr]', 'B[js]', 'W[mn]', 'B[ac]', 'W[ad]', 'B[nn]', 'W[is]', 'B[hs]', 'W[mn]', 'B[fg]', 'W[eh]', 'B[nn]', 'W[cb]', 'B[ca]', 'W[mn]', 'B[ef]', 'W[df]', 'B[ff]', 'W[nn]', 'B[ng]', 'W[dh]', 'B[ms]', 'W[gm]', 'B[bm]', 'W[am]']\n",
      "     13/Unknown - 3s 86ms/step - loss: 6.4324 - accuracy: 0.0144['B[pd]', 'W[dp]', 'B[pq]', 'W[cd]', 'B[ed]', 'W[gc]', 'B[cc]', 'W[dc]', 'B[dd]', 'W[ec]', 'B[fc]', 'W[fb]', 'B[ce]', 'W[bd]', 'B[gd]', 'W[fd]', 'B[fe]', 'W[fc]', 'B[be]', 'W[bc]', 'B[ee]', 'W[ge]', 'B[gf]', 'W[hd]', 'B[dk]', 'W[ff]', 'B[ef]', 'W[fg]', 'B[eg]', 'W[fh]', 'B[eh]', 'W[fi]', 'B[ei]', 'W[hf]', 'B[nc]', 'W[qf]', 'B[qe]', 'W[lc]', 'B[pf]', 'W[qg]', 'B[pg]', 'W[qh]', 'B[ph]', 'W[qj]', 'B[qo]', 'W[qm]', 'B[fq]', 'W[cn]', 'B[cl]', 'W[er]', 'B[fr]', 'W[nq]', 'B[lq]', 'W[lp]', 'B[kp]', 'W[mp]', 'B[kn]', 'W[mm]', 'B[dr]', 'W[dq]', 'B[cr]', 'W[eq]', 'B[es]', 'W[fp]', 'B[gp]', 'W[gq]', 'B[gr]', 'W[hq]', 'B[hp]', 'W[hr]', 'B[fs]', 'W[cq]', 'B[br]', 'W[fo]', 'B[ip]', 'W[bq]', 'B[ir]', 'W[ar]', 'B[ds]', 'W[ro]', 'B[rp]', 'W[rn]', 'B[re]', 'W[po]', 'B[pp]', 'W[qn]', 'B[qp]', 'W[oo]', 'B[or]', 'W[nr]', 'B[sh]', 'W[ri]', 'B[sf]', 'W[pi]', 'B[bn]', 'W[bo]', 'B[bm]', 'W[em]', 'B[jc]', 'W[jd]', 'B[kc]', 'W[kd]', 'B[lb]', 'W[md]', 'B[mc]', 'W[ld]', 'B[kb]', 'W[ni]', 'B[ng]', 'W[lj]', 'B[lg]', 'W[kl]', 'B[jg]', 'W[ic]', 'B[ib]', 'W[hb]', 'B[ia]', 'W[ha]', 'B[mf]', 'W[gm]', 'B[im]', 'W[lr]', 'B[kr]', 'W[mq]', 'B[kq]', 'W[oq]', 'B[pr]', 'W[jj]', 'B[cb]', 'W[bb]', 'B[mn]', 'W[nn]', 'B[lm]', 'W[ml]', 'B[ls]', 'W[lo]', 'B[ln]', 'W[ko]', 'B[jo]', 'W[ae]', 'B[af]', 'W[ad]', 'B[bg]', 'W[sp]', 'B[sq]', 'W[so]', 'B[rr]', 'W[jm]', 'B[jn]', 'W[ll]', 'B[si]', 'W[sj]', 'B[sg]', 'W[nh]', 'B[oh]', 'W[oi]', 'B[nd]', 'W[il]', 'B[hm]', 'W[hl]', 'B[ih]', 'W[hg]', 'B[hi]', 'W[gn]', 'B[in]', 'W[hj]', 'B[fj]', 'W[gj]', 'B[fk]', 'W[ji]', 'B[ao]', 'W[co]', 'B[qc]', 'W[ap]', 'B[an]', 'W[cm]', 'B[bl]', 'W[kg]', 'B[kf]', 'W[kh]', 'B[jf]', 'W[ie]', 'B[lh]', 'W[li]', 'B[jh]', 'W[ki]', 'B[gk]', 'W[ii]', 'B[hh]', 'W[hk]', 'B[mr]', 'W[ns]', 'B[os]', 'W[op]', 'B[mo]', 'W[no]', 'B[dm]', 'W[el]', 'B[dl]', 'W[ek]', 'B[ej]', 'W[dn]', 'B[cj]', 'W[mh]', 'B[mg]', 'W[km]', 'B[ho]', 'W[ms]', 'B[lr]', 'W[me]', 'B[ne]', 'W[rf]', 'B[fl]', 'W[fm]', 'B[gh]', 'W[gg]', 'B[je]', 'W[id]', 'B[le]', 'B[bs]']\n",
      "['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fq]', 'W[jp]', 'B[cq]', 'W[dq]', 'B[cp]', 'W[do]', 'B[dr]', 'W[er]', 'B[cr]', 'W[fr]', 'B[cn]', 'W[dn]', 'B[cm]', 'W[dm]', 'B[fc]', 'W[jd]', 'B[cf]', 'W[df]', 'B[cc]', 'W[dc]', 'B[cd]', 'W[cg]', 'B[de]', 'W[ee]', 'B[ce]', 'W[fd]', 'B[dg]', 'W[ef]', 'B[ch]', 'W[dh]', 'B[bg]', 'W[ci]', 'B[eg]', 'W[di]', 'B[db]', 'W[eb]', 'B[ec]', 'W[ed]', 'B[fb]', 'W[gc]', 'B[gd]', 'W[gb]', 'B[ea]', 'W[ge]', 'B[hd]', 'W[he]', 'B[ic]', 'W[fg]', 'B[fh]', 'W[eh]', 'B[cg]', 'W[gg]', 'B[fi]', 'W[cl]', 'B[ck]', 'W[co]', 'B[bo]', 'W[bl]', 'B[bn]', 'W[dk]', 'B[hi]', 'W[ig]', 'B[fm]', 'W[fk]', 'B[ik]', 'W[gl]', 'B[im]', 'W[hn]', 'B[hb]', 'W[id]', 'B[hc]', 'W[nc]', 'B[oc]', 'W[nd]', 'B[qf]', 'W[po]', 'B[qo]', 'W[qn]', 'B[qp]', 'W[pn]', 'B[nq]', 'W[pj]', 'B[lm]', 'W[lq]', 'B[mo]', 'W[gq]', 'B[pl]', 'W[ql]', 'B[pk]', 'W[qk]', 'B[oj]', 'W[pi]', 'B[oi]', 'W[ph]', 'B[ri]', 'W[rj]', 'B[rh]', 'W[pm]', 'B[nl]', 'W[oh]', 'B[nh]', 'W[ng]', 'B[mh]', 'W[mg]', 'B[lh]', 'W[ji]', 'B[kj]', 'W[in]', 'B[jj]', 'W[ii]', 'B[hj]', 'W[hh]', 'B[fj]', 'W[ek]', 'B[lg]', 'W[lf]', 'B[kf]', 'W[le]', 'B[nb]', 'W[mb]', 'B[ob]', 'W[lc]', 'B[kb]', 'W[kc]', 'B[jb]', 'W[jm]', 'B[jl]', 'W[hm]', 'B[il]', 'W[km]', 'B[kl]', 'W[ln]', 'B[mm]', 'W[lo]', 'B[lp]', 'W[kp]', 'B[mp]', 'W[ko]', 'B[rn]', 'W[rm]', 'B[ro]', 'W[pf]', 'B[pe]', 'W[qg]', 'B[rf]', 'W[of]', 'B[rg]', 'W[mr]', 'B[mq]', 'W[kr]', 'B[nr]', 'W[ns]', 'B[os]', 'W[ms]', 'B[pr]', 'W[mn]', 'B[nn]', 'W[om]', 'B[ol]', 'W[nm]', 'B[ml]', 'W[no]', 'B[jg]', 'W[if]', 'B[ke]', 'W[kd]', 'B[bi]', 'W[bj]', 'B[ai]', 'W[cj]', 'B[ap]', 'W[jf]', 'B[ki]', 'W[je]', 'B[kg]', 'W[oe]', 'B[od]', 'W[me]', 'B[am]', 'W[bm]', 'B[ds]', 'W[es]', 'B[jh]', 'W[ih]', 'B[la]', 'W[ma]', 'B[lb]', 'W[na]', 'B[oa]', 'W[mc]', 'B[sm]', 'W[sl]', 'B[sn]', 'W[si]', 'B[sh]', 'W[sj]', 'B[qh]', 'W[pg]', 'B[np]', 'W[oo]', 'B[pp]', 'W[op]', 'B[oq]', 'W[rd]', 'B[qd]', 'W[qc]', 'B[qb]', 'W[rb]', 'B[qa]', 'W[ra]', 'B[pc]', 'W[rc]', 'B[se]', 'W[re]', 'B[qe]', 'W[sd]', 'B[sb]', 'W[sf]', 'B[sg]', 'W[pb]', 'B[pa]', 'W[qi]', 'B[se]', 'W[fa]', 'B[sc]', 'W[sa]', 'B[sc]', 'W[sf]', 'B[ff]', 'W[pb]', 'B[fe]', 'W[gf]', 'B[gh]', 'W[gj]', 'B[gi]', 'W[gk]', 'B[ak]', 'W[bk]', 'B[aj]', 'W[hk]', 'B[ij]', 'W[hl]', 'B[al]', 'W[em]', 'B[ej]', 'W[dj]', 'B[jc]', 'W[fp]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14/Unknown - 4s 85ms/step - loss: 6.3925 - accuracy: 0.0151['B[dd]', 'W[pq]', 'B[po]', 'W[qo]', 'B[qn]', 'W[qp]', 'B[pn]', 'W[nq]', 'B[qj]', 'W[qh]', 'B[oj]', 'W[cf]', 'B[fc]', 'W[bd]', 'B[cc]', 'W[ci]', 'B[cn]', 'W[co]', 'B[dn]', 'W[fq]', 'B[ck]', 'W[dj]', 'B[dk]', 'W[kc]', 'B[qc]', 'W[pc]', 'B[qd]', 'W[qe]', 'B[re]', 'W[qf]', 'B[rf]', 'W[rg]', 'B[pb]', 'W[ob]', 'B[qb]', 'W[nc]', 'B[ic]', 'W[ip]', 'B[kp]', 'W[kq]', 'B[lq]', 'W[jq]', 'B[mp]', 'W[rn]', 'B[rm]', 'W[ro]', 'B[ql]', 'W[op]', 'B[no]', 'W[oo]', 'B[on]', 'W[oh]', 'B[mj]', 'W[mh]', 'B[kj]', 'W[kh]', 'B[je]', 'W[ke]', 'B[kf]', 'W[lf]', 'B[kd]', 'W[le]', 'B[ld]', 'W[md]', 'B[lc]', 'W[jf]', 'B[ie]', 'W[mc]', 'B[lb]', 'W[km]', 'B[jn]', 'W[jl]', 'B[hn]', 'W[hl]', 'B[fn]', 'W[hj]', 'B[ej]', 'W[ei]', 'B[fj]', 'W[fi]', 'B[gj]', 'W[gi]', 'B[hi]', 'W[ii]', 'B[hh]', 'W[jj]', 'B[hk]', 'W[jk]', 'B[ij]', 'W[ji]', 'B[gl]', 'W[mm]', 'B[nm]', 'W[nl]', 'B[ol]', 'W[nk]', 'B[nj]', 'W[ll]', 'B[ok]', 'W[bn]', 'B[bm]', 'W[bo]', 'B[bc]', 'W[cd]', 'B[de]', 'W[ce]', 'B[df]', 'W[dg]', 'B[eg]', 'W[dh]', 'B[fg]', 'W[hg]', 'B[gh]', 'W[ig]', 'B[cj]', 'W[di]', 'B[bi]', 'W[bh]', 'B[bj]', 'W[ah]', 'B[ad]', 'W[ae]', 'B[ac]', 'W[bg]', 'B[af]', 'W[be]', 'B[gp]', 'W[gq]', 'B[hp]', 'W[hq]', 'B[io]', 'W[jp]', 'B[ko]', 'W[fp]', 'B[fo]', 'W[rj]', 'B[rk]', 'W[ri]', 'B[lr]', 'W[kr]', 'B[os]', 'W[or]', 'B[ns]', 'W[ps]', 'B[mr]', 'W[nr]', 'B[ms]', 'W[qr]', 'B[il]', 'W[ik]', 'B[im]', 'W[hj]', 'B[gk]', 'W[gf]', 'B[fe]', 'W[ge]', 'B[gd]', 'W[he]', 'B[hd]', 'W[if]', 'B[ih]', 'W[jh]', 'B[gg]', 'W[mb]', 'B[ff]', 'W[kg]', 'B[hf]', 'W[mn]', 'B[nn]', 'W[lo]', 'B[lp]', 'W[kn]', 'B[jo]', 'W[mo]', 'B[np]', 'W[pp]', 'B[ks]', 'W[js]', 'B[ls]', 'W[ir]', 'B[do]', 'W[cq]', 'B[an]', 'W[ao]', 'B[am]', 'W[la]', 'B[kb]', 'W[ka]', 'B[ja]', 'W[ma]', 'B[jb]', 'W[lk]', 'B[lj]', 'W[mk]', 'B[kk]', 'W[kl]', 'B[pi]', 'W[ph]', 'B[ni]', 'W[nh]', 'B[li]', 'W[lh]', 'B[sj]', 'W[si]', 'B[sk]', 'W[sf]', 'B[sd]', 'W[pa]', 'B[qa]', 'W[oa]', 'B[sb]', 'W[qi]', 'B[oi]', 'W[ai]', 'B[aj]', 'W[eo]', 'B[en]', 'W[ep]', 'B[sn]', 'W[so]', 'B[sm]', 'W[sg]', 'B[se]', 'W[rc]', 'B[rb]', 'W[jm]', 'B[ij]', 'W[hm]', 'B[gm]', 'W[hj]', 'B[cg]', 'W[ch]', 'B[ij]', 'W[rd]', 'B[sc]', 'W[hj]', 'B[bf]', 'W[ag]', 'B[ij]', 'W[pj]', 'B[pk]', 'W[hj]', 'B[qs]', 'W[rs]', 'B[ij]', 'W[he]', 'B[ge]', 'W[hj]', 'B[cp]', 'W[bp]', 'B[ij]', 'W[go]', 'B[ho]', 'W[hj]', 'B[is]', 'W[hs]', 'B[ij]', 'W[cm]', 'B[cl]', 'W[hj]', 'B[mq]', 'W[ij]', 'B[eh]']\n",
      "     15/Unknown - 4s 85ms/step - loss: 6.3543 - accuracy: 0.0151['B[pd]', 'W[cd]', 'B[dp]', 'W[qp]', 'B[op]', 'W[oq]', 'B[nq]', 'W[pq]', 'B[mp]', 'W[qn]', 'B[iq]', 'W[ec]', 'B[jd]', 'W[cj]', 'B[cl]', 'W[cq]', 'B[dq]', 'W[cp]', 'B[co]', 'W[bo]', 'B[cn]', 'W[bn]', 'B[bm]', 'W[dr]', 'B[er]', 'W[cr]', 'B[fq]', 'W[dg]', 'B[pj]', 'W[jp]', 'B[ip]', 'W[jo]', 'B[io]', 'W[kn]', 'B[kq]', 'W[in]', 'B[hn]', 'W[jq]', 'B[jr]', 'W[hm]', 'B[gn]', 'W[jm]', 'B[jj]', 'W[qc]', 'B[qd]', 'W[pc]', 'B[od]', 'W[nb]', 'B[md]', 'W[qh]', 'B[ph]', 'W[qi]', 'B[pi]', 'W[qj]', 'B[qg]', 'W[rg]', 'B[qf]', 'W[rf]', 'B[re]', 'W[pk]', 'B[ok]', 'W[pl]', 'B[nl]', 'W[jg]', 'B[kg]', 'W[kf]', 'B[lg]', 'W[je]', 'B[kd]', 'W[lf]', 'B[mf]', 'W[jh]', 'B[ki]', 'W[kh]', 'B[lh]', 'W[hf]', 'B[hd]', 'W[fe]', 'B[gc]', 'W[lc]', 'B[ld]', 'W[kc]', 'B[mb]', 'W[mc]', 'B[ob]', 'W[oc]', 'B[na]', 'W[nc]', 'B[rc]', 'W[pb]', 'B[rb]', 'W[oa]', 'B[jc]', 'W[kb]', 'B[jb]', 'W[la]', 'B[hi]', 'W[gh]', 'B[hh]', 'W[hg]', 'B[gi]', 'W[fh]', 'B[gm]', 'W[hl]', 'B[ch]', 'W[di]', 'B[ci]', 'W[bj]', 'B[dh]', 'W[ei]', 'B[dj]', 'W[ej]', 'B[dk]', 'W[cg]', 'B[bh]', 'W[bg]', 'B[bk]', 'W[bi]', 'B[ai]', 'W[eh]', 'B[kl]', 'W[jl]', 'B[jk]', 'W[km]', 'B[ko]', 'W[kp]', 'B[lo]', 'W[kr]', 'B[lq]', 'W[ir]', 'B[po]', 'W[qo]', 'B[hr]', 'W[js]', 'B[lm]', 'W[hs]', 'B[gr]', 'W[lr]', 'B[mr]', 'W[ls]', 'B[ek]', 'W[or]', 'B[pn]', 'W[pm]', 'B[om]', 'W[ln]', 'B[mn]', 'W[lp]', 'B[ll]', 'W[mo]', 'B[no]', 'W[mq]', 'B[lq]', 'W[lo]', 'B[ge]', 'W[gf]', 'B[fd]', 'W[ed]', 'B[ie]', 'W[jf]', 'B[eb]', 'W[db]', 'B[fb]', 'W[da]', 'B[sf]', 'W[sh]', 'B[fc]', 'W[ef]', 'B[an]', 'W[ap]', 'B[gl]', 'W[hk]', 'B[gk]', 'W[ds]', 'B[nd]', 'W[pp]', 'B[gs]', 'W[is]', 'B[ja]', 'W[ma]', 'B[qa]', 'W[qb]', 'B[fj]', 'W[hj]', 'B[le]', 'W[ii]', 'B[gj]', 'W[ih]', 'B[if]', 'W[he]', 'B[gd]', 'W[ig]', 'B[id]', 'W[ea]', 'B[fa]', 'W[ij]', 'B[ik]', 'W[il]', 'B[es]', 'W[nr]', 'B[ms]', 'W[ns]', 'B[mq]', 'W[kq]', 'B[np]', 'W[nn]']\n",
      "     16/Unknown - 4s 85ms/step - loss: 6.3096 - accuracy: 0.0156['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fq]', 'W[nd]', 'B[nc]', 'W[mc]', 'B[oc]', 'W[ld]', 'B[qf]', 'W[iq]', 'B[cq]', 'W[cp]', 'B[dq]', 'W[ep]', 'B[fr]', 'W[go]', 'B[cj]', 'W[po]', 'B[qo]', 'W[qn]', 'B[qp]', 'W[pm]', 'B[nq]', 'W[lp]', 'B[cf]', 'W[cl]', 'B[cd]', 'W[cc]', 'B[ce]', 'W[bc]', 'B[ej]', 'W[ef]', 'B[dg]', 'W[gf]', 'B[hc]', 'W[of]', 'B[ec]', 'W[dc]', 'B[kc]', 'W[lb]', 'B[kb]', 'W[pg]', 'B[qg]', 'W[qh]', 'B[je]', 'W[pi]', 'B[el]', 'W[gl]', 'B[dm]', 'W[cm]', 'B[dn]', 'W[cn]', 'B[dk]', 'W[gi]', 'B[jh]', 'W[ki]', 'B[no]', 'W[mn]', 'B[on]', 'W[om]', 'B[lq]', 'W[kq]', 'B[mp]', 'W[lo]', 'B[hp]', 'W[ho]', 'B[ip]', 'W[jo]', 'B[jp]', 'W[kp]', 'B[jq]', 'W[lr]', 'B[mq]', 'W[io]', 'B[jr]', 'W[kr]', 'B[gp]', 'W[fo]', 'B[ij]', 'W[jk]', 'B[ik]', 'W[il]', 'B[jj]', 'W[kj]', 'B[ih]', 'W[kh]', 'B[kg]', 'W[lg]', 'B[lf]', 'W[mf]', 'B[mg]', 'W[lh]', 'B[me]', 'W[nf]', 'B[le]', 'W[md]', 'B[ke]', 'W[od]', 'B[pc]', 'W[ed]', 'B[fc]', 'W[gd]', 'B[gc]', 'W[hd]', 'B[id]', 'W[if]', 'B[jf]', 'W[ig]', 'B[jg]', 'W[hh]', 'B[hi]', 'W[gh]', 'B[rn]', 'W[rm]', 'B[ro]', 'W[hj]', 'B[ii]', 'W[hk]', 'B[rh]', 'W[ri]', 'B[rg]', 'W[nb]', 'B[ob]', 'W[oa]', 'B[pa]', 'W[na]', 'B[qb]', 'W[pe]', 'B[qe]', 'W[pf]', 'B[bp]', 'W[bo]', 'B[bq]', 'W[pn]', 'B[nn]', 'W[nm]', 'B[op]', 'W[os]', 'B[or]', 'W[ns]', 'B[ms]', 'W[mr]', 'B[ps]', 'W[nr]', 'B[qr]', 'W[sn]', 'B[so]', 'W[sm]', 'B[si]', 'W[sj]', 'B[sh]', 'W[qj]', 'B[db]', 'W[cb]', 'B[eb]', 'W[eh]', 'B[dh]', 'W[ei]', 'B[di]', 'W[fj]', 'B[fk]', 'W[fl]', 'B[fm]', 'W[gm]', 'B[fn]', 'W[gn]', 'B[bk]', 'W[bl]', 'B[ck]', 'W[ka]', 'B[ja]', 'W[la]', 'B[ib]', 'W[kd]', 'B[lc]', 'W[jc]', 'B[jb]', 'W[mb]', 'B[jd]', 'W[ie]', 'B[bd]', 'W[gk]', 'B[ek]', 'W[eq]', 'B[er]', 'W[js]', 'B[ir]', 'W[is]', 'B[hs]', 'W[ks]', 'B[hr]', 'W[ad]', 'B[ae]', 'W[ac]', 'B[bf]', 'W[ak]', 'B[aj]', 'W[al]', 'B[bi]', 'W[ap]', 'B[aq]', 'W[ao]', 'B[fd]', 'W[fe]', 'B[eg]', 'W[fg]', 'B[mo]', 'W[do]', 'B[en]', 'W[da]', 'B[ea]', 'W[ca]', 'B[ne]', 'W[oe]', 'B[de]', 'W[ee]', 'B[dl]', 'W[fp]', 'B[gq]']\n",
      "     17/Unknown - 4s 85ms/step - loss: 6.2853 - accuracy: 0.0156['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fq]', 'W[nc]', 'B[nd]', 'W[qc]', 'B[oc]', 'W[qd]', 'B[mc]', 'W[pe]', 'B[qf]', 'W[pc]', 'B[od]', 'W[qe]', 'B[nb]', 'W[pf]', 'B[cn]', 'W[eo]', 'B[cq]', 'W[cp]', 'B[dq]', 'W[go]', 'B[ep]', 'W[do]', 'B[bp]', 'W[bo]', 'B[bq]', 'W[co]', 'B[cf]', 'W[cj]', 'B[cd]', 'W[cc]', 'B[bc]', 'W[bd]', 'B[ce]', 'W[bb]', 'B[dc]', 'W[cb]', 'B[ed]', 'W[db]', 'B[ec]', 'W[id]', 'B[qo]', 'W[kq]', 'B[mq]', 'W[hq]', 'B[fo]', 'W[fn]', 'B[fp]', 'W[en]', 'B[ql]', 'W[qj]', 'B[jc]', 'W[ic]', 'B[jd]', 'W[ie]', 'B[je]', 'W[if]', 'B[eb]', 'W[bh]', 'B[ch]', 'W[bg]', 'B[cg]', 'W[ci]', 'B[eh]', 'W[jf]', 'B[kf]', 'W[qg]', 'B[fj]', 'W[ek]', 'B[ej]', 'W[dj]', 'B[di]', 'W[ff]', 'B[ef]', 'W[fe]', 'B[ee]', 'W[me]', 'B[ld]', 'W[le]', 'B[ke]', 'W[mg]', 'B[kh]', 'W[ih]', 'B[ji]', 'W[hj]', 'B[ii]', 'W[hi]', 'B[hh]', 'W[gh]', 'B[hg]', 'W[ig]', 'B[gg]', 'W[fg]', 'B[gi]', 'W[fh]', 'B[hk]', 'W[hf]', 'B[ib]', 'W[hb]', 'B[jb]', 'W[fi]', 'B[ei]', 'W[gj]', 'B[ml]', 'W[no]', 'B[mo]', 'W[mn]', 'B[lo]', 'W[nm]', 'B[nl]', 'W[ln]', 'B[ko]', 'W[kn]', 'B[jn]', 'W[nq]', 'B[lq]', 'W[np]', 'B[jq]', 'W[kr]', 'B[jr]', 'W[kl]', 'B[il]', 'W[eg]', 'B[dg]', 'W[fk]', 'B[dh]', 'W[de]', 'B[df]', 'W[gk]', 'B[gc]', 'W[hc]', 'B[gb]', 'W[ha]', 'B[fa]', 'W[jm]', 'B[im]', 'W[jo]', 'B[in]', 'W[jp]', 'B[kp]', 'W[mp]', 'B[ip]', 'W[lp]', 'B[iq]', 'W[io]', 'B[hp]', 'W[ho]', 'B[gp]', 'W[jk]', 'B[jl]', 'W[km]', 'B[kk]', 'W[ik]']\n",
      "     18/Unknown - 4s 85ms/step - loss: 6.2596 - accuracy: 0.0152['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fq]', 'W[cn]', 'B[ql]', 'W[qf]', 'B[qh]', 'W[of]', 'B[nd]', 'W[rd]', 'B[qc]', 'W[rc]', 'B[jp]', 'W[np]', 'B[nq]', 'W[mq]', 'B[oq]', 'W[lp]', 'B[qo]', 'W[jq]', 'B[iq]', 'W[kq]', 'B[ip]', 'W[qj]', 'B[dr]', 'W[cq]', 'B[fc]', 'W[df]', 'B[ic]', 'W[lc]', 'B[mc]', 'W[kc]', 'B[ld]', 'W[oc]', 'B[od]', 'W[mb]', 'B[nc]', 'W[nb]', 'B[dc]', 'W[cc]', 'B[cb]', 'W[bc]', 'B[eb]', 'W[bb]', 'B[ch]', 'W[ca]', 'B[be]', 'W[cd]', 'B[ck]', 'W[kd]', 'B[le]', 'W[pb]', 'B[oh]', 'W[qb]', 'B[oj]', 'W[ph]', 'B[pi]', 'W[rh]', 'B[pg]', 'W[ri]', 'B[pf]', 'W[qe]', 'B[pe]', 'W[qk]', 'B[pl]', 'W[rl]', 'B[rm]', 'W[rk]', 'B[cr]', 'W[br]', 'B[dq]', 'W[bp]', 'B[ep]', 'W[do]', 'B[ie]', 'W[ke]', 'B[lf]', 'W[kf]', 'B[lg]', 'W[kg]', 'B[lh]', 'W[mn]', 'B[lk]', 'W[ml]', 'B[mk]', 'W[nl]', 'B[nk]', 'W[db]', 'B[fe]', 'W[ec]', 'B[gb]', 'W[ea]', 'B[fb]', 'W[ci]', 'B[bi]', 'W[cj]', 'B[bj]', 'W[dh]', 'B[cg]', 'W[dg]', 'B[cf]', 'W[ce]', 'B[jb]', 'W[kb]', 'B[ig]', 'W[gf]', 'B[ge]', 'W[hf]', 'B[if]', 'W[ff]', 'B[he]', 'W[ee]', 'B[ii]', 'W[bl]', 'B[bk]', 'W[pn]', 'B[qn]', 'W[on]', 'B[ek]', 'W[cl]', 'B[dl]', 'W[dm]', 'B[em]', 'W[en]', 'B[fm]', 'W[fn]', 'B[gm]', 'W[ei]', 'B[gj]', 'W[jm]', 'B[jl]', 'W[lm]', 'B[im]', 'W[kl]', 'B[jk]', 'W[kk]', 'B[kj]', 'W[jn]', 'B[jr]', 'W[kr]', 'B[js]', 'W[ks]', 'B[hr]', 'W[mr]', 'B[nr]', 'W[hh]', 'B[ih]', 'W[gi]', 'B[hi]', 'W[gh]', 'B[fj]', 'W[fi]', 'B[qg]', 'W[rg]', 'B[pc]', 'W[ob]', 'B[al]', 'W[bm]', 'B[am]', 'W[an]', 'B[ak]', 'W[gn]', 'B[hn]', 'W[gp]', 'B[gq]', 'W[ho]', 'B[io]', 'W[in]', 'B[hm]', 'W[eo]', 'B[hp]', 'W[fp]', 'B[eq]', 'W[go]', 'B[bs]', 'W[ar]', 'B[ko]', 'W[lo]', 'B[kn]', 'W[km]', 'B[fa]', 'W[da]', 'B[fd]', 'W[jc]', 'B[ib]', 'W[kh]', 'B[ki]', 'W[pp]', 'B[qp]', 'W[op]', 'B[rq]', 'W[ms]', 'B[ns]', 'W[ol]', 'B[jh]', 'W[pk]', 'B[id]', 'W[ok]', 'B[nj]', 'W[om]', 'B[qm]', 'W[bd]', 'B[ae]', 'W[ad]', 'B[bg]', 'W[ja]', 'B[ia]', 'W[ka]', 'B[hg]', 'W[gg]', 'B[ed]', 'W[ej]', 'B[dj]', 'W[di]', 'B[dk]', 'W[sm]', 'B[sn]', 'W[sl]', 'B[ro]', 'W[kp]', 'B[jo]', 'W[po]', 'B[pj]', 'W[dc]', 'B[ll]', 'W[ln]', 'B[cs]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     19/Unknown - 4s 85ms/step - loss: 6.2302 - accuracy: 0.0152['B[pd]', 'W[dp]', 'B[pq]', 'W[dc]', 'B[de]', 'W[po]', 'B[qo]', 'W[qn]', 'B[qp]', 'W[pm]', 'B[nq]', 'W[qi]', 'B[qg]', 'W[ni]', 'B[cn]', 'W[fq]', 'B[bp]', 'W[cq]', 'B[ck]', 'W[cg]', 'B[cc]', 'W[cb]', 'B[cd]', 'W[eb]', 'B[eg]', 'W[cj]', 'B[dj]', 'W[di]', 'B[ej]', 'W[ei]', 'B[fi]', 'W[fh]', 'B[eh]', 'W[fj]', 'B[gi]', 'W[ci]', 'B[fk]', 'W[bk]', 'B[cl]', 'W[bl]', 'B[bm]', 'W[bf]', 'B[cf]', 'W[be]', 'B[ce]', 'W[bi]', 'B[gg]', 'W[gj]', 'B[hj]', 'W[gk]', 'B[gl]', 'W[hk]', 'B[ik]', 'W[hl]', 'B[hm]', 'W[il]', 'B[jl]', 'W[im]', 'B[in]', 'W[jm]', 'B[km]', 'W[jn]', 'B[jo]', 'W[kn]', 'B[ln]', 'W[ko]', 'B[kp]', 'W[lo]', 'B[mo]', 'W[lp]', 'B[lq]', 'W[ek]', 'B[mp]', 'W[dk]', 'B[fj]', 'W[dl]', 'B[cm]', 'W[em]', 'B[en]', 'W[gn]', 'B[fm]', 'W[fn]', 'B[dm]', 'W[eo]', 'B[el]', 'W[pc]', 'B[qc]', 'W[oc]', 'B[qb]', 'W[od]', 'B[pe]', 'W[jd]', 'B[hd]', 'W[he]', 'B[ge]', 'W[ie]', 'B[hf]', 'W[gd]', 'B[fd]', 'W[gc]', 'B[if]', 'W[fe]', 'B[gf]', 'W[ed]', 'B[ee]', 'W[fc]', 'B[ff]', 'W[jf]', 'B[jg]', 'W[kf]', 'B[na]', 'W[nb]', 'B[pa]', 'W[ma]', 'B[oa]', 'W[mb]', 'B[me]', 'W[kg]', 'B[jh]', 'W[bq]', 'B[ap]', 'W[aq]', 'B[cp]', 'W[co]', 'B[bo]', 'W[jr]', 'B[kr]', 'W[ir]', 'B[hq]', 'W[hr]', 'B[gq]', 'W[gr]', 'B[jq]', 'W[do]', 'B[dn]', 'W[fp]', 'B[iq]', 'W[rg]', 'B[rf]', 'W[rh]', 'B[qf]', 'W[nf]', 'B[ne]', 'W[oe]', 'B[of]', 'W[og]', 'B[pf]', 'W[mf]', 'B[pk]', 'W[qk]', 'B[qj]', 'W[rj]', 'B[pj]', 'W[ql]', 'B[pi]', 'W[qh]', 'B[ph]', 'W[pg]', 'B[ng]', 'W[oh]', 'B[mg]', 'W[lf]', 'B[lh]', 'W[nj]', 'B[ol]', 'W[nl]', 'B[nm]', 'W[nk]', 'B[om]', 'W[kh]', 'B[li]', 'W[ki]', 'B[lj]', 'W[ji]', 'B[ii]', 'W[mm]', 'B[mn]', 'W[ll]', 'B[kk]', 'W[rd]', 'B[qd]', 'W[sc]', 'B[sf]', 'W[pb]', 'B[rb]', 'W[qa]', 'B[ra]', 'W[sg]', 'B[sb]', 'W[ro]', 'B[rp]', 'W[rn]', 'B[jj]', 'W[bb]', 'B[bc]', 'W[bd]', 'B[ab]', 'W[ad]', 'B[ac]', 'W[ba]', 'B[dh]', 'W[ch]', 'B[dg]', 'W[gp]', 'B[hp]', 'W[hn]', 'B[ho]', 'W[go]', 'B[gm]', 'W[sp]', 'B[sq]', 'W[so]', 'B[rr]', 'W[op]', 'B[oq]', 'W[pl]', 'B[ok]', 'W[lg]', 'B[mh]', 'W[kj]', 'B[lk]', 'W[fd]', 'B[nn]', 'W[pn]', 'B[ld]', 'W[kc]', 'B[lc]', 'W[lb]', 'B[kb]', 'W[jb]', 'B[ka]', 'W[ja]', 'B[kd]', 'W[jc]', 'B[je]', 'W[ke]', 'B[le]', 'W[je]', 'B[ic]', 'W[hc]', 'B[ib]', 'W[hb]', 'B[pp]', 'W[oo]', 'B[js]', 'W[is]', 'B[ks]', 'W[am]', 'B[an]', 'W[al]', 'B[qa]', 'W[ob]', 'B[np]', 'W[re]', 'W[dd]', 'W[on]', 'W[aa]', 'W[no]', 'W[se]', 'B[rc]']\n",
      "     20/Unknown - 4s 85ms/step - loss: 6.2009 - accuracy: 0.0148['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fq]', 'W[nd]', 'B[nc]', 'W[mc]', 'B[oc]', 'W[ld]', 'B[qf]', 'W[nq]', 'B[op]', 'W[np]', 'B[no]', 'W[mo]', 'B[oo]', 'W[mn]', 'B[oj]', 'W[hq]', 'B[cq]', 'W[cp]', 'B[dq]', 'W[ep]', 'B[eq]', 'W[fp]', 'B[dj]', 'W[gq]', 'B[bp]', 'W[bo]', 'B[bq]', 'W[cn]', 'B[gr]', 'W[hr]', 'B[fs]', 'W[dh]', 'B[gj]', 'W[gh]', 'B[dc]', 'W[ec]', 'B[cd]', 'W[cc]', 'B[db]', 'W[eb]', 'B[cb]', 'W[ce]', 'B[bd]', 'W[be]', 'B[bc]', 'W[ee]', 'B[jj]', 'W[jh]', 'B[kf]', 'W[lg]', 'B[me]', 'W[md]', 'B[le]', 'W[ke]', 'B[lf]', 'W[je]', 'B[jf]', 'W[if]', 'B[ig]', 'W[hf]', 'B[hg]', 'W[gg]', 'B[jg]', 'W[hi]', 'B[hj]', 'W[ii]', 'B[ij]', 'W[kh]', 'B[nf]', 'W[nm]', 'B[pm]', 'W[ol]', 'B[pl]', 'W[ok]', 'B[pk]', 'W[nj]', 'B[oi]', 'W[ni]', 'B[oh]', 'W[om]', 'B[cl]', 'W[dm]', 'B[dl]', 'W[el]', 'B[ek]', 'W[fl]', 'B[fk]', 'W[hl]', 'B[jl]', 'W[im]', 'B[jm]', 'W[jn]', 'B[kn]', 'W[ko]', 'B[jo]', 'W[in]', 'B[lo]', 'W[kp]', 'B[ln]', 'W[lp]', 'B[ll]', 'W[ml]', 'B[ch]', 'W[cg]', 'B[ci]', 'W[eh]', 'B[mb]', 'W[lb]', 'B[nb]', 'W[oe]', 'B[of]', 'W[pe]', 'B[qe]', 'W[pf]', 'B[pg]', 'W[od]', 'B[qc]', 'W[kc]', 'B[ea]', 'W[fa]', 'B[da]', 'W[gb]', 'B[bg]', 'W[bf]', 'B[bh]', 'W[li]', 'B[or]', 'W[nr]', 'B[ns]', 'W[ms]', 'B[os]', 'W[oq]', 'B[qr]', 'W[lr]', 'B[hs]', 'W[is]', 'B[gs]', 'W[jr]', 'B[ds]', 'W[lk]', 'B[kk]', 'W[kj]', 'B[lm]', 'W[gk]', 'B[fi]', 'W[fh]', 'B[ji]', 'W[ih]', 'B[ki]', 'W[lj]', 'B[nh]', 'W[mh]', 'B[mg]', 'W[kg]', 'B[ne]', 'W[on]', 'B[pn]', 'W[bm]', 'B[bl]', 'W[al]', 'B[ak]', 'W[am]', 'B[bj]', 'W[ap]', 'B[aq]', 'W[ao]', 'B[hk]', 'W[il]', 'B[ik]', 'W[gl]', 'B[ei]', 'W[ad]', 'B[ac]', 'W[ae]', 'B[ag]', 'W[cm]', 'B[di]', 'W[nn]', 'B[po]', 'W[ma]', 'B[na]', 'W[la]', 'B[gi]', 'W[hh]', 'B[af]', 'W[ba]', 'B[bb]', 'W[mm]', 'B[dg]', 'W[cf]', 'B[ef]', 'W[de]', 'B[gf]', 'W[ge]']\n",
      "['B[pd]', 'W[dd]', 'B[cp]', 'W[pp]', 'B[nq]', 'W[oq]', 'B[np]', 'W[pm]', 'B[jq]', 'W[eq]', 'B[gq]', 'W[eo]', 'B[dq]', 'W[er]', 'B[dr]', 'W[cn]', 'B[ep]', 'W[fp]', 'B[dp]', 'W[fq]', 'B[fo]', 'W[gp]', 'B[do]', 'W[en]', 'B[dn]', 'W[dm]', 'B[em]', 'W[fn]', 'B[cm]', 'W[dl]', 'B[cl]', 'W[gn]', 'B[dk]', 'W[el]', 'B[fm]', 'W[fl]', 'B[gm]', 'W[ck]', 'B[bn]', 'W[dj]', 'B[hn]', 'W[go]', 'B[im]', 'W[hq]', 'B[bk]', 'W[gl]', 'B[cj]', 'W[hm]', 'B[ej]', 'W[di]', 'B[ci]', 'W[ek]', 'B[ck]', 'W[dh]', 'B[fc]', 'W[ch]', 'B[jd]', 'W[qf]', 'B[pf]', 'W[pg]', 'B[of]', 'W[og]', 'B[qe]', 'W[qg]', 'B[ng]', 'W[nh]', 'B[mg]', 'W[mh]', 'B[lg]', 'W[jn]', 'B[nm]', 'W[in]', 'B[ko]', 'W[nl]', 'B[ml]', 'W[nk]', 'B[lm]', 'W[om]', 'B[nn]', 'W[op]', 'B[oo]', 'W[po]', 'B[db]', 'W[cb]', 'B[dc]', 'W[cc]', 'B[ed]', 'W[de]', 'B[ee]', 'W[qc]', 'B[pc]', 'W[re]', 'B[rd]', 'W[qd]', 'B[rc]', 'W[pe]', 'B[qb]', 'W[qe]', 'B[oe]', 'W[od]', 'B[oc]', 'W[nd]', 'B[nf]', 'W[nc]', 'B[nb]', 'W[pb]', 'B[ob]', 'W[rb]', 'B[pa]', 'W[sd]', 'B[lc]', 'W[da]', 'B[ea]', 'W[ca]', 'B[fb]', 'W[ef]', 'B[bh]', 'W[bg]', 'B[bi]', 'W[cf]', 'B[or]', 'W[pr]', 'B[nr]', 'W[kl]', 'B[ff]', 'W[mk]', 'B[ll]', 'W[lk]', 'B[fh]', 'W[eg]', 'B[fg]', 'W[km]', 'B[ln]', 'W[kh]', 'B[kg]', 'W[jh]', 'B[jg]', 'W[hh]', 'B[hg]', 'W[ih]', 'B[ig]', 'W[gi]', 'B[fi]', 'W[fj]', 'B[ei]', 'W[eh]', 'B[lh]', 'W[li]', 'B[ir]', 'W[ds]', 'B[cs]', 'W[es]', 'B[br]', 'W[hr]', 'B[qq]', 'W[pq]', 'B[qn]', 'W[pn]', 'B[qo]', 'W[qp]', 'B[rp]', 'W[rq]', 'B[rl]', 'W[qm]', 'B[rm]', 'W[ro]', 'B[rn]', 'W[sp]', 'B[ri]', 'W[qk]', 'B[rk]', 'W[qj]', 'B[rj]', 'W[sn]', 'B[sm]', 'W[sk]', 'B[qi]', 'W[sj]', 'B[so]', 'W[rp]', 'B[ql]', 'W[pk]', 'B[pl]', 'W[ol]', 'B[pj]', 'W[ok]', 'B[oi]', 'W[oj]', 'B[pi]', 'W[oh]', 'B[ni]', 'W[mi]', 'B[si]', 'W[rh]']\n",
      "     21/Unknown - 4s 85ms/step - loss: 6.1842 - accuracy: 0.0145['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fq]', 'W[eq]', 'B[fp]', 'W[dn]', 'B[jq]', 'W[po]', 'B[oo]', 'W[on]', 'B[op]', 'W[pn]', 'B[nn]', 'W[nm]', 'B[mn]', 'W[mm]', 'B[ln]', 'W[lm]', 'B[kn]', 'W[km]', 'B[jn]', 'W[jm]', 'B[hn]', 'W[im]', 'B[gn]', 'W[in]', 'B[io]', 'W[fr]', 'B[pj]', 'W[gr]', 'B[qp]', 'W[qo]', 'B[pg]', 'W[nc]', 'B[oc]', 'W[nd]', 'B[jd]', 'W[kc]', 'B[jc]', 'W[kd]', 'B[je]', 'W[ke]', 'B[fc]', 'W[ec]', 'B[fd]', 'W[df]', 'B[ff]', 'W[dh]', 'B[cc]', 'W[cd]', 'B[eb]', 'W[dc]', 'B[db]', 'W[cb]', 'B[nb]', 'W[mb]', 'B[ob]', 'W[jb]', 'B[ib]', 'W[kb]', 'B[hc]', 'W[jf]', 'B[cl]', 'W[cm]', 'B[dl]', 'W[cj]', 'B[fm]', 'W[bl]', 'B[bk]', 'W[bm]', 'B[ck]', 'W[bj]', 'B[aj]', 'W[ai]', 'B[ak]', 'W[bh]', 'B[qm]', 'W[pm]', 'B[ql]', 'W[pl]', 'B[pk]', 'W[ok]', 'B[oj]', 'W[nk]', 'B[nj]', 'W[mj]', 'B[mi]', 'W[lj]', 'B[li]', 'W[kj]', 'B[if]', 'W[jg]', 'B[oe]', 'W[ne]', 'B[nf]', 'W[mf]', 'B[ng]', 'W[mg]', 'B[nh]', 'W[ig]', 'B[hf]', 'W[hg]', 'B[gg]', 'W[gh]', 'B[fh]', 'W[gi]', 'B[fi]', 'W[gj]', 'B[ir]', 'W[hq]', 'B[ip]', 'W[hp]', 'B[ho]', 'W[iq]', 'B[kr]', 'W[hr]', 'B[is]', 'W[hs]', 'B[eo]', 'W[do]', 'B[go]', 'W[en]', 'B[em]', 'W[ep]', 'B[fo]', 'W[gl]', 'B[rn]', 'W[ro]', 'B[rp]', 'W[sp]', 'B[rq]', 'W[sq]', 'B[sr]', 'W[so]', 'B[rr]', 'W[sn]', 'B[sm]', 'W[qn]', 'B[rm]', 'W[qk]', 'B[rk]', 'W[ki]', 'B[qj]', 'W[lh]', 'B[od]', 'W[mh]', 'B[ni]', 'W[fj]', 'B[hm]', 'W[hl]', 'B[dj]', 'W[di]', 'B[ej]', 'W[ei]', 'B[fk]', 'W[gk]', 'B[fl]', 'W[eg]', 'B[fg]', 'W[ee]', 'B[fe]', 'W[da]', 'B[fb]', 'W[bc]', 'B[ea]', 'W[ca]', 'B[ma]', 'W[la]', 'B[na]', 'W[ia]', 'B[ha]', 'W[ja]', 'B[mc]', 'W[lb]', 'B[me]', 'W[le]', 'B[md]', 'W[dm]', 'B[ek]']\n",
      "     22/Unknown - 4s 85ms/step - loss: 6.1566 - accuracy: 0.0153['B[pd]', 'W[dq]', 'B[pq]', 'W[dd]', 'B[do]', 'W[dm]', 'B[cq]', 'W[dp]', 'B[cp]', 'W[eo]', 'B[dn]', 'W[en]', 'B[cm]', 'W[cl]', 'B[cn]', 'W[bl]', 'B[dr]', 'W[er]', 'B[cr]', 'W[fq]', 'B[jp]', 'W[bn]', 'B[bo]', 'W[bm]', 'B[co]', 'W[el]', 'B[fc]', 'W[ec]', 'B[fd]', 'W[eb]', 'B[jc]', 'W[gb]', 'B[nc]', 'W[hd]', 'B[pj]', 'W[gf]', 'B[cf]', 'W[ce]', 'B[ci]', 'W[cg]', 'B[dg]', 'W[bf]', 'B[df]', 'W[ch]', 'B[dh]', 'W[di]', 'B[ei]', 'W[dj]', 'B[fh]', 'W[eg]', 'B[ff]', 'W[fg]', 'B[gg]', 'W[hg]', 'B[gh]', 'W[ef]', 'B[fe]', 'W[ee]', 'B[ge]', 'W[hf]', 'B[he]', 'W[ie]', 'B[id]', 'W[gc]', 'B[if]', 'W[je]', 'B[ig]', 'W[hh]', 'B[hi]', 'W[jd]', 'B[ic]', 'W[kc]', 'B[kb]', 'W[lc]', 'B[lb]', 'W[fb]', 'B[ih]', 'W[le]', 'B[ne]', 'W[lg]', 'B[ng]', 'W[li]', 'B[ik]', 'W[qc]', 'B[qd]', 'W[pc]', 'B[oc]', 'W[ob]', 'B[nb]', 'W[pa]', 'B[rc]', 'W[rb]', 'B[qb]', 'W[pb]', 'B[sb]', 'W[qa]', 'B[rd]', 'W[oh]', 'B[ph]', 'W[og]', 'B[pg]', 'W[of]', 'B[pf]', 'W[oe]', 'B[od]', 'W[nf]', 'B[me]', 'W[mf]', 'B[pe]', 'W[eh]', 'B[fj]', 'W[ej]', 'B[fi]', 'W[qo]', 'B[qp]', 'W[rp]', 'B[rq]', 'W[pp]', 'B[qq]', 'W[op]', 'B[oq]', 'W[np]', 'B[mq]', 'W[nq]', 'B[nr]', 'W[mp]', 'B[lq]', 'W[rn]', 'B[rl]', 'W[sq]', 'B[sr]', 'W[sp]', 'B[rs]', 'W[qk]', 'B[qj]', 'W[rk]', 'B[rj]', 'W[sj]', 'B[si]', 'W[sk]', 'B[rh]', 'W[pk]', 'B[oi]', 'W[nk]', 'B[mj]', 'W[nj]', 'B[ni]', 'W[mi]', 'B[lj]', 'W[kj]', 'B[lk]', 'W[kk]', 'B[ll]', 'W[kl]', 'B[lm]', 'W[kp]', 'B[ki]', 'W[kh]', 'B[ji]', 'W[km]', 'B[nh]', 'W[lh]', 'B[kf]', 'W[lf]', 'B[kg]', 'W[ln]', 'B[mm]', 'W[mn]', 'B[nm]', 'W[om]', 'B[ol]', 'W[ok]', 'B[on]', 'W[pm]', 'B[nn]', 'W[pn]', 'B[pl]', 'W[ql]', 'B[ke]', 'W[ld]', 'B[kd]', 'W[nl]', 'B[mc]', 'W[no]', 'B[md]', 'W[ml]', 'B[jh]', 'W[oo]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     23/Unknown - 4s 85ms/step - loss: 6.1339 - accuracy: 0.0160['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fq]', 'W[fp]', 'B[gp]', 'W[eq]', 'B[fo]', 'W[ep]', 'B[go]', 'W[gq]', 'B[hq]', 'W[gr]', 'B[hr]', 'W[fr]', 'B[lp]', 'W[po]', 'B[oo]', 'W[on]', 'B[op]', 'W[pn]', 'B[nn]', 'W[qf]', 'B[pf]', 'W[pg]', 'B[of]', 'W[qe]', 'B[qd]', 'W[og]', 'B[md]', 'W[nm]', 'B[mn]', 'W[mm]', 'B[qp]', 'W[mg]', 'B[cf]', 'W[df]', 'B[dg]', 'W[ce]', 'B[cg]', 'W[ef]', 'B[ck]', 'W[kd]', 'B[fc]', 'W[lf]', 'B[ic]', 'W[kb]', 'B[db]', 'W[cm]', 'B[bl]', 'W[bm]', 'B[rd]', 'W[eg]', 'B[eh]', 'W[fh]', 'B[ei]', 'W[fi]', 'B[ej]', 'W[fk]', 'B[cc]', 'W[be]', 'B[qg]', 'W[qh]', 'B[rg]', 'W[rh]', 'B[rf]', 'W[ln]', 'B[lo]', 'W[kn]', 'B[ko]', 'W[in]', 'B[rn]', 'W[jo]', 'B[jp]', 'W[ip]', 'B[jq]', 'W[hp]', 'B[iq]', 'W[he]', 'B[hn]', 'W[im]', 'B[io]', 'W[jn]', 'B[hm]', 'W[hl]', 'B[ie]', 'W[hd]', 'B[id]', 'W[hc]', 'B[hb]', 'W[gb]', 'B[ib]', 'W[gc]', 'B[kc]', 'W[lc]', 'B[jc]', 'W[ld]', 'B[hf]', 'W[gf]', 'B[jf]', 'W[hg]', 'B[if]', 'W[me]', 'B[jh]', 'W[kg]', 'B[ih]', 'W[fj]', 'B[li]', 'W[kh]', 'B[ki]', 'W[ji]', 'B[ij]', 'W[jj]', 'B[jk]', 'W[kj]', 'B[lk]', 'W[kk]', 'B[kl]', 'W[jl]', 'B[ik]', 'W[ll]', 'B[km]', 'W[lj]', 'B[ii]', 'W[mj]', 'B[il]', 'W[lm]', 'B[gh]', 'W[gg]', 'B[gk]', 'W[gl]', 'B[fl]', 'W[gm]', 'B[fm]', 'W[ek]', 'B[gn]', 'W[bj]', 'B[cj]', 'W[bi]', 'B[bh]', 'W[bk]', 'B[ci]', 'W[cl]', 'B[bf]', 'W[em]', 'B[dk]', 'W[el]', 'B[hk]', 'W[ro]', 'B[rp]', 'W[rm]', 'B[so]', 'W[qn]', 'B[qo]', 'W[sm]', 'B[nc]', 'W[nd]', 'B[od]', 'W[mc]', 'B[nb]', 'W[nf]', 'B[bd]', 'W[ae]', 'B[ec]', 'W[bb]', 'B[ad]', 'W[cd]', 'B[bc]', 'W[dc]', 'B[cb]', 'W[ed]', 'B[fb]', 'W[fa]', 'B[ba]', 'W[ea]', 'B[da]', 'W[fd]', 'B[eb]', 'W[ga]', 'B[sh]', 'W[si]', 'B[sg]', 'W[re]']\n",
      "['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fq]', 'W[nc]', 'B[qf]', 'W[pc]', 'B[qc]', 'W[qb]', 'B[oc]', 'W[pb]', 'B[od]', 'W[ob]', 'B[rc]', 'W[md]', 'B[cn]', 'W[dn]', 'B[cq]', 'W[co]', 'B[dq]', 'W[bo]', 'B[ci]', 'W[cm]', 'B[cf]', 'W[gc]', 'B[bd]', 'W[cc]', 'B[bc]', 'W[qo]', 'B[qp]', 'W[po]', 'B[nq]', 'W[qk]', 'B[qi]', 'W[ol]', 'B[jp]', 'W[lp]', 'B[hp]', 'W[kq]', 'B[jq]', 'W[ln]', 'B[ro]', 'W[rn]', 'B[rp]', 'W[jc]', 'B[oi]', 'W[he]', 'B[ck]', 'W[bb]', 'B[cd]', 'W[dc]', 'B[de]', 'W[ee]', 'B[ef]', 'W[fe]', 'B[bq]', 'W[dg]', 'B[df]', 'W[ch]', 'B[bh]', 'W[di]', 'B[cg]', 'W[dh]', 'B[bj]', 'W[dj]', 'B[cj]', 'W[dk]', 'B[fn]', 'W[dl]', 'B[kd]', 'W[kc]', 'B[jd]', 'W[id]', 'B[le]', 'W[me]', 'B[lg]', 'W[mf]', 'B[lf]', 'W[rj]', 'B[ri]', 'W[qm]', 'B[mo]', 'W[lo]', 'B[oo]', 'W[op]', 'B[np]', 'W[pp]', 'B[qr]', 'W[oq]', 'B[or]', 'W[on]', 'B[no]', 'W[ll]', 'B[fg]', 'W[gi]', 'B[jn]', 'W[mm]', 'B[jl]', 'W[kr]', 'B[jr]', 'W[rb]', 'B[sb]', 'W[nh]', 'B[li]', 'W[ni]', 'B[jj]', 'W[kj]', 'B[ki]', 'W[ji]', 'B[ii]', 'W[jk]', 'B[ij]', 'W[ik]', 'B[kk]', 'W[kl]', 'B[lj]', 'W[hk]', 'B[ih]', 'W[ld]', 'B[jf]', 'W[pg]', 'B[qg]', 'W[oh]', 'B[ph]', 'W[oj]', 'B[pi]', 'W[og]', 'B[mr]', 'W[go]', 'B[eo]', 'W[ho]', 'B[io]', 'W[gp]', 'B[hq]', 'W[gq]', 'B[gr]', 'W[fp]', 'B[do]', 'W[eq]', 'B[fr]', 'W[ep]', 'B[cp]', 'W[en]', 'B[fo]', 'W[fm]', 'B[gn]', 'W[hn]', 'B[gm]', 'W[gl]', 'B[hm]', 'W[im]', 'B[hl]', 'W[il]']\n",
      "     24/Unknown - 4s 85ms/step - loss: 6.1154 - accuracy: 0.0160['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fq]', 'W[hp]', 'B[cq]', 'W[cp]', 'B[dq]', 'W[fp]', 'B[ep]', 'W[eo]', 'B[eq]', 'W[fo]', 'B[di]', 'W[gq]', 'B[bp]', 'W[bo]', 'B[bq]', 'W[dn]', 'B[cf]', 'W[be]', 'B[cl]', 'W[cn]', 'B[ql]', 'W[nc]', 'B[nd]', 'W[qf]', 'B[qe]', 'W[oc]', 'B[pc]', 'W[od]', 'B[oe]', 'W[ne]', 'B[pf]', 'W[md]', 'B[qg]', 'W[of]', 'B[pe]', 'W[qj]', 'B[qo]', 'W[nq]', 'B[lq]', 'W[pr]', 'B[qr]', 'W[or]', 'B[qq]', 'W[no]', 'B[lo]', 'W[ol]', 'B[lm]', 'W[qn]', 'B[rn]', 'W[qm]', 'B[rm]', 'W[pl]', 'B[qk]', 'W[pk]', 'B[rj]', 'W[qi]', 'B[ri]', 'W[qh]', 'B[rh]', 'W[pg]', 'B[rf]', 'W[oh]', 'B[fc]', 'W[hc]', 'B[cc]', 'W[dc]', 'B[cd]', 'W[ce]', 'B[de]', 'W[fd]', 'B[bf]', 'W[cj]', 'B[ci]', 'W[bi]', 'B[bh]', 'W[bj]', 'B[dj]', 'W[bl]', 'B[ck]', 'W[bk]', 'B[ek]', 'W[ee]', 'B[df]', 'W[ah]', 'B[ag]', 'W[eh]', 'B[dh]', 'W[ef]', 'B[dg]', 'W[eg]', 'B[db]', 'W[gc]', 'B[ec]', 'W[ed]', 'B[eb]', 'W[lr]', 'B[kr]', 'W[kq]', 'B[kp]', 'W[jq]', 'B[mr]', 'W[mq]', 'B[ls]', 'W[mp]', 'B[lp]', 'W[po]', 'B[pp]', 'W[oo]', 'B[ro]', 'W[jr]', 'B[lj]', 'W[mn]', 'B[ln]', 'W[ml]', 'B[ll]', 'W[kk]', 'B[lk]', 'W[hk]', 'B[kj]', 'W[fk]', 'B[jk]', 'W[el]', 'B[ji]', 'W[ej]', 'B[dk]', 'W[ei]', 'B[jf]', 'W[jh]', 'B[kh]', 'W[jg]', 'B[kg]', 'W[if]', 'B[kf]', 'W[ii]', 'B[ij]', 'W[hi]', 'B[hj]', 'W[gj]', 'B[gi]', 'W[ik]', 'B[jj]', 'W[hh]', 'B[ig]', 'W[ih]', 'B[ie]', 'W[hf]', 'B[he]', 'W[hg]', 'B[jd]', 'W[jc]', 'B[kd]', 'W[kc]', 'B[le]', 'W[mm]', 'B[nf]', 'W[og]', 'B[me]', 'W[mf]', 'B[nd]', 'W[ld]', 'B[ne]', 'W[ng]', 'B[mg]', 'W[mh]', 'B[lg]', 'W[mi]', 'B[li]', 'W[mj]', 'B[mk]', 'W[nk]', 'B[mc]', 'W[lc]', 'B[mb]', 'W[lb]', 'B[ob]', 'W[ge]', 'B[il]', 'W[hl]', 'B[im]', 'W[hm]', 'B[in]', 'W[hn]', 'B[io]', 'W[jo]', 'B[jp]', 'W[ip]', 'B[jn]', 'W[ho]', 'B[gb]', 'W[hb]', 'B[ga]', 'W[fr]', 'B[er]', 'W[gr]', 'B[ic]', 'W[ib]', 'B[id]', 'W[hd]', 'B[cm]', 'W[es]', 'B[ds]', 'W[fs]', 'B[bs]', 'W[bm]', 'B[ap]', 'W[ao]', 'B[ai]', 'W[aj]', 'B[ah]', 'W[dm]', 'B[js]', 'W[ma]', 'B[nb]', 'W[is]', 'B[ks]', 'W[ir]', 'B[na]', 'W[la]', 'B[op]', 'W[np]', 'B[qs]', 'W[nr]', 'B[ns]', 'W[os]', 'B[lr]', 'W[ms]', 'B[bd]', 'W[ha]', 'B[cb]', 'W[dl]', 'B[ns]']\n",
      "     25/Unknown - 4s 84ms/step - loss: 6.0875 - accuracy: 0.0169['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[qk]', 'W[nc]', 'B[nd]', 'W[oc]', 'B[pc]', 'W[od]', 'B[oe]', 'W[ne]', 'B[nf]', 'W[md]', 'B[of]', 'W[mf]', 'B[nk]', 'W[gd]', 'B[cf]', 'W[cd]', 'B[ci]', 'W[ck]', 'B[ei]', 'W[dn]', 'B[dq]', 'W[cq]', 'B[cr]', 'W[eq]', 'B[dr]', 'W[cp]', 'B[er]', 'W[fq]', 'B[fr]', 'W[gq]', 'B[gr]', 'W[hq]', 'B[bq]', 'W[bp]', 'B[br]', 'W[je]', 'B[pb]', 'W[ob]', 'B[bd]', 'W[bc]', 'B[be]', 'W[cc]', 'B[kq]', 'W[po]', 'B[qo]', 'W[pp]', 'B[qp]', 'W[oq]', 'B[pr]', 'W[np]', 'B[pn]', 'W[no]', 'B[nn]', 'W[mn]', 'B[nm]', 'W[lo]', 'B[hr]', 'W[iq]', 'B[ir]', 'W[or]', 'B[qq]', 'W[qf]', 'B[qe]', 'W[ph]', 'B[pf]', 'W[qg]', 'B[oi]', 'W[pi]', 'B[pj]', 'W[oh]', 'B[ni]', 'W[nh]', 'B[mh]', 'W[rj]', 'B[rk]', 'W[qj]', 'B[rf]', 'W[rg]', 'B[re]', 'W[ri]', 'B[sg]', 'W[sh]', 'B[qh]', 'W[sj]', 'B[sk]', 'W[sf]', 'B[se]', 'W[sg]', 'B[id]', 'W[ie]', 'B[hd]', 'W[ge]', 'B[he]', 'W[hf]', 'B[gc]', 'W[fc]', 'B[gb]', 'W[fb]', 'B[jd]', 'W[ke]', 'B[lc]', 'W[hb]', 'B[hc]', 'W[ga]', 'B[ib]', 'W[ha]', 'B[ia]', 'W[fa]', 'B[jc]', 'W[kd]', 'B[kc]', 'W[la]', 'B[kb]', 'W[ka]', 'B[ma]', 'W[mb]', 'B[oa]', 'W[na]', 'B[mc]', 'W[ja]', 'B[pa]', 'W[ld]', 'B[jb]']\n",
      "['B[pd]', 'W[dp]', 'B[pq]', 'W[dc]', 'B[de]', 'W[ce]', 'B[cf]', 'W[cd]', 'B[dg]', 'W[fc]', 'B[fq]', 'W[cn]', 'B[qk]', 'W[nc]', 'B[nd]', 'W[md]', 'B[ne]', 'W[oc]', 'B[pc]', 'W[me]', 'B[nf]', 'W[mf]', 'B[jp]', 'W[ng]', 'B[pg]', 'W[og]', 'B[pf]', 'W[ph]', 'B[qh]', 'W[pi]', 'B[qi]', 'W[pj]', 'B[qj]', 'W[pk]', 'B[qo]', 'W[ql]', 'B[ck]', 'W[pm]', 'B[jc]', 'W[hc]', 'B[je]', 'W[he]', 'B[jg]', 'W[hg]', 'B[ji]', 'W[hi]', 'B[jk]', 'W[hk]', 'B[jm]', 'W[hm]', 'B[in]', 'W[hn]', 'B[ho]', 'W[go]', 'B[hp]', 'W[gp]', 'B[gq]', 'W[ep]', 'B[dr]', 'W[cq]', 'B[cr]', 'W[br]', 'B[bm]', 'W[bn]', 'B[cm]', 'W[dm]', 'B[dl]', 'W[em]', 'B[di]', 'W[am]', 'B[al]', 'W[an]', 'B[bk]', 'W[bf]', 'B[bg]', 'W[be]', 'B[pb]', 'W[lc]', 'B[mp]', 'W[po]', 'B[qp]', 'W[qn]', 'B[op]', 'W[oo]', 'B[no]', 'W[nn]', 'B[mn]', 'W[nm]', 'B[mm]', 'W[ml]', 'B[ll]', 'W[mk]', 'B[lk]', 'W[mj]', 'B[lj]', 'W[li]', 'B[ki]', 'W[lh]', 'B[kg]', 'W[lg]', 'B[kd]', 'W[kf]', 'B[jf]', 'W[ke]', 'B[ld]', 'W[le]', 'B[kc]', 'W[lb]', 'B[ob]', 'W[nb]', 'B[fp]', 'W[fo]', 'B[eq]', 'W[dq]', 'B[er]', 'W[ag]', 'B[bh]', 'W[ah]', 'B[ai]', 'W[af]', 'B[bi]', 'W[ef]', 'B[df]', 'W[ee]', 'B[eg]', 'W[fg]', 'B[fh]', 'W[gg]', 'B[gh]', 'W[hh]', 'B[gj]', 'W[gi]', 'B[fi]', 'W[gk]', 'B[fk]', 'W[fl]', 'B[ek]', 'W[hj]', 'B[fj]', 'W[ro]', 'B[rp]', 'W[rn]', 'B[pp]', 'W[sp]', 'B[sq]', 'W[so]', 'B[rr]', 'W[rk]', 'B[rj]', 'W[rl]', 'B[sj]', 'W[of]', 'B[pe]', 'W[ib]', 'B[jb]', 'W[id]', 'B[jd]', 'W[dd]', 'B[bs]', 'W[ar]', 'B[ia]', 'W[ha]', 'B[ja]', 'W[ic]', 'B[il]']\n",
      "     26/Unknown - 5s 85ms/step - loss: 6.0594 - accuracy: 0.0174['B[pd]', 'W[dd]', 'B[cp]', 'W[pp]', 'B[eq]', 'W[cn]', 'B[cl]', 'W[dm]', 'B[ci]', 'W[cg]', 'B[dl]', 'W[fm]', 'B[eh]', 'W[el]', 'B[ek]', 'W[fk]', 'B[ej]', 'W[fp]', 'B[fq]', 'W[ep]', 'B[dp]', 'W[gp]', 'B[gq]', 'W[ip]', 'B[qn]', 'W[mq]', 'B[rp]', 'W[hq]', 'B[qq]', 'W[qp]', 'B[ro]', 'W[pq]', 'B[qr]', 'W[pr]', 'B[qk]', 'W[on]', 'B[kn]', 'W[jm]', 'B[lm]', 'W[ol]', 'B[lk]', 'W[jk]', 'B[li]', 'W[ql]', 'B[rl]', 'W[rk]', 'B[qm]', 'W[qj]', 'B[pl]', 'W[pk]', 'B[ql]', 'W[oj]', 'B[rj]', 'W[ri]', 'B[sk]', 'W[qi]', 'B[qg]', 'W[pg]', 'B[qf]', 'W[pf]', 'B[pe]', 'W[nh]', 'B[lg]', 'W[mg]', 'B[jh]', 'W[lf]', 'B[kg]', 'W[kf]', 'B[jf]', 'W[je]', 'B[if]', 'W[ie]', 'B[gf]', 'W[he]', 'B[hf]', 'W[ge]', 'B[fg]', 'W[ef]', 'B[dc]', 'W[ec]', 'B[cc]', 'W[cd]', 'B[bd]', 'W[be]', 'B[bb]', 'W[eb]', 'B[ac]', 'W[ae]', 'B[ad]', 'W[ff]', 'B[kc]', 'W[nc]', 'B[hc]', 'W[lc]', 'B[lb]', 'W[mc]', 'B[jb]', 'W[hb]', 'B[gb]', 'W[ic]', 'B[ib]', 'W[gc]', 'B[ha]', 'W[hd]', 'B[jc]', 'W[hb]', 'B[ka]', 'W[ia]', 'B[hc]', 'W[pb]', 'B[qc]', 'W[hb]', 'B[mb]', 'W[ga]', 'B[ja]', 'W[fb]', 'B[ma]', 'W[qb]', 'B[rb]', 'W[pc]', 'B[rd]', 'W[ne]', 'B[kp]', 'W[hr]', 'B[bo]', 'W[bn]', 'B[gr]', 'W[kq]', 'B[jo]', 'W[io]', 'B[jq]', 'W[lq]', 'B[jp]', 'W[jr]', 'B[iq]', 'W[ir]', 'B[hk]', 'W[gj]', 'B[ij]', 'W[kj]', 'B[lj]', 'W[im]', 'B[hm]', 'W[hn]', 'B[gm]', 'W[gn]', 'B[gk]', 'W[fj]', 'B[fi]', 'W[hj]', 'B[ik]', 'W[kk]', 'B[kl]', 'W[jl]', 'B[km]', 'W[ki]', 'B[lh]', 'W[hh]', 'B[gh]', 'W[ii]', 'B[ji]', 'W[jj]', 'B[ih]', 'W[hi]', 'B[hg]', 'W[ml]', 'B[ll]', 'W[lo]', 'B[db]', 'W[do]', 'B[mn]', 'W[mo]', 'B[nn]', 'W[ko]', 'B[jn]', 'W[in]', 'B[no]', 'W[np]', 'B[oo]', 'W[po]', 'B[op]', 'W[oq]', 'B[om]', 'W[pn]', 'B[nl]', 'W[ok]', 'B[nk]', 'W[pm]', 'B[nm]', 'W[bq]', 'B[bp]', 'W[cq]', 'B[dq]', 'W[dr]', 'B[er]', 'W[ds]', 'B[br]', 'W[ar]', 'B[bs]', 'W[aq]', 'B[cr]', 'W[bi]', 'B[bj]', 'W[di]', 'B[dj]', 'W[ch]', 'B[cj]', 'W[dh]', 'B[ei]', 'W[bh]', 'B[rh]', 'W[si]', 'B[ra]', 'W[sj]', 'B[rk]', 'W[qh]', 'B[rg]', 'W[od]', 'B[oe]', 'W[of]', 'B[re]', 'W[sh]', 'B[sg]', 'W[qs]', 'B[rs]', 'W[rr]', 'B[ps]', 'W[rq]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     27/Unknown - 5s 85ms/step - loss: 6.0391 - accuracy: 0.0185['B[pd]', 'W[dd]', 'B[cp]', 'W[pp]', 'B[eq]', 'W[qc]', 'B[qd]', 'W[pc]', 'B[od]', 'W[nb]', 'B[qn]', 'W[pj]', 'B[ph]', 'W[qh]', 'B[qg]', 'W[qi]', 'B[pg]', 'W[nj]', 'B[qk]', 'W[qo]', 'B[pn]', 'W[np]', 'B[qq]', 'W[rn]', 'B[or]', 'W[pq]', 'B[pr]', 'W[rm]', 'B[nn]', 'W[qp]', 'B[rq]', 'W[nq]', 'B[nr]', 'W[fp]', 'B[fq]', 'W[co]', 'B[dp]', 'W[do]', 'B[bo]', 'W[bn]', 'B[bp]', 'W[cm]', 'B[cf]', 'W[df]', 'B[dg]', 'W[ef]', 'B[cd]', 'W[ce]', 'B[be]', 'W[de]', 'B[bg]', 'W[bd]', 'B[dj]', 'W[ae]', 'B[bf]', 'W[gq]', 'B[ck]', 'W[dl]', 'B[cc]', 'W[bc]', 'B[cb]', 'W[bb]', 'B[ba]', 'W[db]', 'B[ad]', 'W[dc]', 'B[af]', 'W[ca]', 'B[rc]', 'W[rb]', 'B[rd]', 'W[md]', 'B[ic]', 'W[kc]', 'B[fc]', 'W[gr]', 'B[fr]', 'W[mr]', 'B[kn]', 'W[lo]', 'B[ln]', 'W[om]', 'B[on]', 'W[qm]', 'B[mo]', 'W[lp]', 'B[jp]', 'W[gd]', 'B[gc]', 'W[id]', 'B[hd]', 'W[he]', 'B[hc]', 'W[jd]', 'B[fd]', 'W[ge]', 'B[fe]', 'W[eg]', 'B[ff]', 'W[dh]', 'B[ch]', 'W[cg]', 'B[ci]', 'W[ng]', 'B[nf]', 'W[og]', 'B[of]', 'W[mf]', 'B[rg]', 'W[rh]', 'B[mb]', 'W[mc]', 'B[ob]', 'W[oc]', 'B[kb]', 'W[jb]', 'B[jc]', 'W[lb]', 'B[kd]', 'W[ka]', 'B[ie]', 'W[je]', 'B[if]', 'W[jf]', 'B[ig]', 'W[jq]', 'B[jo]', 'W[iq]', 'B[lr]', 'W[ms]', 'B[lq]', 'W[mq]', 'B[kq]', 'W[mp]', 'B[kp]', 'W[mm]', 'B[mn]', 'W[gm]', 'B[nm]', 'W[pm]', 'B[nl]', 'W[hj]', 'B[ok]', 'W[pk]', 'B[oj]', 'W[li]', 'B[pi]', 'W[qj]', 'B[oi]', 'W[ll]', 'B[lk]', 'W[kk]', 'B[ml]', 'W[mk]', 'B[lj]', 'W[kj]', 'B[lm]', 'W[kl]', 'B[mg]', 'W[lg]', 'B[mh]', 'W[lh]', 'B[mi]', 'W[mj]', 'B[ni]', 'W[fj]', 'B[bl]', 'W[bm]', 'B[go]', 'W[gp]', 'B[ho]', 'W[fo]', 'B[im]', 'W[hl]', 'B[il]', 'W[ik]', 'B[ib]', 'W[gg]', 'B[gf]', 'W[hg]', 'B[hf]', 'W[fg]', 'B[ii]', 'W[ga]', 'B[ia]', 'W[ge]', 'B[fa]', 'W[fb]', 'B[gb]', 'W[ea]', 'B[ha]', 'W[ip]', 'B[io]', 'W[ac]', 'B[ek]', 'W[cl]', 'B[bk]', 'W[ep]', 'B[dr]', 'W[dg]', 'B[fl]', 'W[ae]', 'B[bh]', 'W[dk]', 'B[ej]', 'W[el]', 'B[fk]', 'W[fm]', 'B[gk]', 'W[gj]', 'B[hk]', 'W[ij]', 'B[hm]', 'W[gl]', 'B[gs]', 'W[hs]', 'B[fs]', 'W[ir]', 'B[rp]', 'W[ro]', 'B[me]', 'W[lf]', 'B[le]', 'W[ld]', 'B[ne]', 'W[ke]', 'B[hh]', 'W[ji]', 'B[jh]', 'W[hi]', 'B[ih]', 'W[kh]', 'B[gh]', 'W[fh]', 'B[sb]', 'W[qb]', 'B[sh]', 'W[si]', 'B[sg]', 'W[gn]', 'B[hn]', 'W[ns]', 'B[qs]', 'W[di]', 'B[nd]', 'W[nc]', 'B[os]', 'W[al]', 'B[ak]', 'W[am]', 'B[ao]', 'W[cj]', 'B[bj]', 'W[ei]', 'B[cj]', 'W[jg]', 'B[pl]', 'W[ol]', 'B[ql]', 'W[rj]', 'B[ai]', 'W[jl]', 'B[jm]', 'W[kr]', 'B[ks]', 'W[jr]', 'B[ls]', 'W[hp]', 'B[js]', 'W[is]', 'B[fa]', 'W[eb]', 'B[ja]', 'W[kb]', 'B[oq]', 'W[op]', 'B[so]', 'W[sn]', 'B[sp]', 'W[ra]', 'B[sc]', 'W[gi]', 'B[sa]', 'W[ga]', 'B[an]', 'W[en]', 'B[nk]', 'W[fa]']\n",
      "     28/Unknown - 5s 85ms/step - loss: 6.0289 - accuracy: 0.0181['B[pd]', 'W[cp]', 'B[pq]', 'W[dc]', 'B[de]', 'W[ce]', 'B[cf]', 'W[cd]', 'B[dg]', 'W[fc]', 'B[eq]', 'W[ch]', 'B[dh]', 'W[bf]', 'B[ed]', 'W[ec]', 'B[gf]', 'W[ci]', 'B[qk]', 'W[qf]', 'B[pf]', 'W[pg]', 'B[of]', 'W[qe]', 'B[qd]', 'W[pe]', 'B[oe]', 'W[qh]', 'B[nd]', 'W[mc]', 'B[nc]', 'W[ld]', 'B[jc]', 'W[jd]', 'B[id]', 'W[je]', 'B[ie]', 'W[jf]', 'B[if]', 'W[jg]', 'B[hc]', 'W[gd]', 'B[ge]', 'W[kc]', 'B[jb]', 'W[kb]', 'B[rd]', 'W[re]', 'B[jp]', 'W[dq]', 'B[ep]', 'W[cm]', 'B[qo]', 'W[hp]', 'B[hq]', 'W[gq]', 'B[iq]', 'W[gp]', 'B[mp]', 'W[qm]', 'B[ok]', 'W[om]', 'B[mk]', 'W[mm]', 'B[kl]', 'W[km]', 'B[jm]', 'W[kn]', 'B[jn]', 'W[po]', 'B[pp]', 'W[oo]', 'B[op]', 'W[qn]', 'B[ro]', 'W[ll]', 'B[lk]', 'W[ko]', 'B[jk]', 'W[mo]', 'B[lp]', 'W[kp]', 'B[kq]', 'W[lo]', 'B[jo]', 'W[ig]', 'B[hg]', 'W[ii]', 'B[en]', 'W[gn]', 'B[el]', 'W[gl]', 'B[ej]', 'W[gj]', 'B[hj]', 'W[hi]', 'B[gi]', 'W[hk]', 'B[ij]', 'W[fi]', 'B[gh]', 'W[fj]', 'B[ei]', 'W[fh]', 'B[fg]', 'W[gr]', 'B[er]', 'W[fl]', 'B[fk]', 'W[gk]', 'B[ek]', 'W[em]', 'B[fm]', 'W[dm]', 'B[gm]', 'W[fn]', 'B[hm]', 'W[eo]', 'B[cg]', 'W[bg]', 'B[ck]', 'W[cl]', 'B[cj]', 'W[qj]', 'B[pj]', 'W[rj]', 'B[rk]', 'W[pi]', 'B[oi]', 'W[oh]', 'B[ni]', 'W[nh]', 'B[mh]', 'W[mg]', 'B[lh]', 'W[lg]', 'B[ji]', 'W[jh]', 'B[ih]', 'W[ki]', 'B[jj]', 'W[kh]', 'B[mi]', 'W[kj]', 'B[kk]', 'W[nf]', 'B[me]', 'W[ne]', 'B[md]', 'W[lc]', 'B[nb]', 'W[mb]', 'B[le]', 'W[na]', 'B[oa]', 'W[ma]', 'B[pb]', 'W[sd]', 'B[sc]', 'W[se]', 'B[rb]', 'W[np]', 'B[nq]', 'W[no]', 'B[bl]', 'W[bm]', 'B[bk]', 'W[bi]', 'B[gb]', 'W[fb]', 'B[fa]', 'W[ea]', 'B[ga]', 'W[gc]', 'B[hb]', 'W[fd]', 'B[fe]', 'W[dd]', 'B[ee]', 'W[ja]', 'B[ia]', 'W[ka]', 'B[sj]', 'W[si]', 'B[sk]', 'W[rh]', 'B[hr]', 'W[hs]', 'B[is]', 'W[gs]', 'B[jr]', 'W[hn]', 'B[im]', 'W[pl]', 'B[pk]', 'W[ml]', 'B[rn]', 'W[rm]', 'B[am]', 'W[an]', 'B[al]', 'W[bn]', 'B[ke]', 'W[kd]', 'B[kf]', 'W[kg]', 'B[sm]', 'W[di]', 'B[eh]', 'W[bj]', 'B[nl]', 'W[nm]', 'B[ol]', 'W[ql]', 'B[rl]', 'W[io]', 'B[ip]', 'W[in]', 'B[hd]', 'W[mf]', 'B[og]', 'W[ng]', 'B[od]', 'W[aj]', 'B[lj]', 'W[dj]', 'B[dk]']\n",
      "     29/Unknown - 5s 85ms/step - loss: 6.0051 - accuracy: 0.0183['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[qk]', 'W[mp]', 'B[np]', 'W[no]', 'B[op]', 'W[oo]', 'B[po]', 'W[pn]', 'B[qo]', 'W[om]', 'B[mq]', 'W[lp]', 'B[lq]', 'W[kp]', 'B[kq]', 'W[gq]', 'B[cn]', 'W[co]', 'B[dn]', 'W[dl]', 'B[fn]', 'W[jq]', 'B[cq]', 'W[bo]', 'B[eq]', 'W[ep]', 'B[fr]', 'W[fq]', 'B[dq]', 'W[gr]', 'B[es]', 'W[bm]', 'B[bq]', 'W[go]', 'B[gm]', 'W[di]', 'B[fc]', 'W[hc]', 'B[cc]', 'W[dc]', 'B[cd]', 'W[de]', 'B[db]', 'W[eb]', 'B[cb]', 'W[fb]', 'B[cf]', 'W[ch]', 'B[cg]', 'W[ce]', 'B[be]', 'W[dg]', 'B[df]', 'W[eg]', 'B[ef]', 'W[fe]', 'B[ff]', 'W[ge]', 'B[fg]', 'W[fh]', 'B[gh]', 'W[fi]', 'B[gi]', 'W[fj]', 'B[gj]', 'W[hf]', 'B[gg]', 'W[gk]', 'B[hk]', 'W[hl]', 'B[hm]', 'W[ik]', 'B[jm]', 'W[in]', 'B[im]', 'W[hj]', 'B[kl]', 'W[nc]', 'B[nd]', 'W[md]', 'B[ne]', 'W[pc]', 'B[qc]', 'W[oc]', 'B[qd]', 'W[me]', 'B[nf]', 'W[mf]', 'B[mg]', 'W[ng]', 'B[og]', 'W[nh]', 'B[oh]', 'W[ni]', 'B[kj]', 'W[oi]', 'B[qh]', 'W[qi]', 'B[rh]', 'W[ri]', 'B[rm]', 'W[lk]', 'B[kk]', 'W[lg]', 'B[jh]', 'W[jf]', 'B[ii]', 'W[mm]', 'B[ln]', 'W[lm]', 'B[km]', 'W[kn]', 'B[jn]', 'W[ko]', 'B[io]', 'W[hn]', 'B[ho]', 'W[gn]', 'B[em]', 'W[jo]', 'B[fo]', 'W[gp]', 'B[fp]', 'W[fl]', 'B[fm]', 'W[bn]', 'B[cl]', 'W[cm]', 'B[dm]', 'W[el]', 'B[bk]', 'W[ck]', 'B[bl]', 'W[cj]', 'B[qb]', 'W[pb]', 'B[si]', 'W[sj]', 'B[sh]', 'W[rk]', 'B[ql]', 'W[ph]', 'B[pg]', 'W[pi]', 'B[qf]', 'W[ok]', 'B[mh]', 'W[li]', 'B[mj]', 'W[mi]', 'B[lj]', 'W[nk]', 'B[mk]', 'W[ll]', 'B[ml]', 'W[nl]', 'B[ki]', 'W[kg]', 'B[jr]', 'W[iq]', 'B[ir]', 'W[hr]', 'B[kr]', 'W[rl]', 'B[qm]', 'W[qn]', 'B[rn]', 'W[sl]', 'B[pm]', 'W[on]', 'B[bh]', 'W[bi]', 'B[bg]', 'W[qj]', 'B[pa]', 'W[oa]', 'B[qa]', 'W[nb]', 'B[ea]', 'W[fa]', 'B[da]', 'W[ig]', 'B[ee]', 'W[ec]', 'B[fd]', 'W[gc]', 'B[ed]', 'W[hb]', 'B[gd]', 'W[hd]', 'B[gf]', 'W[he]', 'B[ih]', 'W[hg]', 'B[ai]', 'W[aj]', 'B[ah]', 'W[bj]', 'B[ap]', 'W[bp]', 'B[ao]', 'W[an]', 'B[aq]', 'W[al]', 'B[cp]', 'W[do]', 'B[eo]', 'W[ak]', 'B[kh]', 'W[lh]', 'B[od]', 'W[nj]', 'B[pk]', 'W[pj]', 'B[sm]', 'W[gs]', 'B[cs]', 'W[fs]', 'B[er]', 'W[is]', 'B[js]', 'W[hs]', 'B[hp]', 'W[hq]', 'B[gl]', 'W[jj]', 'B[ij]', 'W[jk]', 'B[il]', 'W[hk]', 'B[hi]', 'W[ol]', 'B[ro]', 'W[fk]']\n",
      "     30/Unknown - 5s 85ms/step - loss: 5.9915 - accuracy: 0.0188['B[pd]', 'W[dc]', 'B[dp]', 'W[pp]', 'B[de]', 'W[ce]', 'B[cf]', 'W[cd]', 'B[dg]', 'W[dj]', 'B[ec]', 'W[ed]', 'B[dd]', 'W[cb]', 'B[fc]', 'W[ee]', 'B[df]', 'W[gd]', 'B[fd]', 'W[fe]', 'B[ge]', 'W[ff]', 'B[hd]', 'W[gf]', 'B[he]', 'W[eb]', 'B[gc]', 'W[fb]', 'B[gb]', 'W[ch]', 'B[eh]', 'W[hg]', 'B[ej]', 'W[di]', 'B[ei]', 'W[ek]', 'B[fk]', 'W[el]', 'B[lc]', 'W[jg]', 'B[cn]', 'W[qf]', 'B[qe]', 'W[pf]', 'B[nd]', 'W[pj]', 'B[nq]', 'W[or]', 'B[nr]', 'W[oq]', 'B[np]', 'W[gq]', 'B[iq]', 'W[dr]', 'B[cq]', 'W[cr]', 'B[br]', 'W[fl]', 'B[gk]', 'W[nf]', 'B[gl]', 'W[gm]', 'B[hm]', 'W[gn]', 'B[hn]', 'W[go]', 'B[io]', 'W[om]', 'B[mm]', 'W[nk]', 'B[lk]', 'W[lg]', 'B[rf]', 'W[rg]', 'B[re]', 'W[qh]', 'B[ck]', 'W[dk]', 'B[cl]', 'W[cj]', 'B[fr]', 'W[fq]', 'B[eq]', 'W[er]', 'B[gr]', 'W[hr]', 'B[hq]', 'W[gs]', 'B[gp]', 'W[fp]', 'B[hp]', 'W[eo]', 'B[do]', 'W[bj]', 'B[bk]', 'W[ak]', 'B[al]', 'W[aj]', 'B[bm]', 'W[mj]', 'B[lj]', 'W[li]', 'B[ki]', 'W[kh]', 'B[ji]', 'W[kd]', 'B[kc]', 'W[jd]', 'B[jc]', 'W[id]', 'B[ic]', 'W[md]', 'B[mc]', 'W[ld]', 'B[ep]', 'W[fs]', 'B[en]', 'W[fo]', 'B[ir]', 'W[em]', 'B[dm]', 'W[fn]', 'B[dn]', 'W[ne]', 'B[od]', 'W[nn]', 'B[mn]', 'W[no]', 'B[mo]', 'W[op]', 'B[if]', 'W[ig]', 'B[jf]', 'W[kf]', 'B[bf]', 'W[be]', 'B[qm]', 'W[pl]', 'B[rp]', 'W[qo]', 'B[ro]', 'W[qn]', 'B[rn]', 'W[rl]', 'B[qq]', 'W[qr]', 'B[rr]', 'W[qs]', 'B[rs]', 'W[qp]', 'B[rm]', 'W[ql]', 'B[sl]', 'W[sk]', 'B[sm]', 'W[rj]', 'B[os]', 'W[pq]', 'B[rq]', 'W[pr]', 'B[is]', 'W[hs]', 'B[bs]', 'W[cs]', 'B[gi]', 'W[hi]', 'B[hj]', 'W[ii]', 'B[ij]', 'W[jh]', 'B[mk]', 'W[nl]', 'B[mi]', 'W[nj]', 'B[lh]', 'W[mh]', 'B[li]', 'W[ni]', 'B[ml]', 'W[gh]', 'B[fh]', 'W[hf]', 'B[ie]', 'W[fg]', 'B[bh]', 'W[bi]', 'B[bg]', 'W[dh]', 'B[eg]', 'W[af]', 'B[ag]', 'W[ae]', 'B[fj]', 'W[oe]', 'B[fa]', 'W[ea]', 'B[ga]', 'W[sf]', 'B[se]', 'W[pm]', 'B[pe]', 'W[sg]', 'B[je]', 'W[ke]', 'B[ns]', 'W[dq]', 'B[bp]', 'W[ho]', 'B[in]', 'W[ps]', 'B[lp]', 'W[nm]', 'B[qc]', 'W[ai]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     31/Unknown - 5s 85ms/step - loss: 5.9768 - accuracy: 0.0197['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[qk]', 'W[nd]', 'B[ne]', 'W[me]', 'B[nf]', 'W[mf]', 'B[od]', 'W[nc]', 'B[ng]', 'W[mg]', 'B[fc]', 'W[nh]', 'B[ph]', 'W[og]', 'B[oe]', 'W[oh]', 'B[pg]', 'W[pi]', 'B[qi]', 'W[pj]', 'B[qj]', 'W[pk]', 'B[ic]', 'W[pf]', 'B[qf]', 'W[qg]', 'B[qh]', 'W[qe]', 'B[rf]', 'W[pe]', 'B[re]', 'W[qd]', 'B[qc]', 'W[rd]', 'B[rc]', 'W[of]', 'B[pc]', 'W[pl]', 'B[lc]', 'W[md]', 'B[rg]', 'W[ql]', 'B[sd]', 'W[lh]', 'B[db]', 'W[fd]', 'B[gd]', 'W[ec]', 'B[fb]', 'W[ge]', 'B[hd]', 'W[ff]', 'B[eb]', 'W[dc]', 'B[cc]', 'W[cd]', 'B[bc]', 'W[bd]', 'B[cn]', 'W[nq]', 'B[qo]', 'W[oo]', 'B[iq]', 'W[po]', 'B[pp]', 'W[oq]', 'B[pr]', 'W[mo]', 'B[fq]', 'W[fp]', 'B[gp]', 'W[fo]', 'B[ck]', 'W[eq]', 'B[fr]', 'W[er]', 'B[lq]', 'W[qn]', 'B[ro]', 'W[rn]', 'B[rq]', 'W[bo]', 'B[bn]', 'W[co]', 'B[ch]', 'W[dn]', 'B[dm]', 'W[em]', 'B[dl]', 'W[dg]', 'B[ko]', 'W[hn]', 'B[km]', 'W[jl]', 'B[ll]', 'W[jn]', 'B[kn]', 'W[el]', 'B[ek]', 'W[fk]', 'B[ej]', 'W[fj]', 'B[bf]', 'W[ei]', 'B[di]', 'W[dh]', 'B[cg]', 'W[dj]', 'B[ci]', 'W[dk]', 'B[cj]', 'W[cm]', 'B[cl]', 'W[bm]', 'B[bl]', 'W[an]', 'B[if]', 'W[hg]', 'B[ig]', 'W[ih]', 'B[kk]', 'W[jj]', 'B[jk]', 'W[ik]', 'B[mb]', 'W[nb]', 'B[ob]', 'W[ma]', 'B[la]', 'W[na]', 'B[lb]', 'W[ld]', 'B[kd]', 'W[ke]', 'B[jd]', 'W[je]', 'B[ie]', 'W[jg]', 'B[nl]', 'W[mk]', 'B[ml]', 'W[nk]', 'B[lj]', 'W[lk]', 'B[kl]', 'W[kj]', 'B[jm]', 'W[il]', 'B[im]', 'W[hm]', 'B[in]', 'W[hp]', 'B[hq]', 'W[go]', 'B[gq]', 'W[io]', 'B[jo]', 'W[ip]', 'B[jp]', 'W[ho]', 'B[rl]', 'W[rm]', 'B[sl]', 'W[mr]', 'B[lr]', 'W[mq]', 'B[es]', 'W[ds]', 'B[fs]', 'W[cr]', 'B[ac]', 'W[ad]', 'B[cb]', 'W[cf]', 'B[be]', 'W[ce]', 'B[bg]', 'W[or]', 'B[ps]', 'W[ls]', 'B[ks]', 'W[ms]', 'B[jr]', 'W[lp]', 'B[kp]', 'W[om]', 'B[jf]', 'W[kf]', 'B[hf]', 'W[gg]', 'B[ij]', 'W[ji]', 'B[ok]', 'W[oj]', 'B[ol]', 'W[mj]', 'B[al]', 'W[oa]', 'B[pa]', 'W[oc]', 'B[pb]', 'W[mn]', 'B[so]', 'W[sm]', 'B[rj]', 'W[lo]', 'B[nm]', 'W[nn]', 'B[op]', 'W[np]', 'B[os]', 'W[ns]', 'B[gf]', 'W[fe]', 'B[he]', 'W[ae]', 'B[af]', 'W[mc]', 'B[kc]', 'W[ln]', 'B[kg]', 'W[jh]', 'B[lf]', 'W[le]', 'B[mi]', 'W[li]']\n",
      "     32/Unknown - 5s 85ms/step - loss: 5.9613 - accuracy: 0.0200['B[pd]', 'W[cp]', 'B[pq]', 'W[dc]', 'B[de]', 'W[dg]', 'B[cc]', 'W[dd]', 'B[cd]', 'W[ee]', 'B[df]', 'W[ef]', 'B[cg]', 'W[ch]', 'B[cf]', 'W[dh]', 'B[db]', 'W[eb]', 'B[cb]', 'W[ec]', 'B[jd]', 'W[nc]', 'B[lc]', 'W[ne]', 'B[pf]', 'W[ic]', 'B[id]', 'W[jb]', 'B[oc]', 'W[mc]', 'B[le]', 'W[me]', 'B[lg]', 'W[kc]', 'B[kd]', 'W[lb]', 'B[ld]', 'W[jc]', 'B[mf]', 'W[nf]', 'B[ng]', 'W[og]', 'B[of]', 'W[nh]', 'B[mg]', 'W[pg]', 'B[qg]', 'W[qh]', 'B[rh]', 'W[qf]', 'B[rg]', 'W[pe]', 'B[qe]', 'W[oe]', 'B[rf]', 'W[ph]', 'B[qd]', 'W[ri]', 'B[qm]', 'W[po]', 'B[qo]', 'W[pp]', 'B[qp]', 'W[oq]', 'B[pr]', 'W[np]', 'B[pn]', 'W[on]', 'B[om]', 'W[nn]', 'B[eq]', 'W[ep]', 'B[fp]', 'W[dq]', 'B[eo]', 'W[dp]', 'B[fq]', 'W[fo]', 'B[go]', 'W[fn]', 'B[jq]', 'W[or]', 'B[ps]', 'W[kq]', 'B[kr]', 'W[jp]', 'B[iq]', 'W[lq]', 'B[ip]', 'W[jo]', 'B[io]', 'W[gn]', 'B[ho]', 'W[jn]', 'B[fk]', 'W[el]', 'B[ek]', 'W[dk]', 'B[dj]', 'W[ck]', 'B[cj]', 'W[fl]', 'B[ei]', 'W[eh]', 'B[fi]', 'W[gk]', 'B[gj]', 'W[hk]', 'B[hj]', 'W[bg]', 'B[bf]', 'W[bh]', 'B[gh]', 'W[ij]', 'B[ii]', 'W[jj]', 'B[ji]', 'W[bj]', 'B[ml]', 'W[pl]', 'B[pm]', 'W[ql]', 'B[rl]', 'W[rk]', 'B[rm]', 'W[nk]', 'B[nm]', 'W[ol]', 'B[mn]', 'W[no]', 'B[mo]', 'W[lp]', 'B[lr]', 'W[mr]', 'B[ob]', 'W[hd]', 'B[he]', 'W[ge]', 'B[hf]', 'W[gf]', 'B[gg]', 'W[ki]', 'B[kh]', 'W[li]', 'B[kk]', 'W[lm]', 'B[mm]', 'W[ll]', 'B[lk]', 'W[mk]', 'B[ln]', 'W[jl]', 'B[km]', 'W[kl]', 'B[kn]', 'W[jm]', 'B[hn]', 'W[hm]', 'B[ea]', 'W[fa]', 'B[da]', 'W[fb]', 'B[ma]', 'W[la]', 'B[na]', 'W[od]', 'B[dr]', 'W[cr]', 'B[er]', 'W[cs]', 'B[ds]', 'W[ls]', 'B[jr]', 'W[ko]', 'B[mp]', 'W[mq]', 'B[mh]', 'W[mi]', 'B[ag]', 'W[ah]', 'B[af]', 'W[sh]', 'B[pf]', 'W[of]', 'B[qf]', 'W[sg]', 'B[sf]', 'W[si]', 'B[sk]', 'W[rj]', 'B[ks]', 'W[ms]', 'B[os]', 'W[ns]', 'B[rq]', 'W[in]', 'B[ff]', 'W[fe]', 'B[fg]', 'W[eg]', 'B[ci]', 'W[bi]', 'B[di]', 'W[nb]', 'B[pc]', 'W[md]', 'B[lh]', 'W[nl]', 'B[sl]', 'W[mb]', 'B[oa]']\n",
      "['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[cq]', 'W[cp]', 'B[dq]', 'W[eq]', 'B[er]', 'W[fq]', 'B[fr]', 'W[gq]', 'B[gr]', 'W[hq]', 'B[dj]', 'W[dl]', 'B[cf]', 'W[ch]', 'B[cc]', 'W[cd]', 'B[dc]', 'W[ed]', 'B[bd]', 'W[be]', 'B[bc]', 'W[ce]', 'B[fc]', 'W[ec]', 'B[eb]', 'W[fd]', 'B[gc]', 'W[gd]', 'B[dh]', 'W[ci]', 'B[di]', 'W[cj]', 'B[dg]', 'W[bf]', 'B[gj]', 'W[dk]', 'B[qk]', 'W[hc]', 'B[hb]', 'W[ib]', 'B[gb]', 'W[id]', 'B[nc]', 'W[qf]', 'B[qe]', 'W[pf]', 'B[qh]', 'W[nf]', 'B[nh]', 'W[lf]', 'B[kh]', 'W[jf]', 'B[hh]', 'W[gl]', 'B[qo]', 'W[nq]', 'B[no]', 'W[lp]', 'B[ln]', 'W[jo]', 'B[jm]', 'W[hn]', 'B[ik]', 'W[lh]', 'B[li]', 'W[mh]', 'B[mi]', 'W[kg]', 'B[jh]', 'W[ng]', 'B[oh]', 'W[qc]', 'B[pc]', 'W[re]', 'B[rd]', 'W[qd]', 'B[rc]', 'W[pe]', 'B[qb]', 'W[qe]', 'B[kc]', 'W[jc]', 'B[kb]', 'W[kd]', 'B[ld]', 'W[le]', 'B[md]', 'W[bq]', 'B[hr]', 'W[ir]', 'B[br]', 'W[bp]', 'B[ar]', 'W[or]', 'B[pr]', 'W[ps]', 'B[qs]', 'W[os]', 'B[rr]', 'W[oq]', 'B[op]', 'W[np]', 'B[oo]', 'W[mo]', 'B[mn]', 'W[lo]', 'B[kn]', 'W[ko]', 'B[rg]', 'W[sd]', 'B[rb]', 'W[sc]', 'B[sb]', 'W[se]', 'B[hf]', 'W[he]', 'B[ff]', 'W[if]', 'B[ig]', 'W[gf]', 'B[hg]', 'W[gg]', 'B[gh]', 'W[fg]', 'B[fh]', 'W[ef]', 'B[eg]', 'W[fe]', 'B[df]', 'W[de]', 'B[is]', 'W[jr]', 'B[js]', 'W[ks]', 'B[hs]', 'W[kr]', 'B[cs]', 'W[fk]', 'B[fj]', 'W[il]', 'B[jl]', 'W[hk]', 'B[ij]', 'W[hj]', 'B[hi]', 'W[im]', 'B[jn]', 'W[in]', 'B[gk]', 'W[hl]', 'B[ek]', 'W[fl]', 'B[el]', 'W[em]', 'B[ej]', 'W[cg]', 'B[pg]', 'W[ne]', 'B[od]', 'W[jb]', 'B[ka]', 'W[ha]', 'B[ga]', 'W[ia]', 'B[db]', 'W[ad]', 'B[ac]']\n",
      "     33/Unknown - 5s 85ms/step - loss: 5.9528 - accuracy: 0.0201['B[pd]', 'W[cp]', 'B[pq]', 'W[dc]', 'B[de]', 'W[ce]', 'B[cf]', 'W[cd]', 'B[dg]', 'W[fc]', 'B[eq]', 'W[ch]', 'B[dh]', 'W[bf]', 'B[qk]', 'W[cg]', 'B[df]', 'W[qf]', 'B[qe]', 'W[pf]', 'B[nd]', 'W[qj]', 'B[pk]', 'W[pj]', 'B[rj]', 'W[ri]', 'B[rk]', 'W[qh]', 'B[qo]', 'W[dq]', 'B[ep]', 'W[cm]', 'B[iq]', 'W[jq]', 'B[ip]', 'W[ir]', 'B[hr]', 'W[jr]', 'B[gq]', 'W[jp]', 'B[mq]', 'W[lr]', 'B[mr]', 'W[lq]', 'B[mp]', 'W[io]', 'B[hp]', 'W[ho]', 'B[go]', 'W[ko]', 'B[kd]', 'W[ic]', 'B[jc]', 'W[on]', 'B[ed]', 'W[ec]', 'B[id]', 'W[hc]', 'B[ci]', 'W[di]', 'B[ei]', 'W[dj]', 'B[ej]', 'W[dk]', 'B[hj]', 'W[ge]', 'B[fh]', 'W[gg]', 'B[fg]', 'W[gf]', 'B[gd]', 'W[hd]', 'B[fd]', 'W[ie]', 'B[gc]', 'W[gb]', 'B[jd]', 'W[je]', 'B[dd]', 'W[qn]', 'B[rn]', 'W[qm]', 'B[rm]', 'W[po]', 'B[qp]', 'W[pp]', 'B[oq]', 'W[mo]', 'B[nk]', 'W[oj]', 'B[ok]', 'W[pe]', 'B[re]', 'W[rf]', 'B[qc]', 'W[np]', 'B[nq]', 'W[op]', 'B[lp]', 'W[kp]', 'B[lo]', 'W[ln]', 'B[mn]', 'W[no]', 'B[kn]', 'W[lm]', 'B[nm]', 'W[mm]', 'B[nn]', 'W[nl]', 'B[om]', 'W[ol]', 'B[bq]', 'W[bp]', 'B[dr]', 'W[br]', 'B[cq]', 'W[dp]', 'B[cr]', 'W[aq]', 'B[sf]', 'W[sg]', 'B[se]', 'W[mi]', 'B[cc]', 'W[bc]', 'B[cb]', 'W[be]', 'B[bb]', 'W[bi]', 'B[hb]', 'W[fb]', 'B[ib]', 'W[he]', 'B[kg]', 'W[jh]', 'B[jg]', 'W[ih]', 'B[ig]', 'W[hh]', 'B[db]', 'W[ab]', 'B[eb]', 'W[kh]', 'B[ke]', 'W[lg]', 'B[lf]', 'W[mf]', 'B[me]', 'W[nf]', 'B[gi]', 'W[ik]', 'B[ij]', 'W[jk]', 'B[ml]', 'W[pm]', 'B[mj]', 'W[lj]', 'B[lk]', 'W[kj]', 'B[nj]', 'W[ni]', 'B[hk]', 'W[hl]', 'B[gl]', 'W[hm]', 'B[gm]', 'W[ll]', 'B[mk]', 'W[kk]', 'B[hn]', 'W[in]', 'B[gn]', 'W[eo]', 'B[fo]', 'W[en]', 'B[ek]', 'W[el]', 'B[fl]', 'W[em]', 'B[oe]', 'W[of]', 'B[ne]', 'W[ff]', 'B[ef]', 'W[cs]', 'B[er]', 'W[ap]', 'B[ds]', 'W[ar]', 'B[bs]', 'W[sj]', 'B[sk]', 'W[si]', 'B[ql]', 'W[ms]', 'B[ns]', 'W[ls]', 'B[or]', 'W[hs]', 'B[gs]', 'W[is]', 'B[ji]', 'W[ki]', 'B[gh]', 'W[hg]', 'B[fe]', 'W[if]', 'B[kf]', 'W[jf]', 'B[jb]', 'W[jj]', 'B[ii]', 'W[pl]', 'B[ro]', 'W[rr]', 'B[rq]', 'W[qr]', 'B[qq]', 'W[sq]', 'B[sp]', 'W[sr]', 'B[ps]', 'W[qs]', 'B[pr]', 'W[as]', 'B[cs]', 'W[hi]', 'B[gj]']\n",
      "     34/Unknown - 5s 85ms/step - loss: 5.9337 - accuracy: 0.0207['B[pd]', 'W[dc]', 'B[cp]', 'W[pp]', 'B[eq]', 'W[df]', 'B[jd]', 'W[jq]', 'B[nq]', 'W[oq]', 'B[np]', 'W[pn]', 'B[hq]', 'W[jo]', 'B[mn]', 'W[mr]', 'B[qq]', 'W[nr]', 'B[qo]', 'W[po]', 'B[qp]', 'W[qn]', 'B[rn]', 'W[rm]', 'B[so]', 'W[ql]', 'B[pr]', 'W[or]', 'B[ml]', 'W[jl]', 'B[mj]', 'W[pi]', 'B[ch]', 'W[cj]', 'B[cc]', 'W[cb]', 'B[cd]', 'W[dd]', 'B[cf]', 'W[ce]', 'B[be]', 'W[de]', 'B[bg]', 'W[dh]', 'B[bb]', 'W[db]', 'B[ba]', 'W[ci]', 'B[bi]', 'W[bj]', 'B[ai]', 'W[dn]', 'B[bn]', 'W[mh]', 'B[kj]', 'W[md]', 'B[lc]', 'W[mc]', 'B[gc]', 'W[qf]', 'B[oc]', 'W[rd]', 'B[qc]', 'W[rc]', 'B[rb]', 'W[kc]', 'B[jc]', 'W[lb]', 'B[kd]', 'W[ld]', 'B[kh]', 'W[kf]', 'B[if]', 'W[li]', 'B[ki]', 'W[lj]', 'B[lk]', 'W[kk]', 'B[kl]', 'W[jk]', 'B[km]', 'W[jm]', 'B[kn]', 'W[jn]', 'B[jg]', 'W[mi]', 'B[nj]', 'W[jb]', 'B[ib]', 'W[ni]', 'B[oj]', 'W[pj]', 'B[ol]', 'W[ke]', 'B[je]', 'W[hd]', 'B[id]', 'W[hc]', 'B[ic]', 'W[hb]', 'B[he]', 'W[jf]', 'B[ih]', 'W[ia]', 'B[ie]', 'W[hg]', 'B[ig]', 'W[ge]', 'B[hh]', 'W[gg]', 'B[gh]', 'W[fg]', 'B[gk]', 'W[ij]', 'B[gn]', 'W[gj]', 'B[fj]', 'W[hj]', 'B[fi]', 'W[fk]', 'B[ek]', 'W[fl]', 'B[el]', 'W[fm]', 'B[em]', 'W[fn]', 'B[ej]', 'W[en]', 'B[cl]', 'W[ei]', 'B[eh]', 'W[di]', 'B[fh]', 'W[eg]', 'B[cm]', 'W[cn]', 'B[bo]', 'W[fp]', 'B[fq]', 'W[go]', 'B[pf]', 'W[pg]', 'B[of]', 'W[qh]', 'B[qe]', 'W[re]', 'B[qd]', 'W[rf]', 'B[mg]', 'W[lg]', 'B[ng]', 'W[lh]', 'B[oi]', 'W[oh]', 'B[nh]', 'W[mf]', 'B[og]', 'W[ph]', 'B[ir]', 'W[jr]', 'B[js]', 'W[ks]', 'B[is]', 'W[iq]', 'B[gr]', 'W[hp]', 'B[lq]', 'W[lr]', 'B[kp]', 'W[jp]', 'B[ko]', 'W[pq]', 'B[ps]', 'W[pb]', 'B[qb]', 'W[ob]', 'B[ne]', 'W[nd]', 'B[od]', 'W[nf]', 'B[me]', 'W[lf]', 'B[oe]', 'W[le]', 'B[sb]', 'W[dp]', 'B[dq]', 'W[ep]', 'B[pk]', 'W[qk]', 'B[bk]', 'W[aj]', 'B[ak]', 'W[co]', 'B[bq]', 'W[ck]', 'B[bl]', 'W[pl]', 'B[ok]', 'W[nn]', 'B[nm]', 'W[no]', 'B[mo]', 'W[mq]', 'B[mp]', 'W[kq]', 'B[lp]', 'W[os]', 'B[rr]', 'W[dg]', 'B[cg]', 'W[ca]', 'B[bc]', 'W[nc]', 'B[on]', 'W[oo]', 'B[om]', 'W[pm]', 'B[sm]', 'W[sl]', 'B[sn]', 'W[gp]', 'B[gf]', 'W[ff]', 'B[gd]', 'W[hf]', 'B[gb]', 'W[ha]', 'B[qa]', 'W[gq]', 'B[hr]', 'W[sc]', 'B[ji]', 'W[ii]', 'B[kg]', 'W[pa]', 'B[pc]', 'W[nb]', 'B[dm]', 'W[dk]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     35/Unknown - 5s 85ms/step - loss: 5.9170 - accuracy: 0.0208['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[qk]', 'W[dj]', 'B[cq]', 'W[cp]', 'B[dq]', 'W[eq]', 'B[er]', 'W[fq]', 'B[fr]', 'W[gq]', 'B[gr]', 'W[hq]', 'B[ek]', 'W[dm]', 'B[dk]', 'W[nc]', 'B[nd]', 'W[md]', 'B[ne]', 'W[oc]', 'B[pc]', 'W[kc]', 'B[pg]', 'W[gc]', 'B[cc]', 'W[dc]', 'B[cd]', 'W[ce]', 'B[be]', 'W[cf]', 'B[bf]', 'W[cg]', 'B[db]', 'W[eb]', 'B[cb]', 'W[fc]', 'B[ej]', 'W[cj]', 'B[eh]', 'W[qi]', 'B[qh]', 'W[pi]', 'B[qn]', 'W[qp]', 'B[rp]', 'W[qq]', 'B[rq]', 'W[pp]', 'B[qr]', 'W[oq]', 'B[pr]', 'W[np]', 'B[kq]', 'W[nn]', 'B[ko]', 'W[pk]', 'B[ql]', 'W[ol]', 'B[km]', 'W[fm]', 'B[gk]', 'W[hm]', 'B[ik]', 'W[ck]', 'B[jl]', 'W[lf]', 'B[if]', 'W[id]', 'B[je]', 'W[gg]', 'B[ih]', 'W[me]', 'B[nf]', 'W[hf]', 'B[hg]', 'W[gf]', 'B[gh]', 'W[ie]', 'B[jf]', 'W[ef]', 'B[bp]', 'W[bo]', 'B[bq]', 'W[cn]', 'B[pb]', 'W[ob]', 'B[bg]', 'W[ch]', 'B[rh]', 'W[ri]', 'B[rj]', 'W[si]', 'B[sh]', 'W[qj]', 'B[rk]', 'W[pa]', 'B[qa]', 'W[oa]', 'B[rb]', 'W[oh]', 'B[ho]', 'W[fo]', 'B[gn]', 'W[gm]', 'B[fn]', 'W[en]', 'B[io]', 'W[jr]', 'B[kr]', 'W[jq]', 'B[jp]', 'W[go]', 'B[hn]', 'W[hr]', 'B[ds]', 'W[og]', 'B[of]', 'W[mh]', 'B[mk]', 'W[li]', 'B[kj]', 'W[jg]', 'B[ig]', 'W[jh]', 'B[ji]', 'W[ki]', 'B[jj]', 'W[kf]', 'B[jd]', 'W[jc]', 'B[aj]', 'W[bh]', 'B[ah]', 'W[al]', 'B[im]', 'W[ak]', 'B[ai]', 'W[nj]', 'B[mj]', 'W[nk]', 'B[mi]', 'W[ni]', 'B[lj]', 'W[lh]', 'B[mo]', 'W[mp]', 'B[lp]', 'W[or]', 'B[rr]', 'W[qo]', 'B[ro]', 'W[pn]', 'B[qm]', 'W[no]', 'B[mn]', 'W[ml]', 'B[ll]', 'W[mm]', 'B[lm]', 'W[mq]', 'B[mg]', 'W[lg]', 'B[mr]', 'W[nr]', 'B[ms]', 'W[ph]', 'B[qf]', 'W[ng]', 'B[mf]', 'W[kd]', 'B[ke]', 'W[le]', 'B[hi]', 'W[eg]', 'B[fg]', 'W[ff]', 'B[fh]', 'W[dh]', 'B[dl]', 'W[cl]', 'B[el]', 'W[em]', 'B[hl]', 'W[od]', 'B[oe]', 'W[is]', 'B[gs]', 'W[ks]', 'B[ls]', 'W[js]', 'B[ao]', 'W[an]', 'B[ap]', 'W[lq]', 'B[lr]', 'W[hp]', 'B[ip]', 'W[iq]', 'B[kp]', 'W[bj]', 'B[ag]', 'W[sj]', 'B[sk]', 'W[pm]', 'B[os]', 'W[ns]', 'B[ps]', 'W[di]', 'B[ei]', 'W[da]', 'B[ca]', 'W[ea]', 'B[fl]', 'W[bi]', 'B[gl]', 'W[pl]']\n",
      "     36/Unknown - 5s 85ms/step - loss: 5.9140 - accuracy: 0.0206['B[pd]', 'W[dd]', 'B[pq]', 'W[dq]', 'B[qk]', 'W[ck]', 'B[fc]', 'W[nc]', 'B[cc]', 'W[dc]', 'B[cd]', 'W[de]', 'B[db]', 'W[eb]', 'B[cb]', 'W[ec]', 'B[cf]', 'W[ce]', 'B[be]', 'W[bg]', 'B[bf]', 'W[ch]', 'B[nd]', 'W[mc]', 'B[oc]', 'W[ld]', 'B[kc]', 'W[kd]', 'B[jd]', 'W[fd]', 'B[jf]', 'W[kf]', 'B[je]', 'W[lf]', 'B[jh]', 'W[oe]', 'B[od]', 'W[nf]', 'B[qf]', 'W[pe]', 'B[qe]', 'W[qd]', 'B[qc]', 'W[rd]', 'B[rc]', 'W[qg]', 'B[re]', 'W[pg]', 'B[sd]', 'W[ic]', 'B[jc]', 'W[gc]', 'B[kg]', 'W[lg]', 'B[lh]', 'W[mh]', 'B[li]', 'W[mi]', 'B[qn]', 'W[lj]', 'B[kj]', 'W[nj]', 'B[ij]', 'W[ig]', 'B[jg]', 'W[ii]', 'B[ji]', 'W[ih]', 'B[hj]', 'W[hf]', 'B[lb]', 'W[mb]', 'B[ib]', 'W[hb]', 'B[jb]', 'W[qp]', 'B[qq]', 'W[po]', 'B[rp]', 'W[pn]', 'B[qo]', 'W[qm]', 'B[rm]', 'W[rn]', 'B[ro]', 'W[rl]', 'B[sn]', 'W[ql]', 'B[ok]', 'W[pl]', 'B[pk]', 'W[rj]', 'B[nl]', 'W[pp]', 'B[ll]', 'W[np]', 'B[nq]', 'W[mq]', 'B[oq]', 'W[mo]', 'B[lk]', 'W[co]', 'B[fq]', 'W[fo]', 'B[iq]', 'W[jp]', 'B[jq]', 'W[kq]', 'B[ip]', 'W[jo]', 'B[io]', 'W[gp]', 'B[gq]', 'W[gm]', 'B[ep]', 'W[dp]', 'B[eo]', 'W[dm]', 'B[dg]', 'W[dh]', 'B[ob]', 'W[eg]', 'B[hd]', 'W[hc]', 'B[he]', 'W[ff]', 'B[rh]', 'W[qi]', 'B[qh]', 'W[pi]', 'B[ph]', 'W[oh]', 'B[rg]', 'W[fn]', 'B[pf]', 'W[og]', 'B[of]', 'W[ne]', 'B[md]', 'W[lc]', 'B[me]', 'W[mf]', 'B[jn]', 'W[ko]', 'B[gh]', 'W[gg]', 'B[fi]', 'W[ei]', 'B[ej]', 'W[dj]', 'B[ek]', 'W[hn]', 'B[in]', 'W[hl]', 'B[il]', 'W[eq]', 'B[fp]', 'W[hp]', 'B[em]', 'W[en]', 'B[dn]', 'W[do]', 'B[dl]', 'W[cm]', 'B[cl]', 'W[bm]', 'B[bl]', 'W[bk]', 'B[ak]', 'W[aj]', 'B[al]', 'W[bi]', 'B[df]', 'W[ef]', 'B[ea]', 'W[fa]', 'B[da]', 'W[fh]', 'B[gi]', 'W[hq]', 'B[hr]', 'W[ir]', 'B[gr]', 'W[jr]', 'B[kr]', 'W[lr]', 'B[ho]', 'W[go]', 'B[fm]', 'W[gn]', 'B[gl]', 'W[hm]', 'B[hk]', 'W[cn]', 'B[im]', 'W[dn]', 'B[gk]', 'W[nr]', 'B[or]', 'W[rk]', 'B[lp]', 'W[lq]', 'B[lo]', 'W[mm]', 'B[ln]', 'W[kp]', 'B[lm]', 'W[mn]', 'B[nk]', 'W[nm]', 'B[mj]', 'W[oj]', 'B[mp]', 'W[op]']\n",
      "     37/Unknown - 6s 85ms/step - loss: 5.9014 - accuracy: 0.0209['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[qk]', 'W[ok]', 'B[pm]', 'W[ql]', 'B[pl]', 'W[qj]', 'B[qm]', 'W[pk]', 'B[rl]', 'W[rk]', 'B[ql]', 'W[ri]', 'B[qg]', 'W[rg]', 'B[rf]', 'W[qh]', 'B[qf]', 'W[sf]', 'B[se]', 'W[sg]', 'B[rd]', 'W[po]', 'B[qo]', 'W[pp]', 'B[qp]', 'W[oq]', 'B[pr]', 'W[or]', 'B[jp]', 'W[ps]', 'B[qr]', 'W[qs]', 'B[rs]', 'W[os]', 'B[rr]', 'W[jq]', 'B[iq]', 'W[ip]', 'B[hq]', 'W[io]', 'B[kq]', 'W[jr]', 'B[kr]', 'W[jo]', 'B[kp]', 'W[ko]', 'B[fq]', 'W[ep]', 'B[dr]', 'W[cr]', 'B[dq]', 'W[cq]', 'B[eq]', 'W[fp]', 'B[gp]', 'W[go]', 'B[dj]', 'W[hp]', 'B[gq]', 'W[jd]', 'B[cf]', 'W[dh]', 'B[ch]', 'W[df]', 'B[dg]', 'W[eg]', 'B[cg]', 'W[ce]', 'B[di]', 'W[eh]', 'B[dm]', 'W[cl]', 'B[dl]', 'W[ck]', 'B[cm]', 'W[bm]', 'B[bn]', 'W[bl]', 'B[co]', 'W[bi]', 'B[bh]', 'W[bj]', 'B[ak]', 'W[ci]', 'B[be]', 'W[bd]', 'B[af]', 'W[ai]', 'B[ah]', 'W[bg]', 'B[cj]', 'W[bk]', 'B[am]', 'W[al]', 'B[an]', 'W[ad]', 'B[dk]', 'W[ae]', 'B[aj]', 'W[bp]', 'B[lc]', 'W[jc]', 'B[le]', 'W[lb]', 'B[mb]', 'W[kb]', 'B[nc]', 'W[kf]', 'B[gf]', 'W[ef]', 'B[gh]', 'W[fi]', 'B[hi]', 'W[gj]', 'B[ji]', 'W[lf]', 'B[li]', 'W[ih]', 'B[ii]', 'W[kj]', 'B[ki]', 'W[lj]', 'B[nh]', 'W[mj]', 'B[mh]', 'W[og]', 'B[nf]', 'W[ng]', 'B[mg]', 'W[mf]', 'B[oh]', 'W[of]', 'B[ne]', 'W[oe]', 'B[od]', 'W[me]', 'B[nd]', 'W[md]', 'B[mc]', 'W[ld]', 'B[pg]', 'W[gg]', 'B[hg]', 'W[fg]', 'B[ig]', 'W[hf]', 'B[if]', 'W[he]', 'B[ie]', 'W[id]', 'B[lo]', 'W[ln]', 'B[mo]', 'W[mn]', 'B[no]', 'W[nn]', 'B[op]', 'W[mq]', 'B[lp]', 'W[lr]', 'B[js]', 'W[nr]', 'B[ms]', 'W[np]', 'B[oo]', 'W[ls]', 'B[mp]']\n",
      "['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[qk]', 'W[nd]', 'B[nc]', 'W[mc]', 'B[oc]', 'W[md]', 'B[qf]', 'W[og]', 'B[cn]', 'W[nq]', 'B[qo]', 'W[oo]', 'B[cf]', 'W[cj]', 'B[cq]', 'W[dq]', 'B[cp]', 'W[do]', 'B[bn]', 'W[dm]', 'B[iq]', 'W[pm]', 'B[qm]', 'W[pk]', 'B[ql]', 'W[pl]', 'B[lq]', 'W[oq]', 'B[pp]', 'W[qj]', 'B[fq]', 'W[fp]', 'B[gp]', 'W[fo]', 'B[go]', 'W[dn]', 'B[cr]', 'W[dr]', 'B[fc]', 'W[df]', 'B[cc]', 'W[dc]', 'B[cd]', 'W[ce]', 'B[be]', 'W[de]', 'B[bg]', 'W[cg]', 'B[bf]', 'W[ch]', 'B[ic]', 'W[hd]', 'B[hc]', 'W[fd]', 'B[gd]', 'W[ge]', 'B[gc]', 'W[id]', 'B[kc]', 'W[jd]', 'B[jc]', 'W[kd]', 'B[db]', 'W[eb]', 'B[cb]', 'W[ec]', 'B[fb]', 'W[fa]', 'B[ga]', 'W[ea]', 'B[hb]', 'W[gn]', 'B[jm]', 'W[in]', 'B[jn]', 'W[ho]', 'B[jp]', 'W[gq]', 'B[hq]', 'W[gr]', 'B[hr]', 'W[hp]', 'B[jk]', 'W[il]', 'B[jl]', 'W[mo]', 'B[ko]', 'W[ik]', 'B[ji]', 'W[jj]', 'B[kj]', 'W[ij]', 'B[kh]', 'W[ii]', 'B[ih]', 'W[hh]', 'B[im]', 'W[hm]', 'B[ig]', 'W[hg]', 'B[qh]', 'W[rj]', 'B[pj]', 'W[pi]', 'B[oj]', 'W[qi]', 'B[pn]', 'W[nm]', 'B[on]', 'W[ml]', 'B[om]', 'W[ol]', 'B[nn]', 'W[mn]', 'B[no]', 'W[mp]', 'B[cl]', 'W[dk]', 'B[dl]', 'W[el]', 'B[cm]', 'W[ck]', 'B[oh]', 'W[oi]', 'B[ni]', 'W[nj]', 'B[ph]', 'W[op]', 'B[ng]', 'W[mi]', 'B[nh]', 'W[lj]', 'B[lk]', 'W[mk]', 'B[ll]', 'W[if]', 'B[lb]', 'W[mb]', 'B[or]', 'W[nr]', 'B[pr]', 'W[lr]', 'B[kr]', 'W[mr]', 'B[ks]', 'W[ok]', 'B[ds]', 'W[es]', 'B[cs]', 'W[fr]', 'B[aj]', 'W[bj]', 'B[al]', 'W[ai]', 'B[ak]', 'W[bh]', 'B[bd]', 'W[bl]', 'B[am]', 'W[bk]', 'B[co]', 'W[rh]', 'B[rg]', 'W[lp]', 'B[kp]', 'W[sm]', 'B[rm]', 'W[sl]', 'B[sn]', 'W[rk]', 'B[rn]', 'W[jg]', 'B[jh]', 'W[kg]', 'B[oe]', 'W[lh]', 'B[ki]', 'W[mg]', 'B[nb]', 'W[na]', 'B[oa]', 'W[ma]', 'B[pb]', 'W[lc]', 'B[kb]', 'W[nf]', 'B[of]', 'W[ne]', 'B[io]', 'W[hn]', 'B[sh]', 'W[ri]', 'B[rl]', 'W[sk]', 'B[po]', 'W[lm]', 'B[km]', 'W[si]', 'B[sg]', 'W[hs]', 'B[is]', 'W[gs]', 'B[ln]', 'W[mm]', 'B[ns]', 'W[ip]', 'B[jo]', 'W[lo]', 'B[kn]', 'W[kk]', 'B[kl]', 'W[li]', 'B[kk]', 'W[jq]', 'B[ir]', 'W[jr]', 'B[kq]', 'W[mq]', 'B[js]', 'W[ms]']\n",
      "     38/Unknown - 6s 85ms/step - loss: 5.8933 - accuracy: 0.0214['B[pd]', 'W[dp]', 'B[pq]', 'W[cd]', 'B[ed]', 'W[ec]', 'B[fc]', 'W[dc]', 'B[gd]', 'W[cf]', 'B[cq]', 'W[kc]', 'B[mc]', 'W[qc]', 'B[pc]', 'W[qd]', 'B[pe]', 'W[qe]', 'B[qo]', 'W[pb]', 'B[ob]', 'W[qb]', 'B[nc]', 'W[pf]', 'B[cp]', 'W[jq]', 'B[do]', 'W[eq]', 'B[dq]', 'W[ep]', 'B[eo]', 'W[fo]', 'B[fn]', 'W[go]', 'B[lq]', 'W[gn]', 'B[fm]', 'W[kr]', 'B[lr]', 'W[ls]', 'B[ms]', 'W[ks]', 'B[nr]', 'W[ci]', 'B[ck]', 'W[bj]', 'B[bk]', 'W[ak]', 'B[al]', 'W[aj]', 'B[bm]', 'W[eb]', 'B[ql]', 'W[qi]', 'B[er]', 'W[fr]', 'B[fq]', 'W[fp]', 'B[gr]', 'W[gq]', 'B[fs]', 'W[hr]', 'B[fq]', 'W[hp]', 'B[fr]', 'W[jo]', 'B[ic]', 'W[jd]', 'B[ie]', 'W[id]', 'B[hd]', 'W[je]', 'B[if]', 'W[kg]', 'B[mf]', 'W[lf]', 'B[mg]', 'W[kh]', 'B[ih]', 'W[ji]', 'B[le]', 'W[jf]', 'B[ig]', 'W[ii]', 'B[hi]', 'W[hj]', 'B[gi]', 'W[gj]', 'B[lo]', 'W[km]', 'B[mm]', 'W[lk]', 'B[ef]', 'W[gm]', 'B[fl]', 'W[gl]', 'B[fb]', 'W[fk]', 'B[ek]', 'W[fi]', 'B[fh]', 'W[ej]', 'B[dk]', 'W[eh]', 'B[eg]', 'W[gh]', 'B[gg]', 'W[hh]', 'B[hg]', 'W[fg]', 'B[ff]', 'W[fh]', 'B[qh]', 'W[ph]', 'B[pi]', 'W[rh]', 'B[qg]', 'W[pg]', 'B[ri]', 'W[qj]', 'B[rg]', 'W[qf]', 'B[rj]', 'W[rf]', 'B[sh]', 'W[pj]', 'B[qk]', 'W[oi]', 'B[lb]', 'W[kb]', 'B[of]', 'W[sf]', 'B[sg]', 'W[ol]', 'B[om]', 'W[nl]', 'B[nm]', 'W[ml]', 'B[mi]', 'W[mj]', 'B[li]', 'W[kj]', 'B[pl]', 'W[pk]', 'B[ib]', 'W[ni]', 'B[nh]', 'W[lm]', 'B[ln]', 'W[dd]', 'B[ee]', 'W[dg]', 'B[cj]', 'W[bh]', 'B[dj]', 'W[di]', 'B[em]', 'W[ke]', 'B[ld]', 'W[de]', 'B[kp]', 'W[jp]', 'B[kq]', 'W[ir]', 'B[ja]', 'W[ka]', 'B[jb]', 'W[fa]', 'B[ga]', 'W[ea]', 'B[la]', 'W[lc]', 'B[mb]', 'W[kd]', 'B[ki]', 'W[jh]', 'B[lj]', 'W[mk]', 'B[mh]', 'W[og]', 'B[nf]', 'W[oa]', 'B[na]', 'W[pa]', 'B[me]', 'W[lg]', 'B[hb]', 'W[kn]', 'B[ko]', 'W[hs]', 'B[dr]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     39/Unknown - 6s 85ms/step - loss: 5.8757 - accuracy: 0.0218['B[pd]', 'W[pp]', 'B[cp]', 'W[fc]', 'B[dc]', 'W[ce]', 'B[cd]', 'W[de]', 'B[be]', 'W[cg]', 'B[bc]', 'W[ic]', 'B[qf]', 'W[cj]', 'B[eq]', 'W[cm]', 'B[qn]', 'W[nq]', 'B[rp]', 'W[qq]', 'B[qk]', 'W[jp]', 'B[hq]', 'W[ho]', 'B[jj]', 'W[eo]', 'B[gj]', 'W[qi]', 'B[rj]', 'W[ok]', 'B[pj]', 'W[ni]', 'B[oi]', 'W[nh]', 'B[oh]', 'W[oj]', 'B[pi]', 'W[ng]', 'B[mj]', 'W[om]', 'B[mm]', 'W[lk]', 'B[lj]', 'W[kk]', 'B[kj]', 'W[mk]', 'B[jk]', 'W[km]', 'B[im]', 'W[hl]', 'B[il]', 'W[hk]', 'B[hj]', 'W[ik]', 'B[jl]', 'W[ij]', 'B[ii]', 'W[fl]', 'B[jh]', 'W[hh]', 'B[hi]', 'W[jg]', 'B[gh]', 'W[hg]', 'B[kg]', 'W[kf]', 'B[jf]', 'W[ig]', 'B[lf]', 'W[kh]', 'B[ke]', 'W[lh]', 'B[mh]', 'W[mg]', 'B[mi]', 'W[lg]', 'B[gf]', 'W[gg]', 'B[fg]', 'W[fh]', 'B[gi]', 'W[ff]', 'B[eg]', 'W[hf]', 'B[ge]', 'W[he]', 'B[ei]', 'W[ef]', 'B[fk]', 'W[hm]', 'B[eh]', 'W[ek]', 'B[ej]', 'W[gk]', 'B[fj]', 'W[dk]', 'B[jn]', 'W[lm]', 'B[ko]', 'W[kq]', 'B[mo]', 'W[lp]', 'B[lo]', 'W[nn]', 'B[np]', 'W[mp]', 'B[no]', 'W[qo]', 'B[on]', 'W[pn]', 'B[oo]', 'W[nm]', 'B[po]', 'W[pm]', 'B[qp]', 'W[pq]', 'B[ro]', 'W[pg]', 'B[qh]', 'W[qg]', 'B[rh]', 'W[rg]', 'B[ri]', 'W[og]', 'B[oc]', 'W[qm]', 'B[rm]', 'W[nd]', 'B[ld]', 'W[nc]', 'B[mc]', 'W[mb]', 'B[lb]', 'W[ob]', 'B[kc]', 'W[pc]', 'B[qc]', 'W[od]', 'B[qb]', 'W[qd]', 'B[rd]', 'W[pe]', 'B[qe]', 'W[pd]', 'B[rf]', 'W[sg]', 'B[sf]', 'W[md]', 'B[jd]', 'W[eb]', 'B[db]', 'W[mf]', 'B[ib]', 'W[hb]', 'B[jb]', 'W[hc]', 'B[ka]', 'W[ma]', 'B[la]', 'W[ia]', 'B[ja]', 'W[jc]', 'B[lc]', 'W[me]', 'B[le]', 'W[ie]', 'B[je]', 'W[pb]', 'B[qa]', 'W[pa]', 'B[sc]', 'W[nj]', 'B[rq]', 'W[rr]', 'B[jr]', 'W[kr]', 'B[ir]', 'W[bf]', 'B[ae]', 'W[co]', 'B[bo]', 'W[bn]', 'B[do]', 'W[cn]', 'B[bp]', 'W[dp]', 'B[dq]', 'W[ep]', 'B[fp]', 'W[fo]', 'B[gp]', 'W[go]', 'B[ip]', 'W[jo]', 'B[kn]', 'W[in]', 'B[jm]', 'W[hp]', 'B[gq]', 'W[jq]', 'B[iq]', 'W[mn]', 'B[ql]', 'W[pk]', 'B[ed]', 'W[fe]', 'B[fd]', 'W[gd]', 'B[ch]', 'W[bh]', 'B[ci]', 'W[bi]', 'B[ks]', 'W[ls]', 'B[js]', 'W[mr]', 'B[io]', 'W[hn]', 'B[kp]', 'W[lq]', 'B[sr]', 'W[rs]', 'B[dg]', 'W[da]', 'B[ca]', 'W[ea]', 'B[bb]', 'W[dd]', 'B[ec]', 'W[fb]', 'B[df]', 'W[cf]', 'B[ee]', 'W[af]', 'B[oq]', 'W[op]', 'B[or]', 'W[pr]', 'B[nr]', 'W[mq]', 'B[ps]', 'W[qr]', 'B[sq]', 'W[ns]', 'B[ss]', 'W[os]', 'B[pf]', 'W[of]', 'B[an]', 'W[am]', 'B[ao]', 'W[di]', 'B[dh]', 'W[dj]', 'B[pl]', 'W[ih]', 'B[ji]', 'W[ol]', 'B[sh]', 'W[ph]', 'B[qj]', 'W[kl]', 'B[id]', 'W[ha]', 'B[hd]', 'W[kf]', 'B[ag]', 'W[bg]', 'B[kg]', 'W[bd]', 'B[ad]', 'W[kf]', 'B[bj]', 'W[bk]', 'B[kg]', 'W[cb]', 'B[cc]', 'W[kf]', 'B[aj]', 'W[ak]', 'B[kg]', 'W[rb]', 'B[rc]', 'W[kf]', 'B[ai]', 'W[ah]', 'B[kg]', 'W[ba]', 'B[aa]', 'W[kf]', 'B[gc]', 'W[ge]', 'B[kg]', 'W[ac]', 'B[ab]', 'W[kf]', 'B[ga]', 'W[gb]', 'B[kg]', 'W[se]', 'B[re]', 'W[kf]', 'B[al]', 'W[bl]', 'B[kg]', 'W[cq]', 'B[bq]', 'W[kf]', 'B[na]', 'W[nb]', 'B[kg]', 'W[cr]', 'B[br]', 'W[kf]', 'B[dr]', 'W[kg]']\n",
      "     40/Unknown - 6s 85ms/step - loss: 5.8623 - accuracy: 0.0221['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[cc]', 'W[cd]', 'B[dc]', 'W[ec]', 'B[eb]', 'W[fc]', 'B[fb]', 'W[gc]', 'B[gb]', 'W[hc]', 'B[dj]', 'W[dl]', 'B[dg]', 'W[eh]', 'B[dh]', 'W[ej]', 'B[ei]', 'W[fi]', 'B[di]', 'W[ek]', 'B[eg]', 'W[fh]', 'B[ee]', 'W[ed]', 'B[gf]', 'W[ff]', 'B[fe]', 'W[fg]', 'B[ef]', 'W[ge]', 'B[ck]', 'W[cl]', 'B[cf]', 'W[bd]', 'B[bf]', 'W[bb]', 'B[hb]', 'W[ic]', 'B[cb]', 'W[ib]', 'B[bc]', 'W[ac]', 'B[da]', 'W[ad]', 'B[bl]', 'W[bm]', 'B[bk]', 'W[cn]', 'B[qk]', 'W[hf]', 'B[fq]', 'W[hq]', 'B[dq]', 'W[cq]', 'B[cr]', 'W[cp]', 'B[er]', 'W[br]', 'B[cs]', 'W[ep]', 'B[hr]', 'W[eq]', 'B[dr]', 'W[fp]', 'B[gq]', 'W[gp]', 'B[iq]', 'W[hp]', 'B[jr]', 'W[ip]', 'B[jp]', 'W[kq]', 'B[jq]', 'W[nq]', 'B[op]', 'W[np]', 'B[jo]', 'W[oq]', 'B[pp]', 'W[nn]', 'B[pn]', 'W[in]', 'B[ik]', 'W[il]', 'B[jl]', 'W[jm]', 'B[kl]', 'W[km]', 'B[jj]', 'W[gg]', 'B[hl]', 'W[im]', 'B[hj]', 'W[gk]', 'B[hk]', 'W[ln]', 'B[ll]', 'W[ml]', 'B[mk]', 'W[nl]', 'B[mi]', 'W[nc]', 'B[oc]', 'W[nd]', 'B[qf]', 'W[ke]', 'B[pi]', 'W[pm]', 'B[qm]', 'W[pl]', 'B[ql]', 'W[on]', 'B[po]', 'W[nk]', 'B[mj]', 'W[mf]', 'B[nb]', 'W[mb]', 'B[ob]', 'W[od]', 'B[qc]', 'W[pe]', 'B[qe]', 'W[pf]', 'B[pg]', 'W[og]', 'B[oh]', 'W[ng]', 'B[nh]', 'W[mg]', 'B[pr]', 'W[or]', 'B[lq]', 'W[lr]', 'B[kp]', 'W[lp]', 'B[kr]', 'W[mq]', 'B[kq]', 'W[lo]', 'B[gm]', 'W[fm]', 'B[fl]', 'W[gl]', 'B[hm]', 'W[fk]', 'B[fn]', 'W[em]', 'B[gn]', 'W[io]', 'B[fr]', 'W[kh]', 'B[ki]', 'W[jh]', 'B[lh]', 'W[lg]', 'B[mh]', 'W[ii]', 'B[ia]', 'W[ja]', 'B[ha]', 'W[ma]', 'B[mr]', 'W[ps]', 'B[rs]', 'W[qs]', 'B[qr]', 'W[os]', 'B[rr]', 'W[pj]', 'B[qj]', 'W[pk]', 'B[qh]', 'W[lm]', 'B[hi]', 'W[hh]', 'B[en]', 'W[dn]', 'B[ba]', 'W[ab]', 'B[bs]', 'W[aq]', 'B[af]', 'W[fd]', 'B[de]', 'W[am]', 'B[al]', 'W[eo]', 'B[dk]', 'W[gi]', 'B[ji]', 'W[gj]', 'B[ij]', 'W[ih]', 'B[ls]', 'W[nj]', 'B[oj]', 'W[ok]', 'B[oi]', 'W[oo]', 'B[na]', 'W[mc]', 'B[ms]', 'W[aa]', 'B[ca]', 'W[hn]', 'B[go]', 'W[fo]', 'B[ce]', 'W[rd]', 'B[qd]', 'W[rc]', 'B[rb]']\n",
      "     41/Unknown - 6s 85ms/step - loss: 5.8537 - accuracy: 0.0217['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fq]', 'W[cn]', 'B[jp]', 'W[po]', 'B[oo]', 'W[on]', 'B[op]', 'W[pn]', 'B[nn]', 'W[nm]', 'B[mn]', 'W[mm]', 'B[ln]', 'W[lm]', 'B[jn]', 'W[km]', 'B[pj]', 'W[kn]', 'B[ko]', 'W[jm]', 'B[in]', 'W[im]', 'B[qg]', 'W[hn]', 'B[hp]', 'W[ho]', 'B[ip]', 'W[fo]', 'B[cf]', 'W[ch]', 'B[cc]', 'W[cd]', 'B[dc]', 'W[ec]', 'B[bc]', 'W[bd]', 'B[ed]', 'W[ee]', 'B[fd]', 'W[df]', 'B[fc]', 'W[fe]', 'B[gi]', 'W[ge]', 'B[di]', 'W[ci]', 'B[dk]', 'W[cj]', 'B[dj]', 'W[ck]', 'B[gl]', 'W[hm]', 'B[ik]', 'W[hj]', 'B[gj]', 'W[hk]', 'B[gk]', 'W[hl]', 'B[hi]', 'W[ij]', 'B[ii]', 'W[jj]', 'B[jg]', 'W[ji]', 'B[jh]', 'W[kh]', 'B[je]', 'W[kg]', 'B[kf]', 'W[lf]', 'B[le]', 'W[mf]', 'B[me]', 'W[nf]', 'B[ne]', 'W[of]', 'B[oe]', 'W[pf]', 'B[qf]', 'W[pe]', 'B[qe]', 'W[qd]', 'B[qc]', 'W[rd]', 'B[rc]', 'W[od]', 'B[pc]', 'W[oc]', 'B[ob]', 'W[nb]', 'B[pb]', 'W[mc]', 'B[nd]', 'W[nc]', 'B[kc]', 'W[kb]', 'B[jb]', 'W[lb]', 'B[jc]', 'W[ma]', 'B[ka]', 'W[la]', 'B[ja]', 'W[oa]', 'B[pa]', 'W[na]', 'B[lc]', 'W[ph]', 'B[qh]', 'W[pi]', 'B[qi]', 'W[oj]', 'B[pk]', 'W[ok]', 'B[pl]', 'W[ol]', 'B[qm]', 'W[qn]', 'B[rq]', 'W[rm]', 'B[rl]']\n",
      "['B[pd]', 'W[cp]', 'B[pp]', 'W[dd]', 'B[ep]', 'W[dk]', 'B[cf]', 'W[ce]', 'B[df]', 'W[jd]', 'B[ci]', 'W[dq]', 'B[eq]', 'W[dn]', 'B[iq]', 'W[cj]', 'B[di]', 'W[be]', 'B[pj]', 'W[nc]', 'B[nd]', 'W[mc]', 'B[oc]', 'W[md]', 'B[ne]', 'W[gd]', 'B[oq]', 'W[jj]', 'B[gi]', 'W[hk]', 'B[gf]', 'W[pm]', 'B[qo]', 'W[lp]', 'B[lq]', 'W[er]', 'B[fr]', 'W[es]', 'B[fs]', 'W[cr]', 'B[kp]', 'W[fq]', 'B[gq]', 'W[kn]', 'B[fp]', 'W[mp]', 'B[mq]', 'W[ko]', 'B[jp]', 'W[oo]', 'B[ql]', 'W[og]', 'B[pg]', 'W[ph]', 'B[qg]', 'W[oi]', 'B[qh]', 'W[of]', 'B[pf]', 'W[lg]', 'B[bj]', 'W[bk]', 'B[dj]', 'W[ck]', 'B[bi]', 'W[gn]', 'B[oj]', 'W[nj]', 'B[pi]', 'W[oh]', 'B[ii]', 'W[hg]', 'B[gg]', 'W[hh]', 'B[hi]', 'W[hf]', 'B[gh]', 'W[ge]', 'B[bf]', 'W[af]', 'B[ag]', 'W[ae]', 'B[bh]', 'W[ok]', 'B[jh]', 'W[ki]', 'B[jf]', 'W[je]', 'B[kg]', 'W[lf]', 'B[if]', 'W[ie]', 'B[in]', 'W[im]', 'B[hm]', 'W[hn]', 'B[jm]', 'W[il]', 'B[jn]', 'W[ll]', 'B[km]', 'W[lm]', 'B[np]', 'W[no]', 'B[jo]', 'W[ln]', 'B[qm]', 'W[pn]', 'B[qn]', 'W[pl]', 'B[ob]', 'W[nb]', 'B[me]', 'W[le]', 'B[ek]', 'W[el]', 'B[fk]', 'W[fl]', 'B[ak]', 'W[al]', 'B[aj]', 'W[bm]', 'B[nf]', 'W[oe]', 'B[od]', 'W[ng]', 'B[pe]', 'W[kh]', 'B[kf]', 'W[ke]', 'B[ih]', 'W[ig]', 'B[jg]', 'W[he]', 'B[do]', 'W[co]', 'B[en]', 'W[em]', 'B[fn]', 'W[gm]', 'B[go]', 'W[ho]', 'B[hp]', 'W[kl]', 'B[jl]', 'W[jk]', 'B[pk]', 'W[po]', 'B[nk]', 'W[ol]', 'B[ni]', 'W[mj]', 'B[mg]', 'W[mh]', 'B[nh]', 'W[mi]', 'B[op]', 'W[ff]', 'B[fg]', 'W[ef]', 'B[eg]', 'W[de]', 'B[gk]', 'W[gl]', 'B[hj]', 'W[ij]', 'B[dp]', 'W[oa]', 'B[pa]', 'W[na]', 'B[qb]', 'W[qq]', 'B[qp]', 'W[pr]', 'B[rq]', 'W[rr]', 'B[qs]', 'W[sq]', 'B[rp]', 'W[or]', 'B[nr]', 'W[rs]', 'B[os]', 'W[sr]', 'B[ps]', 'W[io]', 'B[kq]', 'W[mf]', 'B[ng]', 'W[ip]', 'B[hr]', 'W[fm]']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\engine\\training.py:1189\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1187\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1189\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1191\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\callbacks.py:435\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 435\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\callbacks.py:295\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    293\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 295\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(hook))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\callbacks.py:315\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    312\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    313\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    318\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\callbacks.py:353\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    352\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 353\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    356\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\callbacks.py:1028\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1028\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\callbacks.py:1100\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1096\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1099\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1100\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\utils\\tf_utils.py:516\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    514\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\utils\\tf_utils.py:512\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    511\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 512\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    514\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1094\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1094\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1060\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1059\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m     six\u001b[38;5;241m.\u001b[39mraise_from(core\u001b[38;5;241m.\u001b[39m_status_to_exception(e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmessage), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B[pd]', 'W[dp]', 'B[pq]', 'W[dd]', 'B[fq]', 'W[cn]', 'B[qk]', 'W[nc]', 'B[nd]', 'W[md]', 'B[ne]', 'W[oc]', 'B[pc]', 'W[me]', 'B[nf]', 'W[mf]', 'B[hc]', 'W[jc]', 'B[dc]', 'W[cc]', 'B[ec]', 'W[cd]', 'B[ed]', 'W[ee]', 'B[fe]', 'W[ef]', 'B[cb]', 'W[bb]', 'B[db]', 'W[ff]', 'B[ge]', 'W[gf]', 'B[lc]', 'W[mc]', 'B[kc]', 'W[jd]', 'B[jb]', 'W[ib]', 'B[ic]', 'W[kb]', 'B[hb]', 'W[ja]', 'B[he]', 'W[hf]', 'B[id]', 'W[je]', 'B[ie]', 'W[if]', 'B[pg]', 'W[po]', 'B[qo]', 'W[qn]', 'B[qp]', 'W[pm]', 'B[nq]', 'W[pk]', 'B[qj]', 'W[ql]', 'B[pj]', 'W[ok]', 'B[oj]', 'W[nk]', 'B[iq]', 'W[lq]', 'B[dr]', 'W[cq]', 'B[ci]', 'W[ck]', 'B[cf]', 'W[de]', 'B[ei]', 'W[dh]', 'B[di]', 'W[ch]', 'B[bh]', 'W[bg]', 'B[bi]', 'W[cg]', 'B[ek]', 'W[em]', 'B[gl]', 'W[gm]', 'B[hm]', 'W[gn]', 'B[hk]', 'W[hn]', 'B[im]', 'W[in]', 'B[fl]', 'W[jm]', 'B[jl]', 'W[km]', 'B[jj]', 'W[kl]', 'B[jk]', 'W[jp]', 'B[jr]', 'W[kr]', 'B[pb]', 'W[jq]', 'B[js]', 'W[ks]', 'B[hr]', 'W[cr]', 'B[cs]', 'W[er]', 'B[eq]', 'W[dq]', 'B[ds]', 'W[bs]', 'B[fr]', 'W[es]', 'B[fs]', 'W[gp]', 'B[gq]', 'W[hp]', 'B[hq]', 'W[hs]', 'B[gs]', 'W[ir]', 'B[is]', 'W[ip]', 'B[mp]', 'W[lp]', 'B[mn]', 'W[no]', 'B[mo]', 'W[np]', 'B[mq]', 'W[pp]', 'B[qr]', 'W[oq]', 'B[or]', 'W[op]', 'B[nn]', 'W[on]', 'B[al]', 'W[bl]', 'B[aj]', 'W[am]', 'B[ak]', 'W[bm]', 'B[ag]', 'W[af]', 'B[ah]', 'W[bf]', 'B[ma]', 'W[lb]', 'B[oa]', 'W[mb]', 'B[na]', 'W[od]', 'B[oe]', 'W[ng]', 'B[og]', 'W[nh]', 'B[jg]', 'W[jf]', 'B[ih]', 'W[kg]', 'B[kh]', 'W[lg]', 'B[eh]', 'W[eg]', 'B[gh]', 'W[mr]', 'B[nr]', 'W[ms]', 'B[nm]', 'W[om]', 'B[nl]', 'W[ol]', 'B[rl]', 'W[rm]', 'B[rk]', 'W[ro]', 'B[rp]', 'W[sp]', 'B[rq]', 'W[sq]', 'B[sr]', 'W[so]', 'B[ns]', 'W[nj]', 'B[sm]', 'W[rn]', 'B[oi]', 'W[oh]', 'B[ph]', 'W[ni]', 'B[dl]', 'W[dm]', 'B[cl]', 'W[bk]', 'B[fm]', 'W[fn]', 'B[el]', 'W[cm]', 'B[ep]', 'W[eo]', 'B[lk]', 'W[kk]', 'B[kj]', 'W[ll]', 'B[lj]', 'W[ml]', 'B[kn]', 'W[jn]', 'B[ko]', 'W[kp]', 'B[dj]', 'W[bj]', 'B[ai]', 'W[hh]', 'B[hi]', 'W[hg]', 'B[fh]', 'W[ig]', 'B[jh]', 'W[lh]', 'B[li]', 'W[mi]', 'B[ba]', 'W[bc]', 'B[ha]', 'W[ia]', 'B[la]', 'W[ka]', 'B[fp]']\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs = 1,\n",
    "    validation_data = val_dataset,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaddf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model_kyu_10_15_f64_allk3_12layerC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee70e090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105995/105995 [==============================] - 15947s 150ms/step - loss: 1.7524 - accuracy: 0.5309\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f913656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model_kyu_10_15_f64.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90d58cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105995/105995 [==============================] - 15979s 151ms/step - loss: 1.6064 - accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dcbe8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model_kyu_10_15_f128_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a834cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     38/Unknown - 6s 153ms/step - loss: 1.4566 - accuracy: 0.6145"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\engine\\training.py:1189\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1187\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1189\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1191\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\callbacks.py:435\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 435\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\callbacks.py:295\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    293\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 295\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(hook))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\callbacks.py:315\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    312\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    313\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    318\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\callbacks.py:353\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    352\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 353\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    356\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\callbacks.py:1028\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1028\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\callbacks.py:1100\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1096\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1099\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1100\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\utils\\tf_utils.py:516\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    514\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\keras\\utils\\tf_utils.py:512\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    511\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 512\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    514\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1094\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1094\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\aicup_\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1060\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1059\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m     six\u001b[38;5;241m.\u001b[39mraise_from(core\u001b[38;5;241m.\u001b[39m_status_to_exception(e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmessage), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f436af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model_kyu_10_15_f128_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model_kyu_10_15_f128_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a850b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f6688",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model_kyu_10_15_f128_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd7f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aca136",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model_kyu_10_15_f128_6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990c90e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model_kyu_10_15_f128_7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f38557",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model_kyu_10_15_f128_8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718dcd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model_kyu_10_15_f128_9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68be49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model_kyu_10_14_11.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484be28d",
   "metadata": {},
   "source": [
    "## ALL DONE!\n",
    "\n",
    "For using the model and creating a submission file, follow the notebook **Create Public Upload CSV.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7fafaa",
   "metadata": {},
   "source": [
    "# End of Tutorial\n",
    "\n",
    "You are free to use more modern NN architectures, a better pre-processing, feature extraction methods to achieve much better accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f1db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
