{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6abf8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "542a64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(tf.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.norm = tf.keras.layers.LayerNormalization(axis=1, epsilon=1e-5)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c467cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.net = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(hidden_dim),\n",
    "            tf.keras.layers.Activation(tf.nn.gelu),\n",
    "            tf.keras.layers.GaussianNoise(dropout),\n",
    "            tf.keras.layers.Dense(dim),\n",
    "            tf.keras.layers.GaussianNoise(dropout)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3d4af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = tf.nn.softmax(dim=-1)\n",
    "        self.to_qkv = tf.keras.layers.Dense(dim, inner_dim * 3, use_bias=False)\n",
    "\n",
    "        if project_out:\n",
    "            self.to_out = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(inner_dim, dim),\n",
    "                tf.keras.layers.GaussianNoise(dropout)\n",
    "            ])\n",
    "        else:\n",
    "            self.to_out = tf.identity\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _ = x.shape\n",
    "        h = self.heads\n",
    "\n",
    "        qkv = tf.split(self.to_qkv(x), num_or_size_splits=3, axis=-1)\n",
    "        q, k, v = map(lambda t: tf.reshape(t, (b, n, h, -1)), qkv)\n",
    "\n",
    "        dots = tf.einsum('bhid, bhjd->bhij', q, k) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = tf.einsum('bhij, bhjd->bhid', attn, v)\n",
    "        out = tf.reshape(out, (b, n, -1))\n",
    "        \n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a60fc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layers = []\n",
    "        for _ in range(depth):\n",
    "            self.layers.append([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = tf.add(attn(x), x)\n",
    "            x = tf.add(ff(x), x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38cdeffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = tf.zeros((max_len, d_model), dtype=tf.float32)\n",
    "        position = tf.range(0, max_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * (-np.log(10000.0) / d_model))\n",
    "        pe = tf.cast(pe, dtype=tf.float32)\n",
    "        pe = tf.cast(pe, dtype=tf.float32)\n",
    "        pe = pe + tf.sin(position * div_term)\n",
    "        pe = pe + tf.cos(position * div_term)\n",
    "        pe = tf.expand_dims(pe, axis=0)\n",
    "        self.pe = tf.Variable(pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1], :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fda5e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(tf.Module):\n",
    "    def __init__(self, *, input_dim=320, output_dim=512, dim=1024, depth=6, heads=16, mlp_dim=2048, pool='cls', dim_head=64, dropout=0.1, emb_dropout=0.1):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.project = tf.keras.layers.Dense(input_dim,dim)\n",
    "        self.pos_encoder = PositionalEncoding(dim)\n",
    "        self.cls_token = tf.Variable(tf.random.normal((1, 1, dim), dtype=tf.float32))\n",
    "        self.dropout = tf.keras.layers.Dropout(emb_dropout) \n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "        self.pool = pool\n",
    "        self.to_latent = tf.identity\n",
    "        self.mlp_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.LayerNormalization(epsilon=1e-5),\n",
    "            tf.keras.layers.Dense(output_dim)\n",
    "        ])\n",
    "        self.tanh = tf.keras.activations.tanh\n",
    "    def forward(self, x):\n",
    "        x = self.project(x)\n",
    "        b, n, _ = x.shape\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]\n",
    "        x = self.to_latent(x)\n",
    "        return self.tanh(self.mlp_head(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaae35b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
